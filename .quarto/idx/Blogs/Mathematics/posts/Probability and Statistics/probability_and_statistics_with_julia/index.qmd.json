{"title":"Probability and statistics with Julia","markdown":{"yaml":{"title":"Probability and statistics with Julia","author":"Rui Yang","date":"2024-09-22","date-modified":"last-modified","categories":["probability","statistics","julia"],"format":{"html":{"toc":true,"toc-depth":6,"toc-location":"left","fig-align":"center","number-depth":6,"number-sections":true,"fig-cap-location":"bottom","fig-format":"png","lightbox":true,"tbl-cap-location":"top","page-layout":"full"}},"execute":{"warning":false},"jupyter":"julia-1.10"},"headingText":"References","containsRefs":false,"markdown":"\n\n\n1. **Statistics with Julia** by Yoni Nazarathy (2021).\n\n## Pseudorandom number generation\n\nFor pseudorandom number generation, there is some deterministic (non-random and well defined) sequence $\\{x_n\\}$, specified by\n\n$$\nx_{n+1} = f(x_n, x_{n-1}, ...)\n$$\n\noriginating from some specified *seed* $x_0$. The mathematical function $f(\\cdot)$ is designed to yield desirable properties for the sequence $\\{x_n\\}$ that make it appear random.\n\nThose properties include:\n\n1.  Elements $x_i$ and $x_j$ for $i \\neq j$ should appear statistically independent. That is, knowing the value of $x_i$ should not yield any information about the value $x_j$.\n\n2.  The distribution of $\\{x_n\\}$ should appear uniform. That is, there shouldn't be values (or ranges of values) where elements of $\\{x_n\\}$ occur more frequently than others.\n\n3.  The range covered by $\\{x_n\\}$ should be well defined.\n\n4.  The sequence should repeat itself as rarely as possible.\n\nIn Julia, the main player for pseudorandom number generation is the function `rand()`, which generates a random number in each call without giving any arguments once a seed is set (it is usually set to the current time by default). You can set the seed yourself by using the `Random.seed!()` function from the Random package.\n\n```{julia}\nusing Random\n\nRandom.seed!(2023)\nprintln(\"Seed 2023: \", rand(), \"\\t\", rand(), \"\\t\", rand())\nRandom.seed!(2024)\nprintln(\"Seed 2024: \", rand(), \"\\t\", rand(), \"\\t\", rand())\nRandom.seed!(2023)\nprintln(\"Seed 2023: \", rand(), \"\\t\", rand(), \"\\t\", rand())\n```\n\nAs can be seen from the output, setting the same seed will generate the same sequence.\n\n### Creating a simple pseudorandom number generator\n\nHere, we create a *Linear Congruential Generator* (LCG). The function $f(\\cdot)$ used here is just a linear transformation modulo $m$: $x_{n+1} = (ax_n + c) \\mod m$.\n\nHere, we pick $m = 2^{32}$, $a = 69069$, $c = 1$, which yields sensible performance.\n\n```{julia}\nusing DataFrames, AlgebraOfGraphics, CairoMakie\n\na, c, m = 69069, 1, 2^32\nnext(x) = (a * x + c) % m\n\nN = 10^6\nvec = Array{Float64,1}(undef, N)\n\nx = 2024  # Seed\nfor i in 1:N\n    global x = next(x)\n    vec[i] = x / m  # Scale x to [0, 1]\nend\ndf = DataFrame(x=1:N, y=vec)\n\nfig = Figure()\np1 = data(first(df, 5000)) * mapping(:x, :y) * visual(Scatter, markersize=3)\np2 = data(df) * mapping(:y) * visual(Hist, bins=50, normalization=:pdf)\ndraw!(fig[1, 1], p1, axis=(xlabel=\"n\", ylabel=L\"\\mathbf{x_n}\"))\ndraw!(fig[1, 2], p2, axis=(xlabel=\"x\", ylabel=\"Density\"))\nfig\n```\n\n### More about Julia's pseudorandom number generator\n\nIn addition to `rand()`, we can also use `randn()` to generate **normally distributed** random numbers.\n\nAfter invoking `using Random`, the following functions are available:\n\n- `Random.seed!()`\n\n- `randsubseq()`\n\n- `randstring()`\n\n- `randcycle()`\n\n- `bitrand()`\n\n- `randperm()` and `shuffle()` for permutations\n\nIn addition, in Julia, we can create an object representing a pseudorandom number generator implemented via a specified algorithm, for example, the *Mersenne Twister* pseudorandom number generator, which is considerably more complicated than the LCG described above. In Julia, we can create such an object of the *Mersenne Twister* pseudorandom number generator by calling `rng = MersenneTwister(seed)`, and then pass the `rng` to `rand()` to let it use the given pseudorandom number generator to generate pseudorandom numbers.\n\n## Monte Carlo simulation\n\nThe core idea of Monte Carlo simulation lies in building a mathematical relationship between an unknown quantity to be estimated and the probability of a certain event, which can be estimated by statistical sampling. Then, we can get an estimate of this unknown quantity.\n\nWe can use this idea to estimate the value of $\\pi$.\n\n```{julia}\nusing DataFrames, AlgebraOfGraphics, CairoMakie\n\nline_df = DataFrame(x=[0, 0, 1, 1, 0],\n    y=[0, 1, 1, 0, 0])\n\nx = range(0, 1, length=1000)\nquarter_circle_df = DataFrame(x=x,\n    y=@. sqrt(1 - x^2))\n\nrect = data(line_df) * mapping(:x, :y) * visual(Lines)\nquarter_circle = data(quarter_circle_df) * mapping(:x, :y) * visual(Lines)\ndraw(rect + quarter_circle, axis=(limits=(0, nothing, 0, nothing),))\n```\n\nAs can be seen from the above figure, we know:\n\n1.  The area of the unit square is 1;\n\n2.  The area of the first quadrant of the unit circle is $\\pi / 4$;\n\n3.  Then, if we randomly throw a ball within the unit square, the probability of the event that this ball falls into the area of the first quadrant of the unit circle is $\\pi / 4$. Further, we know that the probability of this event can be estimated by its frequency if we repeat this experiment infinitely many times; therefore, we can estimate the value of $\\pi$ by the following formula:\n\n$$\n\\hat{\\pi} = 4 \\frac{\\text{The number of times falling in }x^2 + y^2 \\leq 1}{\\text{Total number of times}}\n$$\n\n```{julia}\nusing Random, LinearAlgebra, AlgebraOfGraphics, CairoMakie, DataFrames\n\nRandom.seed!(1234)\n\nN = 10^5\ndf = DataFrame([(x=rand(), y=rand()) for _ in 1:N])\ntransform!(df, [:x, :y] => ByRow((x, y) -> ifelse(norm([x, y]) <= 1, \"in\", \"out\")) => :flag)\npi_estimate = 4 * count(df.flag .== \"in\") / N\nprintln(\"π estimate: \", pi_estimate)\n\nfig = Figure()\np = data(df) * mapping(:x, :y, color=:flag) * visual(Scatter, markersize=1)\ndraw!(fig, p, axis=(limits=(0, nothing, 0, nothing),))\nfig\n```\n\n## `Distributions` and related packages for probability distributions\n\n### Introduction\n\nPackages:\n\n- `Statistics` (built-in)\n\n- `StatsBase`\n\n- `Distributions`\n\n#### Weighted vectors\n\nThe `StatsBase` package provides the \"weighted vector\" object via `Weights()`, which allows for an array of values to be given probabilistic weights.\n\nAn alternative of `Weights()` is to use the `Categorical` distribution supplied by the `Distributions` package.\n\nTogether with `Weights()`, you can use the `sample()` function from `StatsBase` to generate observations.\n\n```{julia}\nusing StatsBase, Random\n\nRandom.seed!(1234)\n\ngrades = 'A':'E'\nweights = Weights([0.1, 0.2, 0.1, 0.2, 0.4])\n\nN = 10^6\nd = sample(grades, weights, N)\n[count(i -> i == g, d) for g in grades] / N\n```\n\n#### Distribution type objects\n\nThe `Distributions` package allows us to create distribution type objects based on **what family they belong to**. Then these distribution type objects can be used as arguments for other functions.\n\n```{julia}\nusing Distributions, CairoMakie\n\ndist = TriangularDist(0, 2, 1)  # Triangular distribution\nx = 0:0.01:2\nu = 0:0.01:1\n\nfig = Figure(size=(800, 250))\nlines!(Axis(fig[1, 1], xlabel=\"x\", ylabel=\"f(x)\"), x, pdf.(dist, x))  # PDF\nlines!(Axis(fig[1, 2], xlabel=\"x\", ylabel=\"F(x)\"), x, cdf.(dist, x))  # CDF\nlines!(Axis(fig[1, 3], xlabel=\"u\", ylabel=L\"\\mathbf{F^{-1}(u)}\"), u, quantile.(dist, u))  # ICDF\nfig\n```\n\n```{julia}\nprintln(\"Parameters: \", params(dist))\nprintln(\"Central descriptors: \", mean(dist), \", \", median(dist))\nprintln(\"Dispersion descriptos: \", var(dist), \", \", std(dist))\nprintln(\"Higher-order moment shape descriptors: \", skewness(dist), \", \", kurtosis(dist))\nprintln(\"Range: \", minimum(dist), \", \", maximum(dist))\nprintln(\"Mode: \", mode(dist), \", \", modes(dist))  # Value(s) of x where PMF or PDF is maximized\n```\n\n## Univariate distributions\n\n### Families of discrete distributions\n\n#### Discrete uniform distribution\n\n```{julia}\nusing StatsBase, CairoMakie\n\nfaces, N = 1:6, 10^6\n\nmcEstimate = counts(rand(faces, N), faces) / N  # rand(faces, N) is identical to rand(DiscreteUniform(1, 6), N)\ntheory = [1 / 6 for _ in faces]\n\nfig, ax = stem(faces, mcEstimate, label=\"Estimate\",\n    color=:black, stemcolor=:black,\n    stemwidth=6, markersize=18,\n    axis=(xlabel=\"x\", ylabel=\"f(x)\"))\nstem!(ax, faces, theory, label=\"Theory\",\n    color=:red, stemcolor=:red)\nylims!(ax, nothing, 0.25)\naxislegend(ax)\nfig\n```\n\n#### Binomial distribution\n\n```{julia}\nusing StatsBase, Distributions, CairoMakie\n\nbinomialRV(n, p) = sum(rand(n) .< p)\n\np, n, N = 0.25, 10, 10^6\n\nb_dist = Binomial(n, p)\nx = 0:n\nb_pmf = pdf.(b_dist, x)\nest_data = [binomialRV(n, p) for _ in 1:N]\nest_pmf = counts(est_data, 0:n) / N\n\nfig, ax = stem(x, est_pmf, label=\"Estimate\",\n    color=:black, stemcolor=:black,\n    stemwidth=6, markersize=18,\n    axis=(xlabel=\"x\", ylabel=\"f(x)\"))\nstem!(ax, x, b_pmf, label=\"Theory\",\n    color=:red, stemcolor=:red)\naxislegend(ax)\nfig\n```\n\n#### Geometric distribution\n\nConsider an **infinite** sequence of independent trials, each with sucess probability $p$, and let $X$ be the **first** trial that is successful. Then the PMF is:\n\n$$\nP(X=x) = p(1-p)^{x-1}\n$$\n\nfor $x = 1, 2, ...$.\n\nAn alternative version is to count **the number of failures until success**. Obviously, we have $\\tilde{X} = X - 1$. Then the PMF is:\n\n$$\nP(\\tilde{X} = x) = p(1-p)^x\n$$\n\nfor $x = 0, 1, 2, ...$.\n\nIn `Distributions` package, `Geometric` stands for the distribution of $\\tilde{X}$.\n\n```{julia}\nusing StatsBase, Distributions, CairoMakie\n\nfunction geometricRV(p)\n    x = 0\n    while true\n        if rand() < p\n            return x\n        end\n        x += 1\n    end\nend\n\np = 0.25\nx = 0:25\nN = 10^6\n\ng_dist = Geometric(p)\ng_pmf = pdf.(g_dist, x)\nmcEstimate = counts([geometricRV(p) for _ in 1:N], x) / N\n\nfig, ax = stem(x, mcEstimate, label=\"Estimate\",\n    color=:black, stemcolor=:black,\n    stemwidth=6, markersize=18,\n    axis=(xlabel=\"x\", ylabel=\"f(x)\"))\nstem!(ax, x, g_pmf, label=\"Theory\",\n    color=:red, stemcolor=:red)\naxislegend(ax)\nfig\n```\n\n#### Negative binomial distribution\n\n$X$ stands for **the number of trials** until the $r$-th success. The PMF is:\n\n$$\nP(X=x) = \\binom{x-1}{r-1} p^r (1-p)^{x-r}\n$$\n\nfor $x = r, r+1, r+2, ...$.\n\nSimilarly to the geometric distribution, we usually count **the number of failures** until the $r$-th success. The PMF is:\n\n$$\nP(\\tilde{X} = x) = \\binom{x+r-1}{x} p^r (1-p)^x\n$$\n\nfor $x = 0, 1, 2, ...$.\n\n```{julia}\nusing StatsBase, Distributions, CairoMakie\n\nfunction nbRV(r, p)\n    x = 0\n    success = 0\n    while true\n        if success == r\n            return x\n        end\n        if rand() < p\n            success += 1\n        else\n            x += 1\n        end\n    end\nend\n\nr = 5\np = 0.25\nx = 0:60\nN = 10^6\n\nnb_dist = NegativeBinomial(r, p)\nnb_pmf = pdf.(nb_dist, x)\nmcEstimate = counts([nbRV(r, p) for _ in 1:N], x) / N\n\nfig, ax = stem(x, mcEstimate, label=\"Estimate\",\n    color=:black, stemcolor=:black,\n    stemwidth=6, markersize=18,\n    axis=(xlabel=\"x\", ylabel=\"f(x)\"))\nstem!(ax, x, nb_pmf, label=\"Theory\",\n    color=:red, stemcolor=:red)\naxislegend(ax)\nfig\n```\n\n::: {.callout-note title=\"Summary\"}\n\nSo far, we've seen that binomial distribution, Bernoulli distribution (0-1 distribution, two-point distribution), geometric distribution, and negative binomial distribution all involve **Bernoulli trials** which has exactly **two** possible outcomes, \"success\", and \"failure\", where the probability of success is the same every time the experiment is conducted.\n\nIn a word:\n\n- Binomial distribution ($X \\sim B(n, p)$): $X$ indicates the number of successes in $n$ Bernoulli experiments.\n\n- Bernoulli distribution ($X \\sim B(1, p)$): $X$ indicates the number of successes in $1$ Bernoulli experiments.\n\n- Geometric distribution ($X \\sim Ge(p)$): $X$ indicates the number of total Bernoulli experiments until the **first** success.\n\n- Negative binomial distribution ($X \\sim Nb(r, p)$): $X$ indicates the number of total Bernoulli experiments until the $r$-th success.\n\nObviously, a binomial distribution or a negative binomial distribution can be divided into $n$ Bernoulli distributions or $r$ geometric distributions, respectively.\n\n:::\n\n#### Hypergeometric distribution\n\nHypergeometric distribution means **sampling without replacement**, which means the probability of success changes for each subsequent sample.\n\nThe PMF is:\n\n$$\np(x) = \\frac{\\binom{M}{x} \\binom{N-M}{n-x}}{\\binom{N}{n}}\n$$\n\nfor $x = max(0, n+M-N), ..., min(n, M)$, where $N$ (the population size), $M$ (the number of successes), and $n$ (the sample size) are all parameters.\n\nNote: $max(0, n+M-N)$: if $n \\gt N-M$ (i.e., $n$ is greater than **the number of failures**), then at least $n - (N-M)$ successes must occur.\n\n```{julia}\nusing Distributions, CairoMakie\n\nN, M, n = 500, 100, 60\nx = max(0, n - (N - M)):min(n, M)\n\nh_dist = Hypergeometric(M, N - M, n)  # the 1st is the number of successes; the 2nd is the number of failures; the 3rd is the sample size\nh_pmf = pdf.(h_dist, x)\n\nstem(x, h_pmf,\n    color=:black, stemcolor=:black,\n    axis=(xlabel=\"x\", ylabel=\"f(x)\"))\n```\n\n#### Poisson distribution\n\nThe **Poisson process** is a **stochastic process** (random process) which can be used to model **occurrences of events over time** or more generally in space.\n\nIn a Poisson process, during an infinitesimally small time interval, $\\Delta t$, it is assumed that as $\\Delta t \\rightarrow 0$:\n\n1. There is an occurrence with probability $\\lambda \\Delta t$ and no occurrence with probability $1 - \\lambda \\Delta t$.\n\n2. The chance of 2 or more occurences during an interval of length $\\Delta t$ tends to $0$.\n\nHere, $\\lambda \\gt 0$ is the **intensity** of the Poisson process, and has the property that when multiplied by an interval of length $T$, **the mean occurrences** during the interval is $\\lambda T$.\n\nFor a Poisson process over the time interval $[0, T]$, the Poisson distribution can be used to describe **the number of occurrences**. The PMF is:\n\n$$\nP(x\\text{ Poisson process occurrences during interval }[0, T]) = \\frac{(\\lambda T)^x}{x!} e^{-\\lambda T}\n$$\n\nfor $x = 0, 1, 2, ...$.\n\nWhen the interval is $[0, 1]$, then we have the PMF:\n\n$$\np(x) = \\frac{\\lambda ^x}{x!} e^{-\\lambda}\n$$\n\nfor $x = 0, 1, 2, ...$, where $\\lambda$ is the mean of occurences.\n\nIn addition, **the times between occurrences** in the Poisson process are exponentially distributed.\n\nThe Poisson process has many elegant analytic properties. One such property is to consider the random variable $N \\ge 0$ such that\n\n$$\n\\prod_{i=1}^{N} U_i \\ge e^{-\\lambda} \\gt \\prod_{i=1}^{N+1} U_i\n$$\n\nwhere $U_1, U_2, ...$ is a sequence of i.i.d uniform $(0, 1)$ random variables and $\\prod_{i=1}^{0} \\equiv 1$.\n\nIt turns out that $N$ is Poisson distributed with mean $\\lambda$.\n\n```{julia}\nusing StatsBase, Distributions, CairoMakie\n\nfunction pRV(lambda)\n    N, p = 0, 1\n    while p >= MathConstants.e^(-lambda)\n        N += 1\n        p *= rand()\n    end\n    return N - 1\nend\n\nx = 0:20\nlambda = 5.5\nN = 10^6\n\np_dist = Poisson(lambda)\np_pmf = pdf.(p_dist, x)\n\nmcEstimate = counts([pRV(lambda) for _ in 1:N], x) / N\n\nfig, ax = stem(x, mcEstimate, label=\"Estimate\",\n    color=:black, stemcolor=:black,\n    stemwidth=6, markersize=18,\n    axis=(xlabel=\"x\", ylabel=\"f(x)\"))\nstem!(ax, x, p_pmf, label=\"Theory\",\n    color=:red, stemcolor=:red)\naxislegend(ax)\nfig\n```\n\n### Families of continuous distributions\n\n#### Continuous uniform distribution\n\n```{julia}\nusing Distributions, CairoMakie\n\nx = 0:0.1:2π\nN = 10^6\n\nc_unif_dist = Uniform(0, 2π)\nc_unif_pmf = pdf.(c_unif_dist, x)\nest_data = rand(N) * 2π  # Equivalent to rand(c_unif_dist, N)\n\nfig, ax = stephist(est_data, normalization=:pdf, color=:black, label=\"Estimate\")\nlines!(ax, x, c_unif_pmf, color=:red, label=\"Theory\")\nylims!(ax, nothing, 0.3)\naxislegend(ax)\nfig\n```\n\n#### Exponential distribution\n\nAs mentioned before, the exponential distribution is often used to model random durations between occurrences in the Poisson process.\n\nA **non-negative** random variable $X$, expoentially distributed with a rate parameter $\\lambda$, has PDF:\n\n$$\nf(x) = \\lambda e^{-\\lambda x}\n$$\n\nAs can be verified, the mean is $\\frac{1}{\\lambda}$, the variance is $\\frac{1}{\\lambda ^2}$, and the CCDF is $\\bar{F}(x) = e^{-\\lambda x}$.\n\nIn addition, exponential random variables possess **a lack of memory** property:\n\n$$\nP(X>t+s|X>t) = P(X>s)\n$$\n\nWhile geometric random variables also have such a property, this hints at the fact that **exponential random variables are the continuous analogs of geometric random variables**.\n\n::: {.callout}\n\nSuppose that X is an exponential random variable, and $Y = \\lfloor X \\rfloor$, where $\\lfloor \\cdot \\rfloor$ represents the **floor function**. Then we'll know that $Y$ is a geometric random variable:\n\n$$\np_Y (y) = P(\\lfloor X \\rfloor = y) = \\int_y^{y+1} \\lambda e^{-\\lambda x} \\mathrm{d}x = (e^{-\\lambda})^y (1-e^{-\\lambda})\n$$\n\nfor $y = 0, 1, 2, ...$.\n\nIf we set $p = 1 - e^{-\\lambda}$, then $Y$ is a geometric random variable (representing the number of failures until the first success) which starts at $0$ and has the success parameter $p$.\n\n:::\n\n**Note:** the parameter of `Exponential` is $\\frac{1}{\\lambda}$, instead of $\\lambda$.\n\nExponential distribution:\n\n```{julia}\nusing Distributions, CairoMakie\n\nlambda = 1\nx = 0:0.01:10\n\nexp_dist = Exponential(1 / lambda)\nexp_pmf = pdf.(exp_dist, x)\n\nlines(x, exp_pmf, color=:black)\n```\n\nThe PMF of the floor of an exponential random variable is a geometric distribution:\n\n```{julia}\nusing StatsBase, Distributions, CairoMakie\n\nlambda = 1\nN = 10^6\nx = 0:6\n\nexp_dist = Exponential(1 / lambda)\nfloor_data = counts(convert.(Int, floor.(rand(exp_dist, N))), x) / N\n\ngeom_dist = Geometric(1 - MathConstants.e^-lambda)\n\nfig, ax = stem(x, floor_data, label=\"Floor of Exponential\",\n    color=:black, stemcolor=:black,\n    stemwidth=6, markersize=18,\n    axis=(xlabel=\"x\", ylabel=\"f(x)\"))\nstem!(ax, x, geom_dist, label=\"Geometric\",\n    color=:red, stemcolor=:red)\naxislegend(ax)\nfig\n```\n\n#### Gamma distribution\n\nThe gamma distribution is commonly used to model **asymmetric non-negative** data.\n\n**It generalizes the *exponential distribution* and the *chi-squared distribution*.**\n\nConsider such an example, where **the lifetimes of light bulbs** are **exponentially distributed** with mean $\\frac{1}{\\lambda}$. Now imagine that we are lighting a room continuously with a single light bulb, and that we replace the bulb with a new one when it burns out. If we start at time $0$, what is **the distribution of time** until $n$ bulbs are replaced?\n\nOne way to describe this time is by the random variable $T$, where\n\n$$\nY = X_1 + X_2 + ... + X_n\n$$\n\nand $X_i$ are i.i.d. exponential random variables representing the lifetimes of light bulbs. It turns out that the distribution of $T$ is a gamma distribution.\n\nAt a first glance, this is quite similar with the case, where the random variable of geometric distribution indicates the total number of Bernoulli trials until the first success, while the random variable of negative binomial distribution indicates the total number of Bernoulli trials until the $r$-th success, and we have $Y = X_1 + X_2 + ... + X_r$, where $Y$ is a random variable of negative binomial distribution, and $X_i$ are i.i.d. geometric random variables.\n\nThe PDF of the gamma distribution is proportional to $x^{\\alpha - 1} e^{-\\lambda x}$, where $\\alpha$ is called the **shape parameter**, and $\\lambda$ is called the **rate parameter**.\n\nIn order to normalize $x^{\\alpha - 1} e^{-\\lambda x}$, we need to divide by $\\int_0^\\infty x^{\\alpha -1} e^{-\\lambda x} \\mathrm{d}x$, which can be represented by $\\frac{\\Gamma (\\alpha)}{\\lambda ^\\alpha}$, where $\\Gamma (\\cdot)$ is called the gamma function.\n\nThen, the PDF of the gamma distribution is:\n\n$$\nf(x) = \\frac{\\lambda ^\\alpha}{\\Gamma (\\alpha)} x^{\\alpha - 1} e^{-\\lambda x}\n$$\n\ni.e., $X \\sim Ga(\\alpha, \\lambda)$.\n\nIn the light bulbs case, we have $T \\sim Ga(n, \\lambda)$, where $\\alpha = n$.\n\nIt can also be evaluated that $E[X] = \\frac{\\alpha}{\\lambda}$ and $Var(X) = \\frac{\\alpha}{\\lambda ^2}$.\n\n::: {.callout-note title=\"Squared coefficient of variation\"}\n\n**Squared coefficient of variation** is often used for non-negative random variables:\n\n$$\nSCV = \\frac{Var(X)}{[E(X)]^2}\n$$\n\nThe SCV is a **normalized** or **unit-less** version of the variance.\n\nThe lower it is, the less variability in the random variable.\n\nIt can be seen that for a gamma random variable, the SCV is $\\frac{1}{\\alpha}$.\n\nFor the light bulbs case, the SCV is $\\frac{1}{n}$, which indicates for large $n$, i.e., more light bulbs, there is less variability.\n\n:::\n\n```{julia}\nusing Distributions, CairoMakie\n\nlambda = 1 / 3\nN = 10^6\nbulbs = [1, 10, 50]  # α = 1 is exponential\nx = 0:0.1:20\ncolors = [:blue, :red, :green]\n\n# Theoretical gamma PDFs\n# For each case, we set the rate parameter at λn, so that the mean time until all light bulbs run out is n/(λn) = 1/λ, independent of n\n# For the rate parameter, like Exponential, Gamma also accepts 1/λ, not λ\nga_dists = [Gamma(n, 1 / (n * lambda)) for n in bulbs]\n\n# Generate exponentially distributed pseudorandom numbers by using the inverse probability transformation\nfunction approxBySumExp(dist::Gamma)\n    n = Int64(shape(dist))  # shape() is used to get the shape parameter α\n    [sum(-(1 / (n * lambda)) * log.(rand(n))) for _ in 1:N]  # Generate n exponentially distributed pseudorandom numbers, and then add them up to generate N gamma distributed pseudorandom numbers\nend\n\nest_data = approxBySumExp.(ga_dists)\n\nfig = Figure()\nax = Axis(fig[1, 1])\nfor i in 1:length(bulbs)\n    label = string(\"Shape = \", round(shape(ga_dists[i]), digits=2), \", Scale = \", round(Distributions.scale(ga_dists[i]), digits=2))  # The inverse of the rate parameter is called the scale parameter. Of coourse, you can also use the rate() function to get the rate parameter (λ)\n    stephist!(ax, est_data[i], normalization=:pdf, color=colors[i], label=label, bins=50)\nend\nfor i in 1:length(bulbs)\n    lines!(ax, x, pdf.(ga_dists[i], x), color=colors[i])\nend\nxlims!(ax, 0, 20)\nylims!(ax, 0, 1)\naxislegend(ax)\nfig\n```\n\n**Note:** in the above code, we generate the exponentially distributed pseudorandom numbers by using the **inverse probability transformation**: $F(x) = P(X \\le x) = 1 - e^{-\\lambda x} \\Longrightarrow U = F(X) \\Longrightarrow U = 1 - e^{-\\lambda X} \\Longrightarrow X = -\\frac{1}{\\lambda} \\log(1-U) \\Longrightarrow X = -\\frac{1}{\\lambda} \\log U$ (since we'll use the `rand` function to generate uniformly distributed pseudorandom numbers in $[0, 1]$, it's reasonable that replacing $1-U$ with $U$).\n\n#### Beta distribution\n\nThe beta distribution is a commonly used distribution **when seeking a parameterized shape over a finite support**.\n\nThe PDF is:\n\n$$\nf(x) = \\frac{x^{\\alpha - 1} (1-x)^{\\beta -1}}{B(\\alpha, \\beta)}\n$$\n\nfor $x \\in [0, 1]$. Both $\\alpha$ and $\\beta$ are shape parameters.\n\n```{julia}\nusing Distributions, CairoMakie\n\nx = 0:0.01:1\n\nfig, ax = lines(x, pdf.(Beta(2, 2), x), label=\"α = β = 2\")\nlines!(ax, x, pdf.(Beta(1, 1), x), label=\"α = β = 1\")  # U(0, 1)\naxislegend(ax)\nfig\n```\n\n**Note:** you can use **mathematical special functions** like beta or gamma function calling `beta` or `gamma` provided by the `SpecialFunctions` package. In addition, `QuadGK` provides the `quadgk` function to integrate one-dimensional function.\n\n#### Weibull distribution\n\nFor a random variable $T$, representing **the lifetime of an individual or a component**, an interesting quantity is **the instantaneous chance of failure** at any time, given that the component has been operating **without failure** up to time $x$.\n\nThe instantaneous chance of failure at time $x$ can be expressed as\n\n$$\nh(x) = \\lim_{\\Delta \\to 0} \\frac{1}{\\Delta} P(T \\in [x, x+\\Delta] | T \\gt x)\n$$\n\nAlternatively, by using the conditional probability ($P(T \\in [x, x+\\Delta] | T \\gt x) = \\frac{P(T \\in [x, x+\\Delta])}{P(T \\gt x)} = \\frac{P(T \\in [x, x+\\Delta])}{1 - P(X \\le x)}$) and noticing that the PDF $f(x)$ satisfies $f(x)\\Delta \\approx P(x \\le T \\lt x + \\Delta)$ for small $\\Delta$, we can express $h(x)$ as\n\n$$\nh(x) = \\lim_{\\Delta \\to 0} \\frac{f(x)\\Delta}{\\Delta (1-F(x))} = \\frac{f(x)}{1-F(x)}\n$$\n\nHere the function $h(\\cdot)$ is called **the hazard rate function**, which is often used in **reliability analysis** and **survival analysis**. It's a common method of viewing the distribution for lifetime random variables $T$.\n\nIn fact, we can reconstruct the CDF of $T$ as\n\n$$\nF(x) = 1 - \\exp(-\\int_0^x h(t)\\mathrm{d}t)\n$$\n\nThe **Weibull distribution** is defined through the hazard rate function of the form $h(x) = \\lambda x^{\\alpha - 1}$. Where $\\lambda$ is **positive**, and $\\alpha$ takes on **any real value**.\n\nThe parameter $\\alpha$ gives the Weibull distribution different modes of behavior:\n\n- $\\alpha = 1$: the hazard rate is constant, in which case the Weibull distribution is actually an exponential distribution with rate $\\lambda$.\n\n- $\\alpha > 1$: the hazard rate increases over time. This depicts a situation of \"aging components\", i.e., the longer a components has lived, the higher the instantaneous chance of failure. This is sometimes called **Increasing Failure Rate (IFR)**.\n\n- $\\alpha < 1$: this is an opposite case against $\\alpha > 1$. This is sometimes called **Decreasing Failure Rate (DFR)**.\n\nFor Weibull distribution, we have\n\n$$\nF(x) = 1 - \\exp(-\\int_0^x h(t)\\mathrm{d}t)\n$$\n\nand\n\n$$\nh(x) = \\lambda x^{\\alpha - 1}\n$$\n\nThen the CDF and PDF are\n\n$$\nF(x) = 1 - e^{-\\frac{\\lambda}{\\alpha} x^\\alpha}\n$$\n\nand\n\n$$\nf(x) = \\lambda x^{\\alpha - 1} e^{-\\frac{\\lambda}{\\alpha} x^\\alpha}\n$$\n\nNote that in Julia, the distribution is parameterized via\n\n$$\nf(x) = \\frac{\\alpha}{\\theta} (\\frac{x}{\\theta})^{\\alpha - 1} e^{-(\\frac{x}{\\theta})^\\alpha} = \\alpha \\theta ^{-\\alpha} x^{\\alpha - 1} e^{-\\theta ^{-\\alpha} x^\\alpha}\n$$\n\nwhere the bijection from $\\lambda$ to $\\theta$ is\n\n$$\n\\lambda = \\alpha \\theta ^{-\\alpha}\n$$\n\nand\n\n$$\n\\theta = (\\frac{\\alpha}{\\lambda})^{\\frac{1}{\\alpha}}\n$$\n\nIn this case, $\\theta$ is called the **scale** parameter, and $\\alpha$ is the **shape** parameter.\n\n```{julia}\nusing Distributions, CairoMakie\n\nalphas = [0.5, 1, 1.5]\ngiven_lambda = 2\nx = 0.01:0.01:10\ncolors = [:blue, :red, :green]\n\nactual_lambda(dist::Weibull) = shape(dist) * Distributions.scale(dist)^(-shape(dist))\ntheta(lambda, alpha) = (alpha / lambda)^(1 / alpha)\n\nwb_dists = [Weibull(alpha, theta(given_lambda, alpha)) for alpha in alphas]\n\nhazardA(dist, x) = pdf(dist, x) / ccdf(dist, x)\nhazardB(dist, x) = actual_lambda(dist) * x^(shape(dist) - 1)\n\n# We usually use the hazard rate function to view the Weibull distribution\nhazardsA = [hazardA.(dist, x) for dist in wb_dists]\nhazardsB = [hazardB.(dist, x) for dist in wb_dists]\n\nprintln(\"Maximum difference between two implementations of hazard: \",\n    maximum(maximum.(hazardsA - hazardsB)))\n\nfig = Figure()\nax = Axis(fig[1, 1],\n    xlabel=\"x\",\n    ylabel=\"Instantaneous failure rate\")\nfor i in 1:length(hazardsA)\n    label = string(\"λ = \", round(actual_lambda(wb_dists[i]), digits=2), \", α = \", round(shape(wb_dists[i]), digits=2))\n    lines!(ax, x, hazardsA[i], color=colors[i], label=label)\nend\nxlims!(ax, 0, 10)\nylims!(ax, 0, 10)\naxislegend(ax)\nfig\n```\n\n#### Normal distribution\n\nThe normal distribution (also known as Gaussian distribution) is defined by two parameters, $\\mu$ and $\\sigma ^2$, which are the mean and variance respectively.\n\nThe PDF is\n\n$$\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma ^2}}\n$$\n\nThe normal distribution usually comes with the standard form with $\\mu = 0$ and $\\sigma ^2 = 1$. The PDF is\n\n$$\nf(u) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{u^2}{2}}\n$$\n\nThe CDF of the standard normal distribution (the CDF of the normal distribution is not available as a simple expression) is\n\n$$\n\\Phi (u) = \\int_{-\\infty}^u \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{t^2}{2}} \\mathrm{d}t = \\frac{1}{2} (1 + \\mathrm{erf}(\\frac{x}{\\sqrt{2}})\n$$\n\nwhere $\\mathrm{erf}(\\cdot)$ is a mathematical special function, called **error function**, and defined as\n\n$$\n\\mathrm{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_0^x e^{-t^2} \\mathrm{d}t\n$$\n\nFor any general normal random variable with mean $\\mu$ and variance $\\sigma ^2$, the CDF is available via $\\Phi(\\frac{x-\\mu}{\\sigma})$.\n\n```{julia}\nusing Distributions, Calculus, SpecialFunctions, CairoMakie\n\nx = -5:0.01:5\n\n# erf function from the SpecialFunctions package\nphiA(x) = 0.5 * (1 + erf(x / sqrt(2)))  # Calculate Φ(u) using the error function\nphiB(x) = cdf(Normal(), x)  # Calculate Φ(u)\n\nprintln(\"Maximum difference between two CDF implementations: \",\n    maximum(phiA.(x) - phiB.(x)))\n\nnormalDensity(x) = pdf(Normal(), x)\n\n# Calculate the numerical derivatives from the Calculus package\nd0 = normalDensity.(x)\nd1 = derivative.(normalDensity, x)  # We'll know that x = 0 is the unique extremum\nd2 = second_derivative.(normalDensity, x)  # We'll know x=±1 are two inflection points\n\nfig, ax = lines(x, d0, color=:red, label=\"f(x)\")\nlines!(x, d1, color=:blue, label=\"f'(x)\")\nlines!(x, d2, color=:green, label=\"f''(x)\")\naxislegend(ax)\nfig\n```\n\n#### Rayleigh distribution\n\nConsider an exponentially distributed random variable $X$, with rate parameter $\\lambda = \\frac{\\sigma ^{-2}}{2}$, where $\\sigma > 0$.\n\nLet $R = \\sqrt{X}$, and then we have\n\n$$\nF_R(y) = P(R \\le y) = P(\\sqrt{X} \\le y) = P(X \\le y^2) = F_X(y^2) = 1 - exp(-\\frac{y^2}{2\\sigma ^2})\n$$\n\nand by differentiating, we get\n\n$$\nf_R(y) = \\frac{y}{\\sigma ^2} exp(-\\frac{y^2}{2\\sigma ^2})\n$$\n\nwhich is called the PDF of **Rayleigh distribution**.\n\nThe mean of a Rayleigh random variable is $\\sigma \\sqrt{\\frac{\\pi}{2}}$.\n\nAs mentioned before, we have $U \\sim U(0, 1) \\xrightarrow{X = -\\frac{1}{\\lambda}\\log U} X \\sim Exp(\\lambda) \\xrightarrow{R=\\sqrt{X}, \\lambda = \\frac{\\sigma ^{-2}}{2}} R \\sim Rl(\\sigma)$\n\nIn addition, if $N_1$ and $N_2$ are two i.i.d. **normally distributed** random variables, each with $\\mu = 0$ and std. $\\sigma$, then $\\tilde{R} = \\sqrt{N_1^2 + N_2^2}$ is also Rayleigh distributed just as $R$ above.\n\nTherefore, we have three ways to generate Rayleigh distributed random variables:\n\n```{julia}\nusing Distributions, CairoMakie\n\nN = 10^6\nsigma = 1.5\n\n# U(0, 1) ⟶ Exp(λ) ⟶ Rl(σ)\nrlA = sqrt.(-(2 * sigma^2) * log.(rand(N)))\n\n# From two i.i.d. normally distributed random variables\nnormal_dist = Normal(0, sigma)\nrlB = sqrt.(rand(normal_dist, N) .^ 2 + rand(normal_dist, N) .^ 2)\n\nrlC = rand(Rayleigh(sigma), N)\n\nmean.([rlA, rlB, rlC, sigma * sqrt(π / 2)])\n```\n\nA common way to generate **normal random variables**, called the **Box-Muller Transform**, is to use the relationship $R = \\sqrt{N_1^2 + N_2^2}$.\n\nThe relationship between the pair $(N_1, N_2)$ and their polar coordinate counterpart $(\\theta, R)$ is\n\n$$\n\\begin{cases}\n   N_1 = R\\cos(\\theta) \\\\\n   N_2 = R\\sin(\\theta)\n\\end{cases}\n$$\n\nwhere $\\theta$ is a **uniformly distributed** random variable on $[0, 2\\pi]$, and $R$ is a **Rayleigh distributed** random variable with parameter $\\sigma$.\n\nGiven this, we can first generate $\\theta$ and $R$, and then transform them via the above formula into $N_1$ and $N_2$. Often, $N_2$ is not needed. Hence, in practice, given two independent uniform $(0, 1)$ random variables $U_1$ and $U_2$, we set $Z = \\sqrt{-2\\sigma ^2 \\log U_1} \\cdot \\cos(2\\pi U_2)$. Here $Z$ is a normally distributed random variable with $\\mu = 0$ and std. $\\sigma$.\n\nGenerate $N(0, 1)$:\n\n```{julia}\nusing Distributions, CairoMakie\n\nZ(sigma) = sqrt(-2 * sigma * log(rand())) * cos(2 * pi * rand())\n\nfig, ax = hist([Z(1) for _ in 1:10^6], bins=50,\n    normalization=:pdf, label=\"MC estimate\")\nlines!(-4:0.01:4, pdf.(Normal(), -4:0.01:4),\n    label=\"PDF\", color=:red, linewidth=3)\naxislegend(ax)\nfig\n```\n\n#### Cauchy distribution\n\nThe PDF is\n\n$$\nf(x) = \\frac{1}{\\pi \\gamma (1 + (\\frac{x - x_0}{\\gamma})^2)}\n$$\n\nwhere $x_0$ is the location parameter at which the peak is observed, and $\\gamma$ is the scale parameter.\n\n**Note:** the mean and variance are undefined for Cauchy distribution.\n\n#### Summary\n\n![A brief summay of univariate distributions](./figures/Summary_of_Univariate_Distributions.jpg){.lightbox fig-alt=\"Click to see a larger version of the image\" fig-align=\"center\"}\n\n## Multivariate distributions\n\nConsider $\\mathbf{X} = (X_1, ..., X_n)$ as a random vector with multiple random variables, defined in the same probability space.\n\n### Covarianve and correlation coefficient\n\nCovariance: $Cov(X, Y) = E[(X-\\mu_X)(Y-\\mu_Y)] = E[XY] - \\mu_X \\mu_Y$.\n\nCorrelation coefficient: $\\rho_{XY} = \\frac{Cov(X, Y)}{\\sigma_X \\sigma_Y}$, where $-1 \\le \\rho_{XY} \\le 1$.\n\nThe correlation coefficient is a normalized covariance standing for the **linear** correlation relationship between $X$ and $Y$.\n\n### Expectation vector and covariance matrix\n\nConsider a random vector $X = [X_1, ..., X_n]^\\top$:\n\nThe expectation vector is defined as\n\n$$\n\\mu_{\\mathbf{X}} = [E(X_1), ..., E(X_n)]^\\top\n$$\n\nThe covariance matrix is defined as\n\n$$\n\\Sigma_\\mathbf{X} = Cov(\\mathbf{X}) = E[(\\mathbf{X} - \\mu_\\mathbf{X})(\\mathbf{X} - \\mu_\\mathbf{X})^\\top]\n$$\n\nAs can be verified, the $i,j$-th element of $\\Sigma_\\mathbf{X}$ is $Cov(\\mathbf{X}_i, \\mathbf{X}_j)$, and hence the diagonal elements are the variances.\n\n### Affine transformation\n\nFor any collection of random variables,\n\n$$\nE[X_1+ ... + X_n] = E[X_1] + ... + E[X_n]\n$$\n\nFor **uncorrelated** random variables,\n\n$$\nVar(X_1 + ... + X_n) = \\sum_{i} Var(X_i)\n$$\n\nMore generally, if we allow the random variables to be correlated, then\n\n$$\nVar(X_1 + ... + X_n) = \\sum_{i} Var(X_i) + 2\\sum_{i < j} Cov(X_i, X_j)\n$$\n\nObviously, the right-hand side is the sum of the elements of the matrix $Cov(\\mathbf{X})$.\n\nThe above is a special case of the affine transformation, where we take a random vector $\\mathbf{X} = [X_1, ..., X_n]^\\top$ with covariance matrix $\\Sigma_\\mathbf{X}$, and an $m \\times n$ matrix $\\mathbf{A}$ and $m$ vector $\\mathbf{b}$. We then set\n\n$$\n\\mathbf{Y} = \\mathbf{A}\\mathbf{X} + \\mathbf{b}\n$$\n\nThen, the new random vector $\\mathbf{Y}$ has expectation and covariance\n\n$$\nE[\\mathbf{Y}] = \\mathbf{A}E[\\mathbf{X}] + \\mathbf{b}\\ \\ \\ \\ \\text{and}\\ \\ \\ \\ Cov(\\mathbf{Y}) = \\mathbf{A}\\Sigma_\\mathbf{X}\\mathbf{A}^\\top\n$$\n\nThe above case can be retrieved by setting $\\mathbf{A} = [1, ..., 1]$, and $\\mathbf{b} = \\mathbf{0}$.\n\n### The Cholesky decomposition and generating random vectors\n\nNow we want to create an $n$-dimensional random vector $\\mathbf{Y}$ with some specified expectation vector $\\mu_\\mathbf{Y}$ and covariance matrix $\\Sigma_\\mathbf{Y}$, which are known.\n\nFirst, we can generate a random vector $\\mathbf{X}$ with $\\mu_\\mathbf{X} = \\mathbf{0}$ and identity-covariance matrix $\\Sigma_\\mathbf{X} = \\mathbf{I}$ (e.g., a sequence of $n$ i.i.d. N(0, 1) random variables).\n\nThen, by applying the affine transformation $\\mathbf{Y} = \\mathbf{A}\\mathbf{X} + \\mathbf{b}$, we have $\\mu_\\mathbf{Y} = \\mathbf{b}$ and a matrix $\\mathbf{A}$ which satisfies $\\Sigma_\\mathbf{Y} = \\mathbf{A}\\mathbf{A}^\\top$. The Cholesky decomposition will help us get $\\mathbf{A}$ from $\\Sigma_\\mathbf{Y} = \\mathbf{A}\\mathbf{A}^\\top$.\n\n```{julia}\nusing Distributions, LinearAlgebra, Random, CairoMakie\n\nRandom.seed!(1)\n\nN = 10^5\n\nmuY = [15; 20]\nSigY = [6 4; 4 9]\n\nA = cholesky(SigY).L  # The Cholesky decomposition; get the lower triangular form\n\nrngGens = [() -> rand(Normal()),\n    () -> rand(Uniform(-sqrt(3), sqrt(3))),\n    () -> rand(Exponential()) - 1]  # Expectation 0; variance 1\n\nlabels = [\"Normal\", \"Uniform\", \"Exponential\"]\ncolors = [:blue, :red, :green]\n\nrv(rng) = A * [rng(), rng()] + muY\n\nds = [[rv(rng) for _ in 1:N] for rng in rngGens]\n\nprintln(\"E1\\tE2\\tVar1\\tVar2\\tCov\")\nfor d in ds\n    println(round(mean(first.(d)), digits=2), \"\\t\", round(mean(last.(d)), digits=2), \"\\t\",\n        round(var(first.(d)), digits=2), \"\\t\", round(var(last.(d)), digits=2), \"\\t\",\n        round(cov(first.(d), last.(d)), digits=2))\nend\n\nfig = Figure()\nax = Axis(fig[1, 1],\n    xlabel=L\"X_1\",\n    ylabel=L\"X_2\")\nfor i in 1:length(ds)\n    scatter!(ax, first.(ds[i]), last.(ds[i]), color=colors[i], label=labels[i], markersize=2)\nend\naxislegend(ax, position=:rb)\nfig\n```\n\n### Bivariate normal distribution\n\n$$\n\\mu_\\mathbf{XY} = \\left[\\begin{matrix} \\mu_\\mathbf{X} \\\\ \\mu_\\mathbf{Y} \\end{matrix}\\right]\n$$\n\n$$\n\\Sigma_\\mathbf{XY} = \\left[\\begin{matrix} \\sigma_\\mathbf{X}^2 & \\sigma_\\mathbf{X}\\sigma_\\mathbf{Y}\\rho \\\\ \\sigma_\\mathbf{X}\\sigma_\\mathbf{Y}\\rho & \\sigma_\\mathbf{Y}^2\\end{matrix} \\right]\n$$\n\n```{julia}\nusing Distributions, CairoMakie\n\nmeanVect = [27.1554, 26.1638]\ncovMat = [16.1254 13.047; 13.047 12.3673]\n\nbiNorm = MvNormal(meanVect, covMat)  # Multivariate normal distribution\n\nN = 10^3\npoints = rand(biNorm, N)\n\nsupport = 15:0.5:40\nz = [pdf(biNorm, [x, y]) for x in support, y in support]\n\nfig = Figure(size=(900, 400))\nax2 = Axis(fig[1, 1],\n    xlabel=\"x\",\n    ylabel=\"y\")\nscatter!(ax2, points[1, :], points[2, :], markersize=4, color=:black)\ncontour!(support, support, z, levels=[0.001, 0.005, 0.02], color=:red, linewidth=2)\nax3 = Axis3(fig[1, 2],\n    xlabel=\"x\",\n    ylabel=\"y\",\n    zlabel=\"z\")\nsurface!(support, support, z)\ncolsize!(fig.layout, 1, Auto(0.65))\nfig\n```\n\n## Processing and summarizing data\n\n### Processing data\n\nData cleaning.\n\n### Summarizing data\n\nDescriptive statistics.\n\n#### Single sample\n\nGiven a set of observations $x_1, x_2, ..., x_n$.\n\n1. **Sample mean**: measure of centrality.\n\n- Arithmetic mean:\n\n$$\n\\bar{x} = \\frac{\\displaystyle\\sum_{i=1}^{n} x_i}{n}\n$$\n\n- Geometric mean:\n\n$$\n\\bar{x}_g = \\sqrt[n]{\\displaystyle\\prod_{i=1}^n x_i}\n$$\n\nUseful for **averaging growth factors**.\n\ne.g., if we start with an original base level say $L$ with growths of $x_1$, $x_2$, and $x_3$ in three consecutive periods, then after three periods, we have\n\n$$\n\\text{Value after three periods} = L\\cdot x_1\\cdot x_2\\cdot x_3 = L\\cdot \\bar{x}_g^3\n$$\n\nHere, the average growth factor is $\\bar{x}_g$.\n\n- Harmonic mean:\n\n$$\n\\bar{x}_h = \\frac{n}{\\displaystyle\\sum_{i=1}^n \\frac{1}{x_i}}\n$$\n\nUseful for **averaging rates or speeds**.\n\nAssume that you are on a brisk hike, walking $5$ km up a mountain and then $5$ km back down.\n\nSay your speed going up is $x_1 = 5 \\text{ km/h}$, and your speed going down is $x_2 = 10 \\text{ km/h}$.\n\nYou travel up for $1$ h, and down for $0.5$ h and hence your total travel time is $1.5$ h.\n\nWhat is your average speed for the whole journey?\n\nThe avearge speed shoud be $\\frac{10 \\text{ km}}{1.5 \\text{ h}} = 6.6\\bar{6} \\text{ km/h}$.\n\nThis is not the arithmetic mean which is $\\frac{x_1 + x_2}{2} = \\frac{5 \\text{ km/h}+ 10 \\text{ km/h}}{2} = 7.5 \\text{ km/h}$ but rather equals the harmonic mean.\n\n2. Variance: a measure of dispersion.\n\n- Sample variance: the dispersion degree of sample observations away from the sample mean.\n\n$$\ns^2 = \\frac{\\displaystyle\\sum_{i=1}^n (x_i - \\bar{x})^2}{n-1} = \\frac{\\displaystyle\\sum_{i=1}^n x_i^2 - n\\bar{x}^2}{n-1}\n$$\n\nNote that the denominator is $n-1$ instead of $n$, which is the population variance ($s^2$ defined in the above way is an unbiased estimator of the population variance).\n\n- Sample standard deviation: $s = \\sqrt{s^2}$.\n\n- Standard error: $\\frac{s}{\\sqrt{n}}$ (the dispersion degree of sample mean away from the population mean).\n\nAnother breed of descriptive statistics is based on order statistics. This term is used to describe the sorted sample, denoted by\n\n$x_{(1)} \\le x_{(2)} \\le ... \\le x_{(n)}$\n\nBased on the order statistics, we can define a variety of statistics.\n\n1. minimum: $x_{(1)}$.\n\n2. maximum: $x_{(n)}$.\n\n3. median: which in case of $n$ being odd is $x_{(\\frac{n+1}{2})}$; in case of $n$ being even is the arithmetic mean of $x_{(\\frac{n}{2})}$ and $x_{(\\frac{n}{2} + 1)}$. A measure of centrality. It is not influenced by very high or very low measurements.\n\n4. $\\alpha$-quantile: which is $x_{(\\widetilde{\\alpha n})}$, where $\\widetilde{\\alpha n}$ denotes a rounding of $\\alpha n$ to the nearest element of $\\{1, ..., n\\}$.\n\n$\\alpha = 0.25$ and $\\alpha = 0.75$ is called the first quantile and the third quantile, the difference of which is called the inter-quantile range (IQR), which is a measure of dispersion.\n\n5. range: $x_{(n)} - x_{(1)}$, which is also a measure of dispersion.\n\nA measure of centrality: mean (arithmetic mean, geometric mean, harmonic mean), median (i.e., $0.5$-quantile).\n\nA measure of dispersion: variance (sample variance, sample standard deviation, standard error), IQR, range.\n\nIn Julia, packages `Statistics` together with `StatsBase` is usually used to perform descriptive statistics:\n\n```{julia}\nusing Statistics, StatsBase, Distributions\n\nd = rand(Exponential(1 / 2), 10^6)\n\nprintln(\"Sample arithmetic mean, sample geometric mean, sample harmonic mean: \", (mean(d), geomean(d), harmmean(d)))\nprintln(\"Sample variance, sample standard deviation, sample standard error: \", (var(d), std(d), sem(d)))\nprintln(\"Minimum, maximum, range: \", (minimum(d), maximum(d), maximum(d) - minimum(d)))\nprintln(\"95th percentile, 0.95 quantile, IQR: \", (percentile(d, 95), quantile(d, 0.95), iqr(d)), \"\\n\")\n\nsummarystats(d)\n```\n\n#### Observations in pairs\n\nWhen data is configured in the form of pairs, $(x_1, y_1), ..., (x_n, y_n)$, we often consider the (1) **sample covariance**\n\n$$\n\\widehat{cov}_{x,y} = \\frac{\\displaystyle\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{n-1}\n$$\n\nor its normalized form - (2) **correlation coefficient** (Pearson correlation coefficient)\n\n$$\n\\hat{\\rho}_{x,y} = \\frac{\\widehat{cov}_{x,y}}{s_x s_y}\n$$\n\nWe often represent the variances and covariances in the (3) **sample covariance matrix**\n\n$$\n\\hat{\\Sigma} = \\left[ \\begin{matrix} \\widehat{cov}_{x,x} & \\widehat{cov}_{x,y} \\\\ \\widehat{cov}_{x,y} & \\widehat{cov}_{y,y} \\end{matrix} \\right] = \\left[ \\begin{matrix} s_x^2 & \\hat{\\rho}_{x,y} s_x s_y \\\\ \\hat{\\rho}_{x,y} s_x s_y & s_y^2 \\end{matrix} \\right]\n$$\n\n```{julia}\nusing CSV, DataFrames, Statistics\n\nd = CSV.read(\"./data/temperatures.csv\", DataFrame)\n\nx = d.Brisbane\ny = d.GoldCoast\n\ncovXY = cov(x, y)\nsigX = std(x)\nsigY = std(y)\nrhoXY = covXY / (sigX * sigY)\n\nprintln(\"covXY: \", covXY, \"\\n\",\n    \"sigX: \", sigX, \"\\n\",\n    \"sigY: \", sigY, \"\\n\",\n    \"rhoXY: \", rhoXY)\n\nmeanVect = [mean(x), mean(y)]\ncovMat = [sigX^2 covXY\n    covXY sigY^2]\n\nprintln(\"meanVect: \", meanVect)\nprintln(\"covMat: \", covMat)\n```\n\n#### Observations in vectors\n\nThe data is represented by an $n\\times p$ matrix, $\\mathbf{X}$, where **the rows are observations** and **the columns are features**.\n\n$$\n\\mathbf{X} = [\\mathbf{X_1}, ..., \\mathbf{X_p}]\n$$\n\n$\\mathbf{X_j}$ represents the $j$-th feature.\n\nBasically, we can summarize the data matrix $\\mathbf{X}$ by these statistics:\n\n- Sample mean vector\n\n$$\n\\bar{\\mathbf{x}} = [\\bar{x}_1, ..., \\bar{x}_p]^\\top\n$$\n\n- Sample standard deviation vector\n\n$$\n\\mathbf{s} = [s_1, ..., s_p]^\\top\n$$\n\nWith these two statistics, we often standardize or normalize the data by creating a new $n\\times p$ matrix $\\mathbf{Z}$ with entries\n\n$$\nz_{ij} = \\frac{x_{ij} - \\bar{x}_j}{s_j}, i = 1, ..., n,\\ \\ j = 1, ..., p\n$$\n\nalso called z-scores.\n\nThe normalized data has the attribute that each column has a $0$ sample mean and a unit standard deviation. Hence the first- and second-order information of the $j$-th feature is lost when moving from $\\mathbf{X}$ to $\\mathbf{Z}$.\n\nIt can be created via\n\n$$\n\\mathbf{Z} = (\\mathbf{X} - \\mathbf{1}\\mathbf{\\bar{x}}^\\top)diag(\\mathbf{s})^{-1}\n$$\n\nwhere $diag(\\cdot)$ creates a diagonal matrix from a vector by using the `Diagonal` function, and then get the inverse matrix by using the grammar `D^-1`, both of which are from the `LinearAlgebra` package.\n\nIn Julia this can be calculated using the `zscore` function.\n\n- Sample covariance matrix\n\n$$\n\\begin{align}\n\\hat{\\Sigma} & = \\frac{1}{n-1} (\\mathbf{X} - \\mathbf{1}\\mathbf{\\bar{x}}^\\top)^\\top (\\mathbf{X} - \\mathbf{1}\\mathbf{\\bar{x}}^\\top) \\\\\n             & = \\frac{1}{n-1} \\mathbf{X}^\\top (\\mathbf{I} - n^{-1} \\mathbf{1} \\mathbf{1}^\\top) \\mathbf{X}\n\\end{align}\n$$\n\n- Sample correlation matrix\n\nThe following only picks two columns called $x$ and $y$ to perform deduction:\n\n$$\n\\begin{align}\n\\hat{\\rho}_{x,y} & = \\frac{\\widehat{cov}_{x,y}}{s_x s_y} \\\\\n                 & = \\frac{\\displaystyle\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{(n-1) s_x s_y} \\\\\n                 & = \\frac{1}{n-1} \\displaystyle\\sum_{i=1}^n \\frac{x_i - \\bar{x}}{s_x} \\cdot \\frac{y_i - \\bar{y}}{s_y} \\\\\n                 & = \\frac{1}{n-1} \\displaystyle\\sum_{i=1}^n z_{ix} z_{iy} \\\\\n                 & = \\frac{1}{n-1} \\mathbf{Z}^\\top \\mathbf{Z}\n\\end{align}\n$$\n\nIn julia this can be performed via the `cor` function.\n\n```{julia}\nusing Statistics, StatsBase, LinearAlgebra, DataFrames, CSV\n\ndf = CSV.read(\"./data/3featureData.csv\", DataFrame, header=false)\nprintln(df, \"\\n\")\n\nX = Matrix(df)\nprintln(X, \"\\n\")\n\nn, p = size(X)\n\n# Sample mean vector\nxbarA = X' * ones(n) / n\nxbarB = [mean(X[:, j]) for j in 1:p]\nxbarC = sum(X, dims=1) / n\nprintln(\"Sample mean vector: \", \"\\n\", xbarA, \"\\n\", xbarB, \"\\n\", xbarC, \"\\n\")\n\n# Sample standard deviation vector\nsA = [std(X[:, j]) for j in 1:p]\nsB = std(X, dims=1)\nprintln(\"Sample standard deviation vector: \", \"\\n\", sA, \"\\n\", sB, \"\\n\")\n\nxbar = xbarB\ns = sA\n\n# Z-scores matrix\nZA = [((X[i, j] - mean(X[:, j])) / std(X[:, j])) for i in 1:n, j in 1:p]\nZB = (X - ones(n) * xbar') * Diagonal(s)^-1\nZC = hcat([zscore(X[:, j]) for j in 1:p]...)\nprintln(\"Z-scores matrix: \", \"\\n\", ZA, \"\\n\", ZB, \"\\n\", ZC, \"\\n\")\n\n# Sample covariance matrix\ncovA = (X - ones(n) * xbar')' * (X - ones(n) * xbar') / (n - 1)\ncovB = X' * (I - ones(n, n) / n) * X / (n - 1)\ncovC = [cov(X[:, i], X[:, j]) for i in 1:p, j in 1:p]\ncovD = [cor(X[:, i], X[:, j]) * std(X[:, i]) * std(X[:, j]) for i in 1:p, j in 1:p]\ncovE = cov(X)\nprintln(\"Sample covariance matrix: \", \"\\n\", covA, \"\\n\", covB, \"\\n\", covC, \"\\n\", covD, \"\\n\", covE, \"\\n\")\n\nZMat = ZC\n\n# Sample correlation coefficient matrix\ncorA = cov(X) ./ [std(X[:, i]) * std(X[:, j]) for i in 1:p, j in 1:p]\ncorB = cov(X) ./ (std(X, dims=1)' * std(X, dims=1))\ncorC = [cor(X[:, i], X[:, j]) for i in 1:p, j in 1:p]\ncorD = ZMat' * ZMat / (n - 1)\ncorE = cov(ZMat)\ncorF = cor(X)\nprintln(\"Sample correlation coefficient matrix: \", \"\\n\", corA, \"\\n\", corB, \"\\n\", corC, \"\\n\", corD, \"\\n\", corE, \"\\n\", corF, \"\\n\")\n```\n\n### Plots for single samples and time series\n\nHere, we focus on a single collection of observations, $x_1, ..., x_n$.\n\nIf the observations are obtained by **randomly sampling a population**, then **the order of the observations is inconsequential**.\n\nIf the observations represent measurement **over time** then we call the data **time-series**, and in this case, plotting the observations one after another is the standard way for considering temporal patterns in the data.\n\n#### Histograms\n\nConsidering frequencies of occurrences.\n\nFirst denote the support of the observations via $[l, m]$, where $l$ is the minimal observation and $m$ is the maximal observation.\n\nThen the interval $[l, m]$ is partitioned into a finite set of bins $B_1, ..., B_L$, and the frequency in each bin is recorded via\n\n$$\nf_j = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}{\\{x_i \\in B_j}\\}, \\ \\ \\ \\ \\text{for}\\ j = 1, ..., L\n$$\n\nHere $\\mathbf{1}\\{\\cdot\\}$ is $1$ for $x_i \\in \\B_j$, or $0$ if not.\n\nWe have that $\\sum f_j = 1$, and hence $f_i, ..., f_L$ is a PMF.\n\nA histogram is then just a visual representation of PMF. One way to plot the frequencies is via a **stem plot**. However, such a plot would not represent **the widths of the bins**. Instead we plot $h(x)$ defined as\n\n$$\nh(x) = \\sum_{j=1}^L \\frac{f_j}{|B_j|} \\mathbf{1}{\\{x_i \\in B_j}\\}\n$$\n\nwhere $|B_j|$ is the width of bin $j$. Hence $h(x)$ is actually a PDF.\n\nIn a word, calculate frequencies of occurrences in each bin $\\implies$ PMF; further normalized by bin widths $\\implies$ PDF.\n\n```{julia}\nusing Distributions, CairoMakie\n\nn = 2000\nd = rand(Normal(), n)\n\n# PMF\nfig, ax = hist(d, bins=20, normalization=:probability, color=:purple, label=\"PMF\")\n# PDF\nstephist!(ax, d, bins=20, normalization=:pdf, color=:red, label=\"PDF\")\naxislegend(ax)\nfig\n```\n\n#### Density plots and kernel density estimation\n\nA more modern and visually applealing alternative to histograms is the **smoothed histogram**, also known as a **density plot**, often generated via a **kernel density estimate**.\n\n1. Mixture model\n\nGenerating observations from a **mixture model** means that we sample from populations made up of heterogeneous sub-populations.\n\nEach sub-population has its own probability distribution and these are \"mixed\" in the process of sampling.\n\nAt first, a latent (un-observed) random variable determines which sub-population is used, and then a sample is taken from that sub-population.\n\nThat is if the $M$ sub-populations have densities $g_1(x), ..., g_M(x)$ with weights $p_1, ..., p_M$, and $\\sum p_i = 1$, then the density of the mixture is\n\n$$\nf(x) = \\sum_{i=1}^M p_i g_i(x)\n$$\n\n2. Kernel density estimate\n\nGiven a set of observations, $x_1, ..., x_n$, the KDE is the function\n\n$$\n\\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h} K(\\frac{x-x_i}{h})\n$$\n\nwhere $K(\\cdot)$ is some specified kernel function and $h > 0$ is the bandwidth parameter.\n\nThe kernel function is a function that satisfies the properties of a PDF. A typical example is the Gaussian kernel\n\n$$\nK(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}\n$$\n\nWith such a kernel, the estimate $\\hat{f}(x)$ is a PDF because it is a weighted superposition of scaled kernel fucntions centered about each of the observations.\n\nA very small bandwidth implies that the density\n\n$$\n\\frac{1}{h} K(\\frac{x-x_i}{h})\n$$\n\nis very concentrated around $x_i$.\n\nFor any value of $h$, it can be proved under general conditions that if the data is distributed according to some density $f(\\cdot)$, then $\\hat{f}(\\cdot)$ converges to $f(\\cdot)$ when the sample size grows.\n\n```{julia}\nusing Distributions, CairoMakie\n\nmu1, sigma1 = 10, 5\nmu2, sigma2 = 40, 12\n\ndist1, dist2 = Normal(mu1, sigma1), Normal(mu2, sigma2)\nmixRV(p) = (rand() <= p) ? rand(dist1) : rand(dist2)\n\nn = 2000\nd = [mixRV(0.3) for _ in 1:n]\n\n# PMF\nfig, ax = hist(d, bins=20, normalization=:probability, color=:skyblue, label=\"PMF\")\n# PDF\nstephist!(ax, d, bins=20, normalization=:pdf, color=:red, label=\"PDF\")\n# Smoothed PDF\ndensity!(ax, d, color=(:white, 0), label=\"Smoothed PDF\", strokecolor=:green, strokewidth=2)\naxislegend(ax)\nfig\n```\n\nIn a word, the KDE is a useful way to estimate the PDF of the unknown underlying distribution given some sample data.\n\n#### Empirical cumulative distribution function\n\nThe *Empirical Cumulative Distribution Function* (ECDF) can be viewed as an estimate of the underlying CDF.\n\nIn contrast to histograms and KDEs, ECDFs provide an unique representation of the data independent of tuning parameters.\n\nThe ECDF is a stepped function which, given $n$ data points, increases by $\\frac{1}{n}$ at each point.\n\nMathematically, given the sample, $x_1, ..., x_n$, the ECDF is given by\n\n$$\n\\hat{F}(t) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1} \\{x_i \\le t\\}\\ \\ \\ \\ \\text{where }\\mathbf{1}\\text{ is the indicator function}\n$$\n\nIn the case of i.i.d. data from an underlying distribution with CDF $F(\\cdot)$, the Glivenko-Cantelli theorem ensures that the ECDF $\\hat{F}(\\cdot)$ approaches $F(\\cdot)$ as the sample size grows.\n\n```{julia}\nusing Distributions, StatsBase, CairoMakie\n\nmu1, sigma1 = 10, 5\nmu2, sigma2 = 40, 12\n\ndist1, dist2 = Normal(mu1, sigma1), Normal(mu2, sigma2)\n\np = 0.3\n\nmixRV(p) = (rand() <= p) ? rand(dist1) : rand(dist2)\nmixCDF(x) = p * cdf(dist1, x) + (1 - p) * cdf(dist2, x)\n\nn = [30, 100]\n\ndata1 = [mixRV(p) for _ in 1:n[1]]\ndata2 = [mixRV(p) for _ in 1:n[2]]\n\nempiricalCDF1 = ecdf(data1)\nempiricalCDF2 = ecdf(data2)\n\nx = -10:0.1:80\n\nfig, ax = lines(x, empiricalCDF1, label=\"ECDF with n = $(n[1])\")\nlines!(x, empiricalCDF2, label=\"ECDF with n = $(n[2])\")\nlines!(x, mixCDF.(x), label=\"Underlying CDF\")\naxislegend(ax, position=:lt)\nfig\n```\n\n#### Normal probability plot\n\nSee @sec-qqplot for details.\n\n#### Visualizing time series\n\nIn cases where the time-series data appears to be **stationary** (a stationary sequence is one in which the distributional law of observations doesn't depend on the exact time. This means that there isn't an apparent trend nor a cyclic component.), then a histogram is immediately insightful; otherwise, plotting data points one after the other along the time axis is necessary.\n\n```{julia}\nusing DataFrames, CSV, Dates, CairoMakie\n\nd = CSV.read(\"./data/temperatures.csv\", DataFrame)\nbrisbane = d.Brisbane\ngoldcoast = d.GoldCoast\n\ndiff = brisbane - goldcoast\ndates = string.([Date(Year(d.Year[i]),\n    Month(d.Month[i]),\n    Day(d.Day[i]))\n                 for i in 1:nrow(d)])\n\nfortnight_range = 250:263\ndate_fortnight = dates[fortnight_range]\nbris_fortnight = brisbane[fortnight_range]\ngold_fortnight = goldcoast[fortnight_range]\n\nfig = Figure(size=(1100, 900))\n\nax1_slice_indexes = [1, 389, 777]\nax1 = Axis(fig[1, 1],\n    xlabel=\"Time\",\n    ylabel=\"Temperature\",\n    xticks=(ax1_slice_indexes, dates[ax1_slice_indexes]))\nseries!(ax1, stack(zip(brisbane, goldcoast)))\naxislegend(ax1, position=:rb)\n\nax2_slice_indexes = [1, 7, 14]\nax2 = Axis(fig[2, 1],\n    xlabel=\"Time\",\n    ylabel=\"Temperature\",\n    xticks=(ax2_slice_indexes, date_fortnight[ax2_slice_indexes]))\nseries!(ax2, stack(zip(bris_fortnight, gold_fortnight)))\nscatter!(1:length(date_fortnight), bris_fortnight)\nscatter!(1:length(date_fortnight), gold_fortnight)\naxislegend(ax2, position=:lb)\n\nax3_slice_indexes = [1, 389, 777]\nax3 = Axis(fig[3, 1],\n    xlabel=\"Time\",\n    ylabel=\"Temperature Difference\",\n    xticks=(ax3_slice_indexes, dates[ax3_slice_indexes]))\nseries!(ax3, reshape(diff, 1, length(diff)))\n\nax4 = Axis(fig[4, 1],\n    xlabel=\"Temperature Difference\",\n    ylabel=\"Frequency\")\nhist!(ax4, diff, bins=50)\n\nfig\n```\n\n#### Radial plot\n\nRadial plot is useful for presenting **time-series** or **cyclic** data.\n\nA variation of radial plot is the radar plot, which is often used to visualize the levels of different **categorical** variables on the one plot.\n\n```{julia}\nusing DataFrames, CSV, Dates, CairoMakie\n\nd = CSV.read(\"./data/temperatures.csv\", DataFrame)\nsubset!(d, :Year => x -> x .== 2015)\nbrisbane = d.Brisbane\ngoldcoast = d.GoldCoast\n\ndates = [Date(Year(d.Year[i]),\n    Month(d.Month[i]),\n    Day(d.Day[i]))\n         for i in 1:nrow(d)]\n\nx = 0:2pi/(length(brisbane)-1):2pi |> collect\nax_slice_indexes = [findfirst(Dates.month.(dates) .== m) for m in 1:12]\n\nfig = Figure(size=(600, 600))\nax = PolarAxis(fig[1, 1],\n    thetaticks=(x[ax_slice_indexes], Dates.monthabbr.(1:12)))\nseries!(ax, x, [brisbane goldcoast]')\n\nfig\n```\n\n### Plots for comparing two or more samples\n\n#### Quantile-Quantile (Q-Q) plot {#sec-qqplot}\n\nThe Q-Q plot checks if the distributional shape of two samples is the same or not.\n\nFor this plot, we require that the sample sizes are the same.\n\nThen the **ranked** quantiles of the first sample are plotted against the **ranked** quantiles of the second sample.\n\nIn the case where the samples have a similar distributional shape, the resulting plot appears like a collection of increasing points along a straight line.\n\n具体原理解释如下：\n\n给定一列数据 $x_1, ..., x_n$，假定其服从正态分布。现取一个正态分布作为模板，将其 PDF 下的面积等分成 $n$ 份，即每一块区域代表的概率都是相等的，都是 $\\frac{1}{n}$。如果现在要从这个正态分布中抽取一个随机数，在理想情况下，这个数出现在任何一个小区域内的概率都是相等的。也就是说，在该正态分布被分成 $n$ 等份后，如果我们要从其中抽出 $n$ 个随机数，在理想情况下，应该是刚好每个小区间都被抽出了一个数，并且我们预期这些数应该是每个小区间的中位数（二分位数）。\n\n在下图中，$n = 10$：\n\n```{julia}\nusing Distributions, CairoMakie\n\nd = Normal()\n\nn = 10\nx = -4:0.01:4\ny = pdf.(d, x)\nq = quantile.(d, collect(1/n:1/n:(n-1)/n))\n\nfig, ax = lines(x, y, color=:black)\nvlines!(ax, q, color=:red)\nfig\n```\n\n对于 $n = 10$ 来说，累积概率分位数分隔点分别为 $0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9$（$\\frac{1}{n}, \\frac{2}{n}, ..., \\frac{n-1}{n}$），对应的每个小区间的累积概率二分位数应为 $0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95$（$\\frac{i-0.5}{n}\\ \\ \\ \\ \\text{for}\\ i = 1,2, .., n$），再利用公式 $\\Phi^{-1}\\left(\\frac{i-0.5}{n}\\right)\\ \\ \\ \\ \\text{for}\\ i = 1,2, .., n$ 得到每个小区间相应的二分位数值。\n\n在理想情况下，**排完序**的实际观测值 $x_1, ..., x_n$ 应该和上述 $n$ 个二分位数值一致，即以实际值作为纵轴，理论值作为横轴，画出的这些点应该位于斜线 $y = x$ 上。\n\n值得注意的是，取理论分位数值这一步有很多方法，除了等分概率分布取二分位数之外，也有直接将概率分布等分为 $n+1$ 份，直接取对应的 $n$ 个分位数即可。\n\n```{julia}\nusing Random, Distributions, CairoMakie, Statistics\n\nRandom.seed!(1234)\n\nn = 2000\nmu, sigma = 10, 1\nrank = collect(1:2000)\n\nd = Normal(mu, sigma)\n\nempirical_data = rand(d, n) |> sort\ntheoretical_data = quantile.(Normal(), @. (rank - 0.5) / n)\n\n# x = σU + μ\nfig, ax = scatter(theoretical_data, empirical_data, color=:steelblue)\nablines!(ax, mu, sigma, color=:red)\nqqnorm!(Axis(fig[2, 1]), empirical_data, qqline=:fitrobust, color=:red, markercolor=:steelblue)\nfig\n```\n\nIf you want roughly to see if the two given data sets $x_1, ..., x_n$, and $y_1, ..., y_n$ have exactly the same distributional shape, you can just do the following:\n\n```{julia}\nusing Distributions, CairoMakie, Random\n\nRandom.seed!(1)\n\n# Plot the ranked quantiles of x against the ranked quantiles of y\nx = randn(2000) |> sort\ny = randn(2000) |> sort\n\nfig, ax = scatter(x, y, color=:steelblue)\nablines!(0, 1, color=:red)\nqqplot!(Axis(fig[2, 1]), x, y, qqline=:identity, color=:red, markercolor=:steelblue)\nfig\n```\n\n#### Box plot\n\nThe box plot, also known as a box and whisker plot, which displays the **first** and the **third** quantiles along with the **median**. The location of the **whiskers** is typically given by\n\n$$\n\\text{minimum} = Q1 - 1.5 IQR\\ \\ \\text{,}\\ \\ \\text{maximum} = Q3 + 1.5 IQR\n$$\n\nwhere IQR is the inter-quantile range. Observations that lie outside this range are called outliers.\n\n```{julia}\nusing CairoMakie\n\ncategories = repeat(1:3, outer=300)\nv = randn(900)\n\n# notch is used to test the significance of the difference between two medians under the 0.95 confidence interval\nboxplot(categories, v, show_notch=true, color=:cyan)\n```\n\n```{julia}\nusing CairoMakie\n\ncategories = repeat(1:3, inner=800)\ndodge = repeat(repeat(1:2, outer=3), inner=400)\nv = randn(2400)\n\nboxplot(categories, v, dodge=dodge, show_notch=true, color=map(d -> d == 1 ? :cyan : :magenta, dodge))\n```\n\n#### Violin plot\n\nIt is similar to the box plot, however, the shape of each sample is represented by a mirrored kernel density estimate of the data.\n\n```{julia}\nusing CairoMakie\n\ncategories = repeat(1:3, outer=300)\nv = randn(900)\n\nviolin(categories, v, color=:cyan, datalimits=extrema)\n```\n\n```{julia}\nusing CairoMakie\n\ncategories = repeat(1:3, inner=800)\ndodge = repeat(repeat(1:2, outer=3), inner=400)\nv = randn(2400)\n\nviolin(categories, v, dodge=dodge, color=map(d -> d == 1 ? :cyan : :magenta, dodge), datalimits=extrema)\n```\n\n```{julia}\nusing CairoMakie\n\ncategories = repeat(1:3, inner=800)\nside = repeat(repeat([:left, :right], outer=3), inner=400)\nv = randn(2400)\n\nviolin(categories, v, side=side, color=map(d -> d == :left ? :cyan : :magenta, side), datalimits=extrema)\n```\n\n### Plots for multivariate and high-dimensional data\n\nFor vectors of observations, $(x_{11}, ..., x_{1p}), ..., (x_{n1}, ..., x_{np})$, where $n$ is the number of observations and $p$ is the number of variables, or features. In case where $p$ is large the data is called **high dimensional**.\n\n#### Scatter plot matrix\n\nIt consists of taking each possible pair of variables and plotting a scatter plot for that pair.\n\nObviously, with $p$ variables, we need at least $\\frac{p^2-p}{2}$ scatters.\n\n```{julia}\nusing RDatasets, AlgebraOfGraphics, DataFrames, CairoMakie\n\ndf = dataset(\"datasets\", \"iris\")\n\nfeature_names = [\"Sepal Length\", \"Sepal Width\", \"Petal Length\", \"Petal Width\", \"Species\"]\n\nrename!(df, feature_names)\n\nfig = Figure(size=(1200, 1200))\nfor i in 1:4\n    for j in 1:4\n        scatter = data(df) * mapping(feature_names[i], feature_names[j], color=feature_names[5]) * visual(Scatter)\n        ax_scatter = Axis(fig[i, j],\n            xlabel=feature_names[i],\n            ylabel=feature_names[j])\n        grid = draw!(ax_scatter, scatter)\n        if i == 1 && j == 1\n            legend!(fig[i, j], grid; tellheight=false, tellwidth=false, halign=:left, valign=:top)\n        end\n    end\nend\nfig\n```\n\n#### Heat map with marginals\n\nIn cases of pairs of observations $(x_1, y_1), ..., (x_n, y_n)$, the bivariate data can be constructed into a bivariate histogram (shown in the form of heat map in the 2D plane) in a manner similar to the univariate histogram. In addition, we can also add two marginal histograms beside the heat map, which are two separate histograms, one for $x_1, ..., x_n$, and the other for $y_1, ..., y_n$.\n\n```{julia}\nusing Distributions, DataFrames, AlgebraOfGraphics, CairoMakie\n\nN = 10^6\nmeanVect = [27, 26]\ncovMat = [16 13; 13 12]\nbiNorm = MvNormal(meanVect, covMat)\nsimData = DataFrame(rand(biNorm, N)', [:x, :y])\n\nfig = Figure(size=(600, 600))\ngl = fig[1, 1] = GridLayout()\n\nax_x = Axis(gl[1, 1])\nhist!(ax_x, simData[!, :x], bins=50, normalization=:pdf)\n\nax_y = Axis(gl[2, 2])\nhist!(ax_y, simData[!, :y], bins=50, normalization=:pdf, direction=:x)\n\nfor ax in [ax_x, ax_y]\n    hidedecorations!(ax)\n    hidespines!(ax)\nend\n\nax_hm = Axis(gl[2, 1],\n    xlabel=\"x\",\n    ylabel=\"y\")\nhm = data(simData) * mapping(:x, :y) * AlgebraOfGraphics.density(npoints=50)\ngrid = draw!(ax_hm, hm)\ncolorbar!(gl[3, 1], grid; tellheight=true, tellwidth=true, vertical=false, flipaxis=false)\n\ncolgap!(gl, 0)\nrowgap!(gl, 0)\ncolsize!(gl, 2, Auto(0.25))\nrowsize!(gl, 1, Auto(0.25))\n\nfig\n```\n\n### Andrews plot\n\nThe idea of Andrews plot is to represent a data vector $(x_{i1}, ..., x_{ip})$ via a real-valued function. **For any individual vector, such a transformation cannot be generally useful; however, when comparing groups of vectors, it may yield a way to visualize structural differences in the data.**\n\nThe specific transformation rule that we present here creates a plot known as *Andrews plot*.\n\nHere, for the $i$-th data vector $(x_{i1}, ..., x_{ip})$, we create the function $f_i(\\cdot)$ defined on $[-\\pi, \\pi]$ via,\n\n$$\nf_i(t) = \\frac{x_{i1}}{\\sqrt{2}} + x_{i2}\\sin(t) + x_{i3}\\cos(t) + x_{i4}\\sin(2t) + x_{i5}\\cos(2t) + x_{i6}\\sin(3t) + x_{i7}\\cos(3t) + \\cdots\n$$\n\nwith the last term involving a $\\sin()$ if $p$ is even and a $\\cos()$ is $p$ is odd. For $i = 1, ..., n$, the functions $f_1(\\cdot), ..., f_n(\\cdot)$ are plotted.\n\nIn cases where each $i$ has an associated label from a small finite set, different colors or line patterns can be used.\n\n```{julia}\nusing RDatasets, AlgebraOfGraphics, DataFrames, StatsBase, CairoMakie\n\nfunction gen_uni_str(n::Int; exclude_strs::Vector{String}=String[], iter_n::Int=1000)\n    alphabet = [collect('a':'z'); collect('A':'Z')]\n    num_underscore = [collect('0':'9'); \"_\"]\n\n    for i in 1:iter_n\n        uni_str = join([rand(alphabet, 1); rand([alphabet; num_underscore], n - 1)])\n        if uni_str .∉ Ref(exclude_strs)\n            return uni_str\n        end\n    end\n    error(\"cannot generate an unique string against the given arguments\")\nend\n\nfunction andrewsplot(df::DataFrame, features::Vector{String}; npoints::Int=100, scale::Bool=true)\n    if nrow(df) < 1 || length(features) < 1\n        error(\"both the data frame and features must have at least 1 element\")\n    end\n\n    if npoints < 1\n        error(\"the npoints must be an integer greater than 0\")\n    end\n\n    tmp_df = transform(df, eachindex => \"row_number\")\n    transform!(tmp_df, :row_number => (x -> string.(x)) => :row_number)\n    n_vars = length(features)\n\n    if scale\n        # scale each column to mean 0 and std 1\n        # to ensure that all features contribute equally to the shape of the curve\n        scaled_df = DataFrame(hcat([zscore(tmp_df[!, j]) for j in features]...), features)\n    else\n        scaled_df = tmp_df[!, features]\n    end\n\n    if iseven(n_vars)\n        placeholder_column_name = gen_uni_str(12; exclude_strs=features)\n        scaled_df[!, placeholder_column_name] = zeros(nrow(scaled_df))\n        n_vars = n_vars + 1\n    end\n\n    fvs = Vector{Float64}(undef, nrow(scaled_df) * npoints)\n    fvs_index_pairs = [[(i - 1) * npoints + 1, min(i * npoints, length(fvs))] for i in 1:Int(ceil(length(fvs) / npoints))]\n    ob_index_pairs = [[(i - 1) * 2 + 1, min(i * 2, n_vars - 1)] for i in 1:Int(ceil((n_vars - 1) / 2))]\n    f_range = collect(range(-π, π; length=npoints))\n    for i in 1:nrow(scaled_df)\n        ob = Vector(scaled_df[i, :])\n        f_it0 = popfirst!(ob) / √2\n        for j in eachindex(f_range)\n            t = f_range[j]\n            f_it = f_it0\n            for multiplier in 1:length(ob_index_pairs)\n                x1, x2 = ob[ob_index_pairs[multiplier]]\n                f_it = f_it + x1 * sin(multiplier * t) + x2 * cos(multiplier * t)\n            end\n            fvs[fvs_index_pairs[i][1]+j-1] = f_it\n        end\n    end\n\n    fvs_df = DataFrame(andrew_plot_x=repeat(f_range; outer=nrow(scaled_df)),\n        andrew_plot_y=fvs,\n        row_number=repeat(1:nrow(scaled_df); inner=npoints))\n    transform!(fvs_df, :row_number => (x -> string.(x)) => :row_number)\n    return innerjoin(tmp_df, fvs_df; on=:row_number, renamecols=\"_raw\" => \"_new\")\nend\n\niris = dataset(\"datasets\", \"iris\")\nfeatures = [\"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\"]\ndf = andrewsplot(iris, features; scale=false)\np = data(df) * mapping(:andrew_plot_x_new, :andrew_plot_y_new; group=:row_number, color=:Species_raw) * visual(Lines)\ndraw(p; figure=(size=(800, 500),))\n```\n\n### Plots for the board room\n\n#### Pie chart\n\nUsed to convey relative proportions.\n\n```{julia}\nusing CairoMakie\n\nd = [36, 12, 68, 5, 42, 27]\ncolors = [:yellow, :orange, :red, :blue, :purple, :green]\n\npie(d,\n    color=colors,\n    radius=4,  # the radius of the pie plot\n    inner_radius=2,  # the inner radius between 0 and radius to create a donut chart\n    strokecolor=:white,\n    strokewidth=5,\n    axis=(autolimitaspect=1,),\n)\n```\n\n**Note:** introduction to two `Axis()` parameters:\n\n* `aspect=nothing`: defined as the axis aspect ratio of the width over height.\n\nThis will change the size of the axis.\n\nIf you set it to `DataAspect()`, the axis aspect ratio width/heigth will matches that of the data limits.\n\nFor example, if the x limits range from 0 to 300 and the y limits from 100 to 250, then `DataAspect()` will result in an aspect ratio of `(300 - 0) / (250 - 100) = 2`. This can be useful when plotting images, because the image will be displayed unsquished.\n\n`AxisAspect(ratio)` reduces the effective axis size within the available layout space so that the axis aspect ratio width/height matches `ratio`.\n\n* `autolimitaspect=nothing`: the ratio of the limits to the axis size equals that number.\n\nFor example, if the axis size is $100\\times 200$, then with `autolimitaspect=1`, the autolimits will also have a ratio of 1 to 2.\n\n```{julia}\nusing CairoMakie\n\npie([π / 2, 2π / 3, π / 4],\n    normalize=false,\n    offset=π / 2,\n    color=[:orange, :purple, :green],\n    axis=(autolimitaspect=1,),\n)\n```\n\n#### Bar plot\n\nUsed to convey relative proportions.\n\n```{julia}\nusing CSV, DataFrames, AlgebraOfGraphics, CairoMakie, CategoricalArrays\n\ndf = CSV.read(\"./data/companyData.csv\", DataFrame)\ndf[!, \"Year\"] = categorical(df[!, \"Year\"])\ndf[!, \"Type\"] = categorical(df[!, \"Type\"]; levels=[\"C\", \"B\", \"A\"])\n\np = data(df) * mapping(:Year, :MarketCap; color=:Type, stack=:Type) * visual(BarPlot)\ndraw(p)\n```\n\n```{julia}\nusing CSV, DataFrames, AlgebraOfGraphics, CairoMakie\n\ndf = CSV.read(\"./data/companyData.csv\", DataFrame)\n\np = data(df) * mapping(:Year, :MarketCap; color=:Type, dodge=:Type) * visual(BarPlot)\ndraw(p)\n```\n\n#### Stack plot\n\nShow how constituent amounts of a metric change over time.\n\n```{julia}\nusing CSV, DataFrames, AlgebraOfGraphics, CairoMakie, CategoricalArrays\n\nfunction areaplot(df::DataFrame, x::AbstractString, y::AbstractString, group::AbstractString)\n    if nrow(df) == 0\n        error(\"the data frame is empty\")\n    end\n\n    tmp_df = groupby(df[:, [x, y, group]], group)\n    final_df = DataFrame([[], [], [], []], [x, y, group, \"row_number\"])\n    for i in 1:length(tmp_df)\n        sort!(tmp_df[i], x; rev=false)\n        transform!(tmp_df[i], eachindex => :row_number)\n        if i == 1\n            sub_tmp_df = copy(tmp_df[i])\n            sub_tmp_df[!, y] = repeat([0], nrow(sub_tmp_df))\n        else\n            sub_tmp_df = copy(tmp_df[i-1])\n            sub_tmp_df[!, group] = tmp_df[i][:, group]\n            tmp_df[i][!, y] = tmp_df[i][!, y] .+ sub_tmp_df[!, y]\n        end\n        final_df = vcat(final_df, sort(sub_tmp_df, x; rev=false), sort(tmp_df[i], x; rev=true))\n    end\n    transform!(final_df, Cols(:row_number, y) => ByRow(((x, y) -> (x, y))) => :Point)\n\n    return final_df\nend\n\ndf = CSV.read(\"./data/companyData.csv\", DataFrame)\ndf[!, \"Year\"] = categorical(df[!, \"Year\"])\ndf[!, \"Type\"] = categorical(df[!, \"Type\"])\nx, y, group = \"Year\", \"MarketCap\", \"Type\"\nfinal_df = areaplot(df, x, y, group)\n\np = data(final_df) * mapping(:Point; color=:Type) * visual(Poly)\ndraw(p)\n```\n\n### Working with files and remote servers\n\n## Statistical inference concepts\n\nThe statistical inference concepts involve **using mathematical techniques to make conclusions about unkown *population* parameters based on collected data**.\n\nThe analyses and methods of statistical inference can be categorized into:\n\n* Frequentist (classical): based on the assumption that **population parameters of some underlying distribution, or probability law, exist and are fixed, but are yet unknown**. The process of statistical inference then deals with **making conclusions about these parameters based on sampled data**.\n\n* Bayesian: only assumes that **there is a prior distribution of the parameters**. The key process deals with **analyzing a posterior distribtution of the parameters**.\n\n* Machine learning.\n\nIn general, a statistical inference process involves **data**, **model**, and **analysis**. The data is assumed to be comprised of random samples from the model. The goal of the analysis is to make informed statements about population parameters of the model based on the data.\n\nSuch statements typically take one of the following forms:\n\n* Point estimation: determination of a single value (or vector of values) representing a best estimate of the parameter/parameters.\n\n* Confidence intervals: determination of a range of values where the parameter lies. Under the model and the statistical process used, it is guaranteed that **the parameter lies within this range with a pre-specified probability**.\n\n* Hypothesis tests: the process of determining **if the parameter lies in a given region, in the complement of that region, or fails to take on a specific value**.\n\n### A random sample\n\nWhen carrying out **frequentist** statistical inference, we assume that there is some underlying distribution $F(x; \\theta)$ from which we are sampling, where $\\theta$ is the scalar or vector-valued unknown parameter we wish to know.\n\nWe assume that each **observation** is **statistically independent** and **identically distributed** as the rest. That is, from a probablistic perspective, the **observations** are taken as **independent and identically distributed** (i.i.d) **random variables**. In mathematical statistics, this is called **a random sample**. We denote the random variables of the observations by $X_1, ..., X_n$, and their respective values by $x_1, ..., x_n$.\n\nTypically, we compute **statistics** from the random sample, such as the **sample mean** and **sample variance**. We can consider each observation as a random variable, so these statistics are random variables too.\n\n$$\n\\overline{X} = \\frac{1}{n} \\sum_{i=1}^n X_i\\ \\ \\ \\ \\text{and}\\ \\ \\ \\ S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X-\\overline{X})^2\n$$\n\nFor $S^2$, we use $n-1$, which makes $S^2$ an **unbiased estimator** of the population variance.\n\nHere, we consider the sample statistics, such as the sample mean and sample variance, as random variables. This means that these statistics also subject to some underlying distributions. To know what distribution each statistics subject to is the first step to do statistical inference.\n\n### Sampling from a normal population\n\nWe often assume that the distribution we sample from is a normal distribution (i.e. $F(x; (\\mu, \\sigma^2))$).\n\nUnder the normality assumption, the distribution of the random variables $\\overline{X}$ and $S^2$ as well as transformations of them are well known:\n\n$$\n\\begin{align}\n\\overline{X} &\\backsim N(\\mu, \\frac{\\sigma^2}{n}) \\\\\n\\frac{(n-1)S^2}{\\sigma^2} &\\backsim \\chi^2_{n-1} \\\\\nT := \\frac{\\overline{X}-\\mu}{S/\\sqrt{n}} &\\backsim t_{n-1}\n\\end{align}\n$$\n\nThe notations $\\chi^2_{n-1}$ and $t_{n-1}$ denote a chi-squared distribution and a student T-distribution, respectively.\n\n### Independence of the sample mean and sample variance\n\nIn many cases, the sample mean and sample variance calculated from the **same** sample group are not independent, but in the special case where the samples $X_1, ..., X_n$ are from a **normal distribution**, independence between $\\overline{X}$ and $S^2$ holds. In fact, this property characetrizes the normal distribution - that is, this property only holds for the normal distribution.\n\n```{julia}\nusing Distributions, CairoMakie, Random, DataFrames\n\nRandom.seed!(1234)\n\nfunction mean_var(dist, n)\n    sample = rand(dist, n)\n    (mean(sample), var(sample))\nend\n\nuni_dist = Uniform(-sqrt(3), sqrt(3))\nn, N = 3, 10^5\n\n# the sample mean and sample variance are calculated from the same sample group\n# so the two are not independent\ndata_uni = DataFrame([mean_var(uni_dist, n) for _ in 1:N], [:mean, :var])\n# the sample mean and sample variance are calculated from two different sample groups\n# so the two are independent\ndata_uni_ind = DataFrame([(mean(rand(uni_dist, n)), var(rand(uni_dist, n))) for _ in 1:N], [:mean, :var])\n\nfig, ax = scatter(data_uni.mean, data_uni.var; color=:blue, label=\"Same group\", markersize=2)\nscatter!(ax, data_uni_ind.mean, data_uni_ind.var; color=:orange, label=\"Separate group\", markersize=2)\nax.xlabel = L\"\\overline{X}\"\nax.ylabel = L\"S^2\"\nax.title = \"Uniform Distribution\"\naxislegend(ax)\nfig\n```\n\n```{julia}\n# in the case where we sample from the normal distribution\n# the sample mean and sample variance are always independent\n# independent of the way we calculate them i.e., from the same sample group or from two different sample groups\ndata_norm = DataFrame([mean_var(Normal(), n) for _ in 1:N], [:mean, :var])\ndata_norm_ind = DataFrame([(mean(rand(Normal(), n)), var(rand(Normal(), n))) for _ in 1:N], [:mean, :var])\n\nfig, ax = scatter(data_norm.mean, data_norm.var; color=:blue, label=\"Same group\", markersize=2)\nscatter!(ax, data_norm_ind.mean, data_norm_ind.var; color=:orange, label=\"Separate group\", markersize=2)\nax.xlabel = L\"\\overline{X}\"\nax.ylabel = L\"S^2\"\nax.title = \"Normal Distribution\"\naxislegend(ax)\nfig\n```\n\n### T-Distribution\n\nThe random variable **T-statistic** is given by\n\n$$\nT = \\frac{\\overline{X}-\\mu}{S/\\sqrt{n}} \\backsim t_{n-1}\n$$\n\nDenoting the mean and variance of the normally distributed observations by $\\mu$ and $\\sigma^2$, respectively, we can represent the T-statistic as\n\n$$\nT = \\frac{\\frac{\\overline{X}-\\mu}{\\sigma/\\sqrt{n}}}{\\sqrt{\\frac{(n-1)S^2}{\\sigma^2}\\frac{1}{n-1}}} = \\frac{Z}{\\sqrt{\\frac{\\chi^2_{n-1}}{n-1}}}\n$$\n\nHere, the numerator $Z$ is a standard normal random variable, and in the denominator the random variable $\\chi^2_{n-1} = (n-1)S^2/\\sigma^2$ is chi-distributed wit $n-1$ degrees of freedom. Furthermore, the numerator and denominator random variables are independent because they are based on the sample mean and sample variance, respectively.\n\nHence, $T \\backsim t(n-1)$, which means a \"T-Distribution with $n-1$ degrees of freedom\".\n\nHere, we check the above fact that T-statistic is derived from two independent random variables (the numerator is a standard normal random variable, while the denominator is a random variable chi-distributed with $n-1$ degrees of freedom):\n\n```{julia}\nusing Random, StatsBase, Distributions, CairoMakie\n\nRandom.seed!(0)\n\nfunction tStat(degree)\n    z = rand(Normal())\n    c = rand(Chisq(degree))\n    z / sqrt(c / degree)\nend\n\nn, N = 10, 10^6\n\nsimulationTStats = [tStat(n - 1) for _ in 1:N]\n\nxGrid = -5:0.01:5\n\nfig, ax = stephist(simulationTStats; bins=400, color=:blue, label=\"Simulated\", normalization=:pdf)\nlines!(ax, xGrid, pdf.(TDist(n - 1), xGrid); color=:red, label=\"Analytical\")\nax.limits = (first(xGrid), last(xGrid), nothing, nothing)\nfig\n```\n\nA T-Distribution with $k$ degrees of freedom can be shown to have a density function,\n\n$$\nf(x) = \\frac{\\Gamma(\\frac{k+1}{2})}{\\sqrt{k\\pi} \\Gamma(\\frac{k}{2})} \\left(1+\\frac{x^2}{k}\\right)^{-\\frac{k+1}{2}}\n$$\n\nNote that $E(\\chi^2_{n-1}) = n-1$ and $Var(\\chi^2_{n-1}) = 2(n-1)$, so $E\\left(\\frac{\\chi^2_{n-1}}{n-1}\\right) = 1$, and $Var(\\frac{\\chi^2_{n-1}}{n-1}) = \\frac{2}{n-1}$.\n\nTherefore, we have $\\frac{\\chi^2_{n-1}}{n-1} \\rightarrow 1$ as $n \\rightarrow \\infty$, with the same holding for $\\sqrt{\\frac{\\chi^2_{n-1}}{n-1}}$.\n\nHence, for large $n$, the distribution of $T$ will converge to the distribution of $Z$.\n\n```{julia}\nusing Distributions, Random, CairoMakie, DataFrames\n\nRandom.seed!(1234)\n\nn, N, alpha = 3, 10^7, 0.1\n\nmyT(n) = rand(Normal()) / sqrt(rand(Chisq(n - 1)) / (n - 1))\nmcQuantile = quantile([myT(n) for _ in 1:N], alpha)\nanalyticQuantile = quantile(TDist(n - 1), alpha)\n\nprintln(\"Quantile from Monte Carlo: \", mcQuantile)\nprintln(\"Analytic quantile: \", analyticQuantile)\n\nxGrid = -5:0.1:5\n\nfig = Figure()\nax = fig[1, 1] = Axis(fig)\n\nlines!(ax, xGrid, pdf.(Normal(), xGrid), label=\"Normal\", color=:red)\nscatter!(ax, xGrid, pdf.(TDist(1), xGrid), label=\"DOF = 1\", color=:blue)\nscatter!(ax, xGrid, pdf.(TDist(5), xGrid), label=\"DOF = 5\", color=:purple)\nscatter!(ax, xGrid, pdf.(TDist(10), xGrid), label=\"DOF = 10\", color=:orange)\nscatter!(ax, xGrid, pdf.(TDist(100), xGrid), label=\"DOF = 100\", color=:green)\n\naxislegend(ax)\n\nfig\n```\n\n### Two samples and the F-Distribution\n\nMany statistical procedures involve **the ratio of sample variances**, or similar quantities, for two or more samples.\n\nFor example, if $X_1, ..., X_{n_1}$ is one sample, and $Y_1, ..., Y_{n_2}$ is another sample, and **both samples are distributed normally with the same parameters**, then the ratio of the two sample variances\n\n$$\nF = \\frac{S_X^2}{S_Y^2}\n$$\n\nIt turns out such a statistic distributed as the **F-Distribution**, with density given by\n\n$$\nf(x) = K(a, b) \\frac{x^{\\frac{a}{2}-1}}{(ax+b)^{\\frac{a+b}{2}}}\\ \\ \\ \\ \\text{with}\\ \\ \\ \\ K(a, b) = \\frac{\\Gamma(\\frac{a+b}{2}) a^{\\frac{a}{2}} b^{\\frac{b}{2}}}{\\Gamma(\\frac{a}{2}) \\Gamma(\\frac{b}{2})}\n$$\n\nHere, the parameters $a$ and $b$ are the numerator degrees of freedom and denominator degrees of freedom, respectively.\n\n```{julia}\nusing Distributions, CairoMakie\n\nn1, n2 = 10, 15\nN = 10^6\nmu, sigma = 10, 4\nnorm_dist = Normal(mu, sigma)\n\nfvs = Array{Float64}(undef, N)\n\nfor i in 1:N\n    d1 = rand(norm_dist, n1)\n    d2 = rand(norm_dist, n2)\n    fvs[i] = var(d1) / var(d2)\nend\n\nf_range = 0:0.1:5\nfig, ax = stephist(fvs, bins=400, color=:blue, label=\"Simulated\", normalization=:pdf)\nlines!(ax, f_range, pdf.(FDist(n1 - 1, n2 - 1), f_range), color=:red, label=\"Analytic\")\nxlims!(ax, low=0, high=5)\naxislegend(ax)\nfig\n```\n\n### The central limit theorem\n\nThe Central Limit Theorem (CLT) indicates that **summations of a large number of independent random quantities, each with finite variance, yield a sum that is approximately normally distributed**.\n\nThis is why the normal distribution is ubiquitous in nature.\n\nConsider an i.i.d sequence $X_1, X_2, ...$, where all $X_i$ are distributed according to some distribution $F(x_i; \\theta)$ with mean $\\mu$ and finite variance $\\sigma^2$.\n\nThen consider the random variable\n\n$$\nY_n := \\sum_{i=1}^n X_i\n$$\n\nIt is clear that $E(Y_n) = n\\mu$ and $Var(Y_n) = n\\sigma^2$.\n\nHence, we may consider a random variable\n\n$$\n\\widetilde{Y}_n := \\frac{Y_n - n\\mu}{\\sqrt{n}\\sigma}\n$$\n\nObserve that $\\widetilde{Y}_n$ is zero mean and unit variance. The CLT states that as $n \\rightarrow \\infty$, the ditribution of $\\widetilde{Y}_n$ converges to a standard normal distribution. That is, for every $x \\in R$,\n\n$$\n\\lim_{n \\rightarrow \\infty} P(\\widetilde{Y}_n \\le x) = \\int_{-\\infty}^x \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{u^2}{2}} du\n$$\n\nAlternatively, this may be viewed as indicating that for non-small $n$\n\n$$\nY_n\\ \\ \\widetilde{\\text{approx}}\\ \\ N(n\\mu, n\\sigma^2)\n$$\n\nIn addition, we have\n\n$$\n\\overline{Y}_n = \\frac{Y_n}{n}\\ \\ \\widetilde{\\text{approx}}\\ \\ N(\\mu, \\frac{\\sigma^2}{n})\n$$\n\nThis means that sample means from i.i.d samples with finite variances are asymptotically distributed according to a normal distribution as the sample size grows.\n\n```{julia}\nusing Distributions, Random, CairoMakie\n\nRandom.seed!(1234)\n\n# note that n = 30 isn't enough to get a perfect fit to a normal distribution in the case of exponential distribution\nn, N = 30, 10^6\n\ndist1 = Uniform(1 - sqrt(3), 1 + sqrt(3))\ndist2 = Exponential(1)\ndist3 = Normal(1, 1)\n\ndata1 = [mean(rand(dist1, n)) for _ in 1:N]\ndata2 = [mean(rand(dist2, n)) for _ in 1:N]\ndata3 = [mean(rand(dist3, n)) for _ in 1:N]\n\nfig, ax = stephist(data1, bins=100, color=:blue, label=\"Average of Uniforms\", normalization=:pdf)\nstephist!(ax, data2, bins=100, color=:orange, label=\"Average of Exponentials\", normalization=:pdf)\nstephist!(ax, data3, bins=100, color=:green, label=\"Average of Normals\", normalization=:pdf)\nlines!(ax, 0:0.01:2, pdf.(Normal(1, 1 / sqrt(n)), 0:0.01:2), color=:red, label=\"Analytic Normal Distribution\")\naxislegend(ax)\nfig\n```\n\n### Point estimation\n\nGiven a random sample, $X_1, ..., X_n$, a common task of statistical inference is to estimate a parameter $\\theta$, or a function of it, say $h(\\theta)$.\n\nThe process of **designing an estimator**, **analyzing its performance**, and **carrying out the estimation** is called *point estimation*.\n\nAlthough we can never know the underlying parameter $\\theta$, or $h(\\theta)$ exactly, we can arrive at an estimate for it via an estimator $\\hat{\\theta} = f(X_1, ..., X_n)$. Here, the design of the estimator is embodied by $f(\\cdot)$, a function that specifies how to construct the estimate from the sample.\n\nWhen performing point estimation, the first question we must answer is how close is $\\hat{\\theta}$ to the actual unknown quantity $\\theta$ or $h(\\theta)$?\n\n#### Describing the performance and behavior of estimators\n\nWhen analyzing the performance of an estimator $\\hat{\\theta}$, it is important to understand that it is a random variable.\n\nOne common measure of its performance is the **Mean Squared Error** (MSE),\n\n$$\n\\begin{align}\nMSE_\\theta(\\hat{\\theta}) &:= E[(\\hat{\\theta}-\\theta)^2] \\\\\n&= E(\\hat{\\theta}^2-2\\hat{\\theta}\\theta+\\theta^2) \\\\\n&= E(\\hat{\\theta}^2) - 2\\theta E(\\hat{\\theta}) + \\theta^2 \\\\\n&= E(\\hat{\\theta}^2) - [E(\\hat{\\theta})]^2 + [E(\\hat{\\theta})]^2 - 2\\theta E(\\hat{\\theta}) + \\theta^2 \\\\\n&= Var(\\hat{\\theta}) + (E(\\hat{\\theta}) - \\theta)^2 \\\\\n&:= variance + bias^2\n\\end{align}\n$$\n\nHere, the MSE can be decomposed into **the variance of the estimator** and **its bias squared**.\n\n* The variance of the estimator represents the dispersion degree of the estimator itself. Low variance is clearly a desirable performance measure. This indicates the stability of the estimator.\n\n* The bias squared represents whether the estimator $\\hat{\\theta}$ is an **unbiased** estimator of the parameter $\\theta$ or $h(\\theta)$. This indicates how close is the $E(\\hat{\\theta})$ to $\\theta$ or $h(\\theta)$.\n\nThis can be illustrated by the folllowing plot:\n\n```{julia}\nusing Distributions, CairoMakie, Random\n\nRandom.seed!(1234)\n\nest_pts = rand(Normal(3, 1), 10)\n\nfig = Figure()\nax = Axis(fig[1, 1])\n\nscatter!(repeat([0], 10), est_pts; color=:black, markersize=10, label=L\"\\hat{\\theta}\")\nscatter!(0, mean(est_pts); label=L\"E(\\hat{\\theta})\", color=:cyan, markersize=20)\nscatter!(0, 10; color=:red, markersize=20, label=L\"\\theta\")\n\nbracket!(0, mean(est_pts), 0, 10; text=L\"(E(\\hat{\\theta})-\\theta)^2\", offset=20, style=:square, orientation=:up)\nbracket!(0, minimum(est_pts), 0, maximum(est_pts); text=L\"Var(\\hat{\\theta})\", offset=20, style=:square, orientation=:down)\n\naxislegend(ax)\nfig\n```\n\nCertainly, we are really interested in **whether an estimator is unbiased**, and then **how low the variance of the estimator is**.\n\nWhether an estimator is unbiased means that $E(\\hat{\\theta}) = \\theta$.\n\nHere, we give some examples:\n\nConsider $X_1, ..., X_n$ distributed according to any distribution with a finite mean $\\mu$.\n\n1. The sample mean $\\overline{X}$ is an unbiased estimator of the population mean $\\mu$:\n\n$$\n\\begin{align}\nE(\\overline{X}) &= E\\left[\\frac{1}{n} \\sum_{i=1}^n X_i\\right] \\\\\n&= \\frac{1}{n} \\sum_{i=1}^n E(X_i) \\\\\n&= \\frac{1}{n} n\\mu \\\\\n&= \\mu\n\\end{align}\n$$\n\n$$\n\\begin{align}\nVar(\\overline{X}) &= Var\\left(\\frac{1}{n} \\sum_{i=1}^n X_i\\right) \\\\\n&= \\frac{1}{n^2} \\sum_{i=1}^n Var(X_i) \\\\\n&= \\frac{1}{n^2} n\\sigma^2 \\\\\n&= \\frac{\\sigma^2}{n}\n\\end{align}\n$$\n\nSo the sample mean $\\overline{X} = \\frac{1}{n} \\sum_{i=1}^n X_i$ is an unbiased estimator of the population mean $\\mu$ with the variance $\\frac{\\sigma^2}{n}$.\n\n2. In the case where the population mean $\\mu$ is known, but the population variance $\\sigma^2$ is unknown, then $\\hat{\\sigma^2} := \\frac{1}{n} \\sum_{i=1}^n (X_i-\\mu)^2$ is an unbiased estimator of the population variance $\\sigma^2$, but $\\hat{\\sigma} := \\sqrt{\\hat{\\sigma^2}}$ is not an unbiased estimator of $\\sigma$ (in fact, $\\hat{\\sigma}$ is asymptotically unbiased. That is, the bias tends to $0$ as the sample size grows):\n\n$$\n\\begin{align}\nE(\\hat{\\sigma^2}) &= E\\left(\\frac{1}{n} \\sum_{i=1}^n (X_i-\\mu)^2\\right) \\\\\n&= \\frac{1}{n} \\sum_{i=1}^n E\\left((X_i-\\mu)^2\\right) \\\\\n&= \\frac{1}{n} n\\sigma^2 \\\\\n&= \\sigma^2\n\\end{align}\n$$\n\nSo in the case where the population mean $\\mu$ is known, $\\hat{\\sigma^2}$ is an unbiased estimator of $\\sigma^2$, but $\\hat{\\sigma}$ is not an unbiased estimator of $\\sigma$.\n\n```{julia}\nusing Random, Statistics\n\nRandom.seed!(1234)\n\n# consider an uniform distribution over [0, 1]\ntrueVar, trueStd = 1 / 12, sqrt(1 / 12)\n\nfunction estVar(n)\n    sample = rand(n)\n    sum((sample .- 0.5) .^ 2) / n\nend\n\nN = 10^7\nfor n in 10:20:90\n    biasVar = mean([estVar(n) for _ in 1:N]) - trueVar\n    biasStd = mean([sqrt(estVar(n)) for _ in 1:N]) - trueStd\n    println(\"n = \", n, \" Var bias: \", round(biasVar; digits=6),\n        \"\\t Std bias: \", round(biasStd; digits=5))\nend\n```\n\n3. In the case where the population mean $\\mu$ is not known, the sample variance $S^2 := \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\overline{X})^2$ is an unbiased estimator of $\\sigma^2$:\n\n$$\n\\begin{align}\n\\sum_{i=1}^{n}(x_i-\\bar{x})^2 &= \\sum_{i=1}^{n}(x_i^2-2x_i\\bar{x}+\\bar{x}^2) \\\\\n&= \\sum_{i=1}^{n}x_i^2 - 2n\\bar{x}\\frac{1}{n}\\sum_{i=1}^{n}x_i + n\\bar{x}^2 \\\\\n&= \\sum_{i=1}^{n}x_i^2 - n\\bar{x}^2\n\\end{align}\n$$\n\n$$\nE(X^2) = Var(X) + [E(X)]^2 = \\sigma^2 + \\mu^2\n$$\n\n$$\n\\begin{align}\nE(S^2) &= E\\left(\\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\overline{X})^2\\right) \\\\\n&= \\frac{1}{n-1}E\\left(\\sum_{i=1}^{n}X_i^2 - n\\overline{X}^2\\right) \\\\\n&= \\frac{1}{n-1}\\left(\\sum_{i=1}^{n}E(X_i^2) - nE(\\overline{X}^2)\\right) \\\\\n&= \\frac{1}{n-1}\\left(n(\\sigma^2+\\mu^2) - n(\\frac{\\sigma^2}{n}+\\mu^2)\\right) \\\\\n&= \\frac{1}{n-1} (n-1)\\sigma^2 \\\\\n&= \\sigma^2\n\\end{align}\n$$\n\nIn summary, we can evaluate the performance and behavior of an estimator from different aspects, such as **unbias** ($E(\\hat{\\theta}) = \\theta$), **effectiveness** ($Var(\\hat{\\theta})$ is as small as possible), **consistency** (an estimator is consistent if it converges to the true value as the number of observations grows to infinity), etc.\n\n#### Designing estimators\n\n##### Method of moments\n\nThe key idea is that the $k$'s moment estimator calculated from the random sample should be equal to the $k$'s moment of the underlying distribution from which we sample (i.e. $\\hat{m}_k = m_k$). Then we can obtain parameter estimates for a distribution.\n\n$$\n\\hat{m}_k = \\frac{1}{n} \\sum_{i=1}^{n}X_i^k\n$$\n\n$$\nm_k = E(X^k) = \\begin{cases}\n\\sum_{i=1}^{\\infty} x_i p(x_i) &\\text{for discrete case} \\\\\n\\int_{-\\infty}^{\\infty} x p(x) dx &\\text{for continuous case}\n\\end{cases}\n$$\n\nIn cases where there are multiple unkown parameters, say $K$, we use the first $K$ moment estimates to formulate a system of $K$ equations and $K$ unkowns. This system of equations can be written as $E[X^k; \\theta_1, ..., \\theta_K] = \\hat{m}_k\\ \\ \\ \\ \\text{for}\\ \\ \\ \\ k=1,...,K$.\n\n```{julia}\nusing Random, Distributions, NLsolve\n\nRandom.seed!(1234)\n\n# Triangular distribution\na, b, c = 3, 5, 4\ndist = TriangularDist(a, b, c)\nn = 2000\nsamples = rand(dist, n)\n\nm_k(k, data) = 1 / n * sum(data .^ k)\nmHats = [m_k(i, samples) for i in 1:3]\n\nfunction equations(F, x)\n    F[1] = 1 / 3 * (x[1] + x[2] + x[3]) - mHats[1]\n    F[2] = 1 / 6 * (x[1]^2 + x[2]^2 + x[3]^2 +\n                    x[1] * x[2] + x[1] * x[3] + x[2] * x[3]) - mHats[2]\n    F[3] = 1 / 10 * (x[1]^3 + x[2]^3 + x[3]^3 +\n                     x[1]^2 * x[2] + x[1]^2 * x[3] + x[2]^2 * x[1] +\n                     x[2]^2 * x[3] + x[3]^2 * x[1] + x[3]^2 * x[2] +\n                     x[1] * x[2] * x[3]) - mHats[3]\nend\n\nnlOutput = nlsolve(equations, [0.1, 0.1, 0.1])\nsol = sort(nlOutput.zero)\naHat, bHat, cHat = sol[1], sol[3], sol[2]\nprintln(\"Found estimates for (a, b, c) = \", (aHat, bHat, cHat), \"\\n\")\nprintln(nlOutput)\n```\n\n##### Maximum likelihood estimation (MLE)\n\nThe key is to consider the **likelihhod** of the parameter $\\theta$ having a specific value given observations $x_1, ..., x_n$. This is done via the likelihood function,\n\n$$\nL(\\theta; x_1, ..., x_n) = f_{X_1, ..., X_n}(x_1, ..., x_n; \\theta)\n$$\n\nIf $X_1, ..., X_n$ are i.i.d., where the joint PDF of $X_1, ..., X_n$ is represented as the product of the individual PDF, then we have\n\n$$\nL(\\theta; x_1, ..., x_n) = f_{X_1, ..., X_n}(x_1, ..., x_n; \\theta) = \\prod_{i=1}^n f(x_i; \\theta)\n$$\n\nNow given the likelihood function, the **maximum likelihood estimator** is a value $\\theta$ that **maximizes** $L(\\theta; x_1, ..., x_n)$. So an MLE is the maximizer of the likelihood.\n\n```{julia}\nusing Random, Distributions, CairoMakie\n\nRandom.seed!(1234)\n\nrealAlpha, realLambda = 2, 3\ngammaDist = Gamma(realAlpha, 1 / realLambda)\n\nn = 10^2\nsamples = rand(gammaDist, n)\n\nalphaGrid = collect(1:0.02:3)\nlambdaGrid = collect(1:0.02:5)\n\nlikelihood = [prod(pdf.(Gamma(a, 1 / l), samples)) for a in alphaGrid, l in lambdaGrid]\n\nsurface(alphaGrid, lambdaGrid, likelihood, axis=(type=Axis3,))\n```\n\nObserve that any maximizer $\\hat{\\theta}$ of $L(\\theta; x_1, ..., x_n)$ will also maximize its logarithm. Practically, both from an analytic and numerical perspective, considering this $log-likelihood function$ is often more attractive:\n\n$$\nl(\\theta; x_1, ..., x_n) := \\log L(\\theta; x_1, ..., x_n) = \\sum_{i=1}^{n}\\log \\left(f(x_i; \\theta)\\right)\n$$\n\n**Note:** the second equality holds only for i.i.d. $X_1, ..., X_n$.\n\nHence, given a sample from a gamma distribution, the log-likelihood function is\n\nFirst, the PDF of the gamma distribution is\n\n$$\nf(x) = \\frac{\\lambda^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha - 1}e^{-\\lambda x}\n$$\n\nwith parameters $\\lambda > 0$ and $\\alpha > 0$.\n\n$$\nl(\\theta; x_1, ..., x_n) = n\\alpha\\log(\\lambda) - n\\log(\\Gamma(\\alpha)) + (\\alpha - 1)\\sum_{i=1}^{n}\\log(x_i) - \\lambda\\sum_{i=1}^{n}x_i\n$$\n\nDivide by $n$ to obtain the following function that needs to be maximized:\n\n$$\n\\tilde{l}(\\theta; \\bar{x}, \\bar{x}_l) = \\alpha\\log(\\lambda) - \\log(\\Gamma(\\alpha)) + (\\alpha - 1)\\bar{x}_l - \\lambda \\bar{x}\n$$\n\nwhere $\\bar{x}$ is the sample mean, and $\\bar{x}_l := \\frac{1}{n}\\sum_{i=1}^{n}\\log(x_i)$.\n\nFurther simplification is possible by removing the stand-alone $-\\bar{x}_l$ term, as it does not affect the optimal value. Heance, our optimization problem is then,\n\n$$\n\\max_{\\lambda > 0, \\alpha > 0} \\alpha (\\log(\\lambda + \\bar{x}_l)) - \\log(\\Gamma(\\alpha)) - \\lambda \\bar{x}\n$$\n\nAs is typical in such cases, the function actually depends on the sample only through the two sufficient statistics $\\bar{x}$ and $\\bar{x}_l$.\n\nThen, taking $\\alpha$ as fixed, we may consider the derivative with respect to $\\lambda$, and equate this to $0$:\n\n$$\n\\frac{\\alpha}{\\lambda} - \\bar{x} = 0\n$$\n\nHence, for any optimal $\\alpha^\\star$, we have $\\lambda = \\frac{\\alpha}{\\bar{x}}$. This allows us to substitute $\\lambda^\\star$ for $\\lambda$ to obtain\n\n$$\n\\max_{\\alpha > 0} \\alpha (\\log(\\alpha) - \\log(\\bar{x}) + \\bar{x}_l) - \\log(\\Gamma(\\alpha)) - \\alpha\n$$\n\nNow by taking the derivative with respective to $\\alpha$, and equating this to $0$, we obtain\n\n$$\n\\log(\\alpha) + 1 - \\log(\\bar{x}) + \\bar{x}_l - \\psi(\\alpha) - 1 = 0\n$$\n\nwhere $\\psi(z) := \\frac{d}{dz}\\log(\\Gamma(z))$ is the well-known **digamma function**.\n\nHence, we find that $\\alpha^\\star$ must satisfy\n\n$$\n\\log(\\alpha) - \\psi(\\alpha) - \\log(\\bar{x}) + \\bar{x}_l = 0\n$$\n\nwith $\\lambda^\\star = \\frac{\\alpha^\\star}{\\bar{x}}$. Our optimal MLE solution is given by $(\\alpha^\\star, \\lambda^\\star)$. In order to find this value, we must solve it numerically.\n\n```{julia}\nusing SpecialFunctions, Distributions, Roots, CairoMakie, Random\n\nRandom.seed!(1234)\n\neq(alpha, xb, xbl) = log(alpha) - digamma(alpha) - log(xb) + xbl\n\nrealAlpha, realLambda = 2, 3\ngammaDist = Gamma(realAlpha, 1 / realLambda)\n\nfunction mle(sample)\n    alpha = find_zero((a) -> eq(a, mean(sample), mean(log.(sample))), 1)\n    lambda = alpha / mean(sample)\n    return [alpha, lambda]\nend\n\nN = 10^4\n\nmles10 = [mle(rand(gammaDist, 10)) for _ in 1:N]\nmles100 = [mle(rand(gammaDist, 100)) for _ in 1:N]\nmles1000 = [mle(rand(gammaDist, 1000)) for _ in 1:N]\n\nfig = Figure()\nax = Axis(fig[1, 1],\n    xlabel=L\"\\alpha\",\n    ylabel=L\"\\lambda\",\n    limits=(0, 6, 0, 8))\n\nscatter!(ax, first.(mles10), last.(mles10),\n    color=:blue, markersize=2, label=\"n = 10\")\nscatter!(ax, first.(mles100), last.(mles100),\n    color=:red, markersize=2, label=\"n = 100\")\nscatter!(ax, first.(mles1000), last.(mles1000),\n    color=:green, markersize=2, label=\"n = 1000\")\n\nmarker_elem1 = MarkerElement(color=:blue, marker=:circle, markersize=20, points=Point2f[(0.5, 0.5)])\nmarker_elem2 = MarkerElement(color=:red, marker=:circle, markersize=20, points=Point2f[(0.5, 0.5)])\nmarker_elem3 = MarkerElement(color=:green, marker=:circle, markersize=20, points=Point2f[(0.5, 0.5)])\nLegend(fig[1, 1],\n    [marker_elem1, marker_elem2, marker_elem3],\n    [\"n = 10\", \"n = 100\", \"n = 1000\"],\n    tellwidth=false, tellheight=false,\n    halign=:left, valign=:top)\nfig\n```\n\n##### Comparing the method of moments and MLE\n\nNow we use the Mean Squared Error (MSE)\n\n$$MSE_\\theta(\\hat{\\theta}) = E[(\\hat{\\theta}-\\theta)^2] = Var(\\hat{\\theta}) + (E[\\hat{\\theta}] - \\theta)^2 = \\text{variance} + \\text{bias}^2$$\n\nto compare the performance and behavior of moments and MLE.\n\nConsider a random sample $x_1, ..., x_n$ from an uniform distribution on the interval $(a, b)$.\n\nThe MLE for the parameter $\\theta = (a, b)$ can be shown to be\n\n$$\n\\begin{align}\n\\hat{a} &= \\min\\{x_1, ..., x_n\\} \\\\\n\\hat{b} &= \\max\\{x_1, ..., x_n\\}\n\\end{align}\n$$\n\nFor the method of moments, since $X \\backsim U(a, b)$, it follows that\n\n$$\n\\begin{align}\nE[X] &= \\frac{a+b}{2} \\\\\nVar(X) &= \\frac{(b-a)^2}{12}\n\\end{align}\n$$\n\nHence, by solving for $a$ and $b$, and replacing $E[X]$ and $Var(X)$ with $\\bar{x}$ and $s^2$ respectively, we obtain\n\n$$\n\\begin{align}\n\\hat{a} &= \\bar{x}-\\sqrt{3}s \\\\\n\\hat{b} &= \\bar{x}+\\sqrt{3}s\n\\end{align}\n$$\n\n```{julia}\nusing Distributions, CairoMakie\n\nN = 10^5\nnMin, nStep, nMax = 10, 10, 200\nsampleSizes = nMin:nStep:nMax\ntrueB = 5\ntrueDist = Uniform(-2, trueB)\n\nMLEest(data) = maximum(data)\nMMest(data) = mean(data) + sqrt(3) * std(data)\n\nres = Dict{Symbol,Array{Float64}}(\n    (sym -> sym => Array{Float64}(undef, length(sampleSizes))).(\n        [:MSEMLE, :MSEMM, :VarMLE, :VarMM, :BiasMLE, :BiasMM]))\n\nfor (i, n) in enumerate(sampleSizes)\n    mleEst, mmEst = Array{Float64}(undef, N), Array{Float64}(undef, N)\n    for j in 1:N\n        samples = rand(trueDist, n)\n        mleEst[j] = MLEest(samples)\n        mmEst[j] = MMest(samples)\n    end\n    meanMLE, meanMM = mean(mleEst), mean(mmEst)\n    varMLE, varMM = var(mleEst), var(mmEst)\n\n    res[:MSEMLE][i] = varMLE + (meanMLE - trueB)^2\n    res[:MSEMM][i] = varMM + (meanMM - trueB)^2\n    res[:VarMLE][i] = varMLE\n    res[:VarMM][i] = varMM\n    res[:BiasMLE][i] = meanMLE - trueB\n    res[:BiasMM][i] = meanMM - trueB\nend\n\nfig = Figure(size=(600, 1200))\nax_mse = Axis(fig[1, 1],\n    xlabel=\"Sample size\",\n    ylabel=\"MSE\")\nscatter!(ax_mse, sampleSizes, res[:MSEMLE]; color=:blue, label=\"MSE (MLE)\")\nscatter!(ax_mse, sampleSizes, res[:MSEMM]; color=:red, label=\"MSE (MM)\")\naxislegend(ax_mse)\n\nax_var = Axis(fig[2, 1],\n    xlabel=\"Sample size\",\n    ylabel=\"Variance\")\nscatter!(ax_var, sampleSizes, res[:VarMLE]; color=:blue, label=\"Variance (MLE)\")\nscatter!(ax_var, sampleSizes, res[:VarMM]; color=:red, label=\"Variance (MM)\")\naxislegend(ax_var)\n\nax_bias = Axis(fig[3, 1],\n    xlabel=\"Sample size\",\n    ylabel=\"Bias\")\nscatter!(ax_bias, sampleSizes, res[:BiasMLE]; color=:blue, label=\"Bias (MLE)\")\nscatter!(ax_bias, sampleSizes, res[:BiasMM]; color=:red, label=\"Bias (MM)\")\naxislegend(ax_bias; position=:rb)\nfig\n```\n\nIn fact, there is more supporting theory for the usefulness of maximum likelihood estimation as $n \\rightarrow \\infty$.\n\n### Confidence interval\n\nGiven a single sample $X_1, ..., X_n$, how does one obtain an indication about the accuracy of the estimate?\n\nConsider the case where we are trying to estimate the parameter $\\theta$. A confidence interval is then an interval $[L, U]$ obtained from our sample data, such that $P(L\\le \\theta \\le U) = 1-\\alpha$, where $1-\\alpha$ is called the confidence level.\n\nConsider a case of a single observation $X$ ($n=1$) taken from a symmetric triangular distribution, with a spread of 2 and an unknown center (mean) $\\mu$.\n\nIn this case, we would set\n\n$$\n\\begin{align}\nL &= X + q_{\\frac{\\alpha}{2}} \\\\\nU &= X + q_{1-\\frac{\\alpha}{2}}\n\\end{align}\n$$\n\nwhere $q_u$ is the $u$'th quantile of a triangular distribution centered at $0$, and having a spread of $2$.\n\nSetting $L$ and $U$ in this manner ensures that $P(L\\le \\theta \\le U) = 1-\\alpha$ holds.\n\nIn this case, we know that $q_{\\frac{\\alpha}{2}} = -(1 - \\sqrt{\\alpha})$ and $q_{1-\\frac{\\alpha}{2}} = 1 - \\sqrt{\\alpha}$.\n\n::: {.callout-important}\n\nTo understand the confidence interval $P(L\\le \\theta \\le U) = 1-\\alpha$, a key point is that there is $1-\\alpha$ chance that the actual parameter $\\theta$ **is covered by** the interval $[L, U]$. This means that if the sampling experiment is repeated say $N$ times, then on average, $N\\times(1-\\alpha)$ of the time the actual parameter $\\theta$ is covered by the interval.\n\n:::\n\n```{julia}\nusing Distributions, CairoMakie, Random\n\nRandom.seed!(1234)\n\nalpha = 0.05\nL(obs) = obs - (1 - sqrt(alpha))\nU(obs) = obs + (1 - sqrt(alpha))\n\nmu = 5.57\ntriDist = TriangularDist(mu - 1, mu + 1, mu)\n\nN = 100\nbounds = zeros(N, 2)\nhits = Vector{Symbol}(undef, N)\nfor i in 1:N\n    obs = rand(triDist)\n    LL, UU = L(obs), U(obs)\n    bounds[i, :] = [LL UU]\n    if LL <= mu && mu <= UU\n        hits[i] = :blue\n    else\n        hits[i] = :red\n    end\nend\n\nfig, ax = rangebars(1:N, bounds[:, 1], bounds[:, 2], color=hits)\nhlines!(ax, mu; color=:green, label=\"Parameter value\")\naxislegend(ax)\nfig\n```\n\n### Hypothesis tests\n\nThe approach involves partitioning the parameter space $\\Theta$ into $\\Theta_0$ and $\\Theta_1$, and then, based on the sample, concluding whether one of the two hypotheses, $H_0 : \\theta \\in \\Theta_0$ or $H_1 : \\theta \\in \\Theta_1$ holds.\n\nThe hypothesis $H_0$ is called the **null hypothesis** and $H_1$ the **alternative hypothesis**. The former is the default hypothesis, and in carrying out hypothesis testing our general aim is to reject this hypothesis.\n\nSince our decision is based on a random sample, there is always a chance of making a mistakenly false conclusion. There are two types of errors that can be made in carrying out a hypothesis testing.\n\n![Type I and type II erorrs with their probabilities $\\alpha$ and $\\beta$ respectively](./figures/two_types_of_errors_in_carrying_out_hypothesis_testing.png){.lightbox fig-alt=\"Click to see a larger version of the image\" fig-align=\"center\"}\n\nNote that $1-\\beta$ is known as the **power** of the hypothesis test.\n\n::: {.callout-important title=\"How to understand: fail to reject $H_0$ and reject $H_0$\"}\n\nNote that in carrying out a hypothesis test, $\\alpha$ is typically specified, while power is not direcly controlled, but rather is influenced by the sample size and other factors.\n\nAn important point in terminology is that we don't use the phrase \"accept\" for the null hypothesis, rather we \"**fail to reject it**\" (if we stick with $H_0$) or \"**reject it**\" (if we choose $H_1$). This is because when we fail to reject $H_0$, we typically don't know the actual value of $\\beta$, hence we aren't able to put a level of certainty on $H_0$ being the case. In other words, when we fail to reject $H_0$, this doesn't mean that $H_0$ is true. We just haven't enough evidence to reject it based on the sample data. This means that we are still likely to make type II error with the probability $\\beta$ (i.e. in reality, $H_0$ is false, but we fail to reject it) though we fail to reject $H_0$. Because we usualy don't know the actual value of $\\beta$, we cannot give such an assertion that $H_0$ is true with a specified confidence level. This is why we just say that we fail to reject $H_0$ based on the sample data, instead of $H_0$ is true.\n\nHowever, if we do reject $H_0$, then by the design of hypothesis tests we can say that our error probability is bounded by $\\alpha$.\n\n:::\n\n#### How to design and perform a hypothesis testing\n\n1. Formulate the scientific question as a hypothesis by partitioning the parameter space $\\Theta$ into $\\Theta_0$ and $\\Theta_1$, and then formimg the two hypotheses $H_0 : \\theta \\in \\Theta_0$ and $H_1 : \\theta \\in \\Theta_1$.\n\n2. Define the **test statistic**, denoted $X^*$, as a function of the sample data.\n\nSince the test statistic follows some distribution under $H_0$, the next step is to consider **how likely** it is to observe the specific value ($X^*$) calculated from the sample data under $H_0$.\n\nTo this end, in setting up a hypothesis test, we typically choose a significance level $\\alpha$ (e.g. $0.05$ or $0.01$), which quantifies our level of tolerance for enduring a type I error. For example, setting $\\alpha = 0.01$ implies we wish to design a test where the probability of type I error is **at most** $0.01$ if $H_0$ holds. Clearly, a low $\\alpha$ is desirable, however, there are tradeoffs involved since seeking a very low $\\alpha$ will imply a high $\\beta$ (low power).\n\n3. Pick a significance level $\\alpha$.\n\nConsider that we have a series of sample observations distributed as continuous uniform between $0$ and some unknown upper bound $m$.\n\nSay we set\n\n$$\nH_0: m=1,\\ \\ \\ \\ H_1: m<1\n$$\n\nwith observations $X_1, ..., X_n$, one possible test statistic is the sample range:\n\n$$\nX^* = max(X_1, ..., X_n) - min(X_1, ..., X_n)\n$$\n\nAs is always the case, the test statistic is a random variable. Under $H_0$, we expect the distribution of $X^*$ to have support $[0, 1]$ with the most likely value being close to $1$. This is because low values of $X^*$ are less plausible under $H_0$. The explicit form of the distribution of $X^*$ can be analytically obtained however for simplicity we use a Monte Carlo simulation to estimate it and present the density.\n\nFor this case, it is sensible to reject $H_0$ if $X^*$ is small enough.\n\n4. Performing hypothesis testing.\n\nAt present, there are two alternatives for performing hypothesis tests:\n\n* Using **rejection region**:\n\nDenoting **quantiles** of this distribution by $q_0(u)$, then we set the rejection region as $R = [0, q_0(\\alpha)]$. Using Monte Carlo, we compute the rejection region where the critical value is the upper boundary $q_0(\\alpha)$ of the rejection region in this case. Note that computing the rejection region does not require any sample data as it is based on model assumptions and not the sample.\n\nThe decision rule for this hypothesis test is simple: compare the observed value of the test statistic, $x^*$, to the critical value $q_0(\\alpha)$ and reject $H_0$ if $x^*\\le q_0(\\alpha)$, otherwise do not reject.\n\n* Using **p-value**:\n\nWe collect the data and compute the observed value of the test statistic $x^*$. The p-value is then the maximal $\\alpha$ under which the test would be rejected with the observed test statistic. In other words, we find $p$ which solves $x^* = q_0(p)$. This is computed via $F_0(x^*)$, where $F(\\cdot)$ is the CDF of $X^*$.\n\nUsing the p-vaue approach, reporting a low p-value implies that we are very confident in rejecting $H_0$, while a high p-value implies we are not. The p-value approach can be used to decide whether $H_0$ should be rejected or not with a specified $\\alpha$. For this case, simply comparing $p$ and $\\alpha$, and reject $H_0$ if $p\\le \\alpha$.\n\n![Using rejection region or p-value to perform hypothesis testing](./figures/using_rejection_region_or_p-value_to_perform_hypothesis_testing.jpg){.lightbox fig-alt=\"Click to see a larger version of the image\" fig-align=\"center\"}\n\n```{julia}\nusing Distributions, CairoMakie, Random, Statistics\n\nRandom.seed!(1)\n\nn, N, alpha = 10, 10^7, 0.05\nmActual = 0.7\ndist0, dist1 = Uniform(0, 1), Uniform(0, mActual)\n\nts(sample) = maximum(sample) - minimum(sample)\n\nempiricalDistUnderH0 = [ts(rand(dist0, n)) for _ in 1:N]\nrejectionValue = quantile(empiricalDistUnderH0, alpha)\n\nsamples = rand(dist1, n)\ntestStat = ts(samples)\npValue = sum(empiricalDistUnderH0 .<= testStat) / N\n\nif testStat > rejectionValue\n    println(\"Do not reject: \", round(testStat; digits=4), \" > \", round(rejectionValue; digits=4))\nelse\n    println(\"Reject: \", round(testStat; digits=4), \" <= \", round(rejectionValue; digits=4))\nend\nprintln(\"p-value = \", round(pValue; digits=4))\n\nfig, ax = stephist(empiricalDistUnderH0; bins=100, color=:blue, normalization=:pdf)\nvlines!(ax, testStat; color=:red, label=\"Observed test statistic\")\nvlines!(ax, rejectionValue; color=:black, label=\"Critical value boundary\")\naxislegend(ax; position=:lt)\nax.title = L\"\\text{The distribution of the test statistic }X^*\\text{ under }H_0\"\nfig\n```\n\n#### Understand type I and type II errors\n\nWhen the alternative parameter spaces $\\Theta_0$ and $\\Theta_1$ are only comprised of a single point each, the hypothesis test is called a **simple hypothesis test**.\n\nConsider a container that conatins two identical types of pipes, except that one type weighs $15$ grams on average and the other $18$ grams on avearge. The standard deviation of the weights of both pipe types is $2$ grams.\n\nImagine that we sample a single pipe, and wish to determine its type. Denote the weight of this pipe by the random variable $X$. For this example, we devise the following statistical hypothesis test: $\\Theta_0 = {15}$ and $\\Theta_1 = {18}$. Now, given a threshold $\\tau$, we reject $H_0$ if $X > \\tau$, otherwise we retain $H_0$.\n\nIn this case, we can explicitly analyze the probabilities of both the type I and type II errors, $\\alpha$ and $\\beta$ respectively.\n\n```{julia}\nusing Distributions, StatsBase, CairoMakie\n\nmu0, mu1, sd, tau = 15, 18, 2, 17.5\ndist0, dist1 = Normal(mu0, sd), Normal(mu1, sd)\ngrid = 5:0.1:25\nh0grid, h1grid = tau:0.1:25, 5:0.1:tau\n\n# CCDF: complementary CDF\nprintln(\"Probability of Type I error: \", ccdf(dist0, tau))\nprintln(\"Probability of Type II error: \", cdf(dist1, tau))\n\nfig, ax = lines(grid, pdf.(dist0, grid); color=:red, label=\"Bolt type 15g\")\nband!(ax, h0grid, repeat([0], length(h0grid)), pdf.(dist0, h0grid); color=(:red, 0.2))\nlines!(ax, grid, pdf.(dist1, grid); color=:green, label=\"Bolt type 18g\")\nband!(ax, h1grid, repeat([0], length(h1grid)), pdf.(dist1, h1grid); color=(:green, 0.2))\nlinesegments!(ax, [tau, 25], [0, 0]; color=:black, label=\"Rejection region\", linewidth=5)\ntext!([15, 18, 15.5, 18.5], [0.21, 0.21, 0.02, 0.02]; text=[L\"H_0\", L\"H_1\", L\"\\beta\", L\"\\alpha\"], align=repeat([(:center, :center)], 4), fontsize=28)\naxislegend(ax)\nfig\n```\n\n#### The Receiver Operating Curve (ROC)\n\nIn the previous example, $\\tau = 17.5$ was arbitrarily chosen.\n\nClearly, if $\\tau$ was increased, the probability of making a type I error, $\\alpha$, would decrease, while the probability of making a type II error, $\\beta$, would increase. Conversely, if we decreased $\\tau$ the reverse would occur.\n\nHere we use the Receiver Operating Curve (ROC) to help to visualize the tradeoff between type I and type II errors. It allows us to visualize the error tradeoffs for all possible $\\tau$ values simutaneously for a particular alternative hypothesis $H_1$.\n\nBelow we plot the analytic coordinates of $(\\alpha(\\tau), 1-\\beta(\\tau))$. This is the ROC. It is a parametric plot of the probability of a type I error and power.\n\n```{julia}\nusing Distributions, StatsBase, CairoMakie\n\nmu0, mu1a, mu1b, mu1c, sd = 15, 16, 18, 20, 2\ntauGrid = 5:0.1:25\n\ndist0 = Normal(mu0, sd)\ndist1a, dist1b, dist1c = Normal(mu1a, sd), Normal(mu1b, sd), Normal(mu1c, sd)\n\nfalsePositive = ccdf.(dist0, tauGrid)\ntruePositiveA, truePositiveB, truePositiveC = ccdf.(dist1a, tauGrid), ccdf.(dist1b, tauGrid), ccdf.(dist1c, tauGrid)\n\nfig, ax = ablines(0, 1; color=:black, linestyle=:dash, label=L\"H_0 = H_1 = 15\")\nlines!(ax, falsePositive, truePositiveA; color=:blue, label=L\"H_{1a}: \\mu_1 = 16\")\nlines!(ax, falsePositive, truePositiveB; color=:red, label=L\"H_{1b}: \\mu_1 = 18\")\nlines!(ax, falsePositive, truePositiveC; color=:green, label=L\"H_{1c}: \\mu_1 = 20\")\nax.xlabel = L\"\\alpha\"\nax.ylabel = L\"\\text{Power }(1-\\beta)\"\naxislegend(ax; position=:rb)\nfig\n```\n\n#### A randomized hypothesis test\n\nWe now investigate the concept of a **randomization test**, which is a type of **non-parametric test**, i.e. a statistical test which does not require that we know what type of distribution the data comes from.\n\nA virtue of non-parametric tests is that they do not impose a specific model.\n\nConsider the following example, where a farmer wants to test whether a new fertilizer is effective at increasing the yield of her tomato plants. As an experiment, she took $20$ plants, kept $10$ as controls, and treated the remaining $10$ with fertilizer, After $2$ months, she harvested the plants and recorded the yield of each plant in kg as shown in the following table:\n\n![Yield in kg for $10$ plants with, and $10$ plants without fertilizer (control)](./figures/an_tomato_example_for_randomization_test.png){.lightbox fig-alt=\"Click to see a larger version of the image\" fig-align=\"center\"}\n\nIt can be observed that the group of plants treated with fertilizer have an average yield $0.494$ kg\ngreater than that of the control group. One could argue that this difference is due to the effects of\nthe fertilizer. We now investigate if this is a reasonable assumption. Let us assume for a moment that the fertilizer had no effect on plant yield ($H_0$) and that the result was simply due to random\nchance. In such a scenario, we actually have $20$ observations from the same group, and regardless of\nhow we arrange our observations, we would expect to observe similar results.\n\nHence, we can investigate the likelihood of this outcome occurring by random chance, by considering all possible combinations of $10$ samples from our group of $20$ observations, and counting how many of these combinations result in a difference in sample means greater than or equal to $0.494$ kg. The proportion of times this occurs is analogous to the likelihood that the difference we observe in our sample means was purely due to random chance. It is in a sense the p-value.\n\nBefore proceeding, we calculate the number of ways one can sample $r = 10$ unique items from $n = 20$ total, which is given by\n\n$$\n\\binom{20}{10} = 184,756\n$$\n\nHence, the number of possible combinations in our example is computationally manageable. Note that in a different situation where $n$ and $r$ would be bigger, e.g. $n = 40$ and $r = 20$, the number of\ncombinations would be too big for an exhaustive search (about $137$ billion). In such a case, a viable alternative is to randomly sample combinations for estimating the p-value.\n\n```{julia}\nusing Combinatorics, Statistics, DataFrames, CSV\n\ndf = CSV.read(\"./data/fertilizer.csv\", DataFrame)\ncontrol = df.Control\nfertilizer = df.FertilizerX\n\nsubGroups = collect(combinations([control; fertilizer], 10))\nmeanFert = mean(fertilizer)\npVal = sum([mean(i) >= meanFert for i in subGroups]) / length(subGroups)\nprintln(\"p-value = \", pVal)\n```\n\n### Bayesian statistics\n\nIn the Bayesian paradigm, the scalar or vector of parameter $\\theta$ is not assumed to exist **as some fixed unknown quantity** but instead is assumed to **follow a distribution**.\n\nThat is, the parameter itself, is a random variable, and the act of Bayesian inference is the process of obtaining more information about the distribution of $\\theta$.\n\nThis allows us to incorparate **prior beliefs** about the parameter before experience from new observations is taken into consideration.\n\nThe key objects at play are **the prior distribution** of the parameter and **the posterior distribution** of the parameter.\n\nThe former is postulated beforehand, or exists as a consequence of previous inference, while the latter captures the distribution of the parameter after observations are taken into consideration.\n\nThe relationship between the prior and the posterior is\n\n$$\n\\text{posterior} = \\frac{\\text{likelihood}\\times\\text{prior}}{evidence}\\ \\ \\ \\ \\text{or}\\ \\ \\ \\ f(\\theta|x) = \\frac{f(x|\\theta)\\times f(\\theta)}{\\int f(x|\\theta)f(\\theta)d\\theta}\n$$\n\nThis is nothing but Bayes's rule $P(B_i|A) = \\frac{P(A|B_i)P(B_i)}{\\sum_{j=1}^{n}P(A|B_j)P(B_j)}, A\\in \\cup_{i=1}^n B_i$ applied to densities.\n\nHere the prior distribution (density) is $f(\\theta)$, and the posterior distribution (density) is $f(\\theta|x)$.\n\nObserve that the denominator, known as **evidence** or **marginal likelihood**, is constant with respect to the parameter $\\theta$. This allows the equation to be written as\n\n$$\nf(\\theta|x) \\propto f(x|\\theta)\\times f(\\theta)\n$$\n\nHence, the posterior distribution can be easily obtained up to the normalizing constant (the evidence) by multiplying the prior with the likelihood.\n\nIn general, carrying out Beyesian inference involves the following steps:\n\n1. Assume some distributional model for the data based on the parameter $\\theta$ which is a random variable.\n\n2. Use previous inference experience, elicit an expert, or make an educated guess to determine a prior distribution $f(\\theta)$. The prior distribution might be parameterized by its own parameters, called **hyper-parameters**.\n\n3. Collect data $x$ and create an expression or a computational mechanism for the likelihood $f(x|\\theta)$ based on the distributional model chosen.\n\n4. Use the Bayes's rule applied to densities to obtain the posterior distribution of the parameters $f(\\theta|x)$.\n\nIn most cases, the evidence (the denominator) is not easily computable. Hence the posterior distribution is only available up to a normalizing constant. In some special cases, the form of the posterior distribution is the same as the prior distribution. In such cases, **conjugacy** holds, the prior is called a **conjugate prior**, and the hyper-parameters are updated from prior to posterior.\n\n5. The posterior distribution can then be used to make conclusions about the model.\n\nFor example, if a single specific parameter value is needed to make the model concrete, a Bayes estimate based on the posterior distribution, for example, the posterior mean, may be computed:\n\n$$\n\\hat{\\theta} = \\int \\theta f(\\theta|x) d\\theta\n$$\n\nFurther analyses such as obtaining credible intervals, similar to condidence intervals, may also be carried out.\n\n6. The model with $\\hat{\\theta}$ can then be used for making conclusions. Alternatively, a whole class of models based on the posterior distribution $f(\\hat{\\theta}|x)$ can be used. This often goes hand in hand with simulation as one is able to generate Monte Carlo samples from the posterior distribtion.\n\n#### A Poisson example\n\nConsider an example where an insurance company models **the number of weekly fires** in a city using a **Poisson distribution** with parameter $\\lambda$. Here, $\\lambda$ is also the expected number of fires per week.\n\nAssume that the following data is collected over a period of $16$ weeks:\n\n$$\nx = (x_1, ..., x_{16}) = (2, 1, 0, 0, 1, 0, 2, 2, 5, 2, 4, 0, 3, 2, 5, 0)\n$$\n\nEach data point indicates the number of fires per week.\n\nIn this case, the MLE is $\\hat{\\lambda} = 1.8125$ simply obtained by the sample mean. Hence, **in a frequentist approach**, after $16$ weeks, the distribution of the number of fires per week is modeled by a Poisson distribution with $\\lambda = 1.8125$. One can then obtain estimates for say, the probability of having more than $5$ fires in a given week as follows:\n\n$$\nP(\\text{fires per week} > 5) = 1 - \\sum_{k=0}^{5} e^{-\\lambda \\frac{\\lambda^k}{k!}} \\approx 0.0107\n$$\n\nHowever, the drawback of such an approach in estimating $\\lambda$ is that it didn't make use of previous information.\n\nSay that for example, further knowledge comes to light that the number of fires per week ranges between $0$ and $10$, and that the typical number is $3$ fires per week. In this case, one can assign a prior distribution to $\\lambda$ that captures this belief.\n\nIn this example, we can assume that we decide to use a triangular distribution which captures prior beliefs about the parameter $\\lambda$ well because it has a defined range and a defined mode.\n\nWith the prior assigned and the data collected, we can use the machinery of Bayesian inference.\n\nIn this case, the prior distibution of the parameter $\\lambda$ is the triangular distribution with the PDF:\n\n$$\nf(\\lambda) = \\begin{cases}\n\\frac{1}{15}\\lambda, & \\lambda \\in [0,3] \\\\\n\\frac{1}{35}(10-\\lambda), & \\lambda \\in (3, 10]\n\\end{cases}\n$$\n\nWith the $16$ observations, $x_1, ..., x_{16}$, the likelihood is\n\n$$\nf(x|\\lambda) = \\prod_{k=1}^{16} e^{-\\lambda \\frac{\\lambda^k}{k!}}\n$$\n\nHence, the posterior distribution $f(\\lambda|x) \\propto f(x|\\lambda)f(\\lambda)$, then dividing by the evidence,\n\n$$\n\\int_0^{10} f(x|\\lambda)f(\\lambda)d\\lambda\n$$\n\n```{julia}\nusing Distributions, CairoMakie\n\nalpha, beta = 8, 2\nprior(lam) = pdf(Gamma(alpha, 1 / beta), lam)\nd = [2, 1, 0, 0, 1, 0, 2, 2, 5, 2, 4, 0, 3, 2, 5, 0]\n\nlike(lam) = *([pdf(Poisson(lam), x) for x in d]...)\nposteriorUpToK(lam) = like(lam) * prior(lam)\n\ndelta = 10^-4\nlamRange = 0:delta:10\nK = sum([posteriorUpToK(lam) * delta for lam in lamRange])\nposterior(lam) = posteriorUpToK(lam) / K\n\nbayesEsitmate = sum([lam * posterior(lam) * delta for lam in lamRange])\n\nprintln(\"Computational Bayes Estimate: \", bayesEsitmate)\n\nfig, ax = lines(lamRange, prior.(lamRange); color=:blue, label=\"Prior distribution\")\nlines!(ax, lamRange, posterior.(lamRange); color=:red, label=\"Posterior distribution\")\nax.limits = (0, 10, 0, 1.2)\nax.xlabel = L\"\\lambda\"\nax.ylabel = \"Density\"\naxislegend(ax)\nfig\n```\n\n#### Congugate prior example\n\nThe gamma distribution is said to be a **conjugate** prior to the Poisson distribution, which means that both the prior distribution and the posterior distribution are gamma distributions when the data is distributional as a Poisson distribution. In this case, the hyper-parameters typically have a simple update law from prior to posterior.\n\nIn the case of a **gamma-Poisson conjugate pair**, assume the hyper-parameters of the prior to have $\\alpha$ (shape parameter) and $\\beta$ (rate parameter). We have\n\n$$\n\\begin{align}\n\\text{posterior} &\\propto \\text{likelihood}\\times \\text{prior} \\\\\n&\\propto \\left(\\prod_{k=1}^n e^{-\\lambda \\frac{\\lambda^k}{k!}}\\right) \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} e^{-\\beta\\lambda} \\\\\n&= \\lambda^{\\alpha+(\\sum_{k=1}^{n} x_k)-1}e^{-\\lambda(\\beta+n)} \\\\\n&\\propto \\text{gamma density with shape parameter } \\alpha + \\sum x_i \\text{ and rate parameter } \\beta+n\n\\end{align}\n$$\n\nThe hyper-parameter $\\alpha$ is updated to $\\alpha + \\sum x_i$ and the hyper-parameter $\\beta$ is updated to $\\beta + n$.\n\n```{julia}\nusing Distributions, CairoMakie\n\nalpha, beta = 8, 2\nprior(lam) = pdf(Gamma(alpha, 1 / beta), lam)\nd = [2, 1, 0, 0, 1, 0, 2, 2, 5, 2, 4, 0, 3, 2, 5, 0]\n\nlike(lam) = *([pdf(Poisson(lam), x) for x in d]...)\nposteriorUpToK(lam) = like(lam) * prior(lam)\n\ndelta = 10^-4\nlamRange = 0:delta:10\nK = sum([posteriorUpToK(lam) * delta for lam in lamRange])\nposterior(lam) = posteriorUpToK(lam) / K\n\nbayesEsitmate = sum([lam * posterior(lam) * delta for lam in lamRange])\n\nnewAlpha, newBeta = alpha + sum(d), beta + length(d)\nclosedFormBayesEstimate = mean(Gamma(newAlpha, 1 / newBeta))\n\nprintln(\"Computational Bayes Estimate: \", bayesEsitmate)\nprintln(\"Closed form Bayes Estimate: \", closedFormBayesEstimate)\n\nfig, ax = lines(lamRange, prior.(lamRange); color=:blue, label=\"Prior distribution\")\nlines!(ax, lamRange, posterior.(lamRange); color=:red, label=\"Posterior distribution\")\nax.limits = (0, 10, 0, 1.2)\nax.xlabel = L\"\\lambda\"\nax.ylabel = \"Density\"\naxislegend(ax)\nfig\n```\n\n#### Markov Chain Monte Carlo example\n\nIn cases where the dimension of the parameter space is high, carrying out straighforward integration is not possible.\n\nHowever, there are other ways of carrying out Bayesian inference.\n\nOne such popular way is by using algorithms that fall under the category known as **Markov Chain Monte Carlo** (MCMC).\n\nThe **Metropolis-Hastings** algorithm is one such popular MCMC algorithm.\n\nIt produces a series of samples $\\theta(1), \\theta(2), \\theta(3), ...$, where it is guaranteed that for large $t$, $\\theta(t)$ is distributed according to the posterior distribution.\n\nTechnically, the random sequence $\\{\\theta(t)\\}_{t=1}^\\infty$ is a Markov chain and it is guaranteed that the stationary distribution of this Markov chain is the specified posterior distribution. That is, the posterior distribution is an input parameter to the algorithm.\n\nThe major bebefits of Metropolis-Hastings and similar MCMC algorithms is that **they only use the ratios of the posterior on different parameter values**. For example, for parameter $\\theta_1$ and $\\theta_2$, the algorithm only uses the posterior distribution via the ratio\n\n$$\nL(\\theta_1, \\theta_2) = \\frac{f(\\theta_1|x)}{f(\\theta_2|x)}\n$$\n\nThis means that the normalizing constant (evidence) is not needed as it is implicitly canceled out. Thus, using $\\text{psoterior} \\propto \\text{likelihood}\\times \\text{prior}$ suffices.\n\nFurther to the posterior distribution, an additional input parameter to Metropolis-Hastings is the so-called **proposal density**, denoted by $q(\\cdot|\\cdot)$. This is a family of distributions where given a certain value of $\\theta_1$ taken as a parameter, the new value, say $\\theta_2$ is distributed with PDF\n\n$$\nq(\\theta_2|\\theta_1)\n$$\n\nThe idea of Metropolis-Hastings is to walk around the parameter space by randomly generating new values using $q(\\cdot|\\cdot)$.\n\nAt each step, some new values are accepted while others are not, all in a manner which ensures the desired limiting behavior. The algorithm specification is to accept with probability\n\n$$\nH = \\min\\left\\{1,\\ L(\\theta^*, \\theta(t)) \\frac{q(\\theta(t)|\\theta^*)}{q(\\theta^*|\\theta(t))}\\right\\}\n$$\n\nwhere $\\theta^*$ is the new proposed value, generated via $q(\\cdot|\\theta(t))$, and $\\theta(t)$ is the current value.\n\nWith each such iteration, the new value is accepted with probability $H$ and rejected otherwise. With certian technical requirements on the posterior and proposal densities, the theory of Markov chains then guarantees that the stationary distribution of the sequence $\\{\\theta(t)\\}$ is the posterior distribution.\n\nDifferent variants of the Metropolis-Hastings algorithm employ different types of proposal densities. There are also generalizations and extensions that we don't discuss here, such as *Gibbs Sampling* and *Hamiltonian Monte Carlo*.\n\nAs an example, we use the *folded normal distribution* as a proposal density. This distribution is achieved by taking a normal random variable $X$ with mean $\\mu$ and variance $\\sigma^2$ and considering $Y=|X|$. In this case, the PDF of $Y$ is\n\n$$\nf(y) = \\frac{1}{\\sigma 2\\pi} \\left(e^{-\\frac{(y-\\mu)^2}{2\\sigma^2}} + e^{-\\frac{(y+\\mu)^2}{2\\sigma^2}}\\right)\n$$\n\nIt supports to generate the non-negative parameters in question.\n\n```{julia}\nusing Distributions, CairoMakie\n\nalpha, beta = 8, 2\n# prior with Gamma distribution which has two hyper-parameters α and β\nprior(lam) = pdf(Gamma(alpha, 1 / beta), lam)\nd = [2, 1, 0, 0, 1, 0, 2, 2, 5, 2, 4, 0, 3, 2, 5, 0]\n\n# data with Poisson distribution which has a parameter λ\nlike(lam) = *([pdf(Poisson(lam), x) for x in d]...)\n# posterior ∝ likelihood * prior\nposteriorUpToK(lam) = like(lam) * prior(lam)\n\nsig = 0.5\n# use the folded normal distribution as the proposal density\n# with each parameter θ1, it produces a new parameter θ2\nfoldedNormalPDF(x, mu) = (1 / sqrt(2 * pi * sig^2)) * (exp(-(x - mu)^2 / 2sig^2) + exp(-(x + mu)^2 / 2sig^2))\nfoldedNormalRV(mu) = abs(rand(Normal(mu, sig)))\n\nfunction sampler(piProb, qProp, rvProp)\n    lam = 1\n    warmN, N = 10^5, 10^6\n    samples = zeros(N - warmN)\n\n    for t in 1:N\n        while true\n            lamTry = rvProp(lam)\n            Lo = piProb(lamTry) / piProb(lam)\n            H = min(1, Lo * qProp(lam, lamTry) / qProp(lamTry, lam))\n            if rand() < H\n                lam = lamTry\n                if t > warmN\n                    samples[t-warmN] = lam\n                end\n                break\n            end\n        end\n    end\n    return samples\nend\n\nmcmcSamples = sampler(posteriorUpToK, foldedNormalPDF, foldedNormalRV)\nprintln(\"MCMC Bayes Estimate: \", mean(mcmcSamples))\n\nfig, ax = stephist(mcmcSamples; bins=100, color=:black, normalization=:pdf, label=\"Histogram of MCMC samples\")\n\nlamRange = 0:0.01:10\nlines!(ax, prior.(lamRange); color=:blue, label=\"Prior distribution\")\n\nclosedFormPosterior(lam) = pdf(Gamma(alpha + sum(d), 1 / (beta + length(d))), lam)\nlines!(ax, lamRange, closedFormPosterior.(lamRange); color=:red, label=\"Posterior distribution\")\n\nax.limits = (0, 10, 0, 1.2)\nax.xlabel = L\"\\lambda\"\nax.ylabel = \"Density\"\naxislegend(ax)\nfig\n```\n\n## Confidence intervals\n\nIn the setting of **symmetric** sampling distributions, a typical formula for the confidence interval $[L, U]$ is of the form\n\n$$\n\\hat{\\theta} \\pm K_\\alpha s_\\text{err}\n$$\n\nHere, $\\hat{\\theta}$ is typically the point estimate for the parameter in question, $s_\\text{err}$ is some measure or estimate of the variability (e.g. standard error) of the parameter, and $K_\\alpha$ is a constant which depends on the model at hand and on $\\alpha$. Typically by decreasing $\\alpha \\rightarrow 0$, we have that $K_\\alpha$ increases, implying a wider confidence interval. In addition, the specific form of $K_\\alpha$ often depends on conditions such as\n\n- **Sample size**: is the sample size small or large.\n\n- **Variance**: is the variance $\\sigma^2$ known or unknown.\n\n- **Distribution**: is the population assumed normally distributed or not.\n\n### Single sample confidence intervals for the mean\n\nAssume that we want to estimate the population mean $\\mu$ using a random sample, $X_1, ..., X_n$.\n\nAs mentioned before, an unbiased point estimate for the mean is the sample mean $\\overline{X}$, so a typical formula for the confidence interval of the mean is\n\n$$\n\\overline{X} \\pm K_\\alpha \\frac{S}{\\sqrt{n}}\n$$\n\n#### Population variance known\n\nIf we assume that the population variance $\\sigma^2$ is known and the data is normally distributed, then the sample mean $\\overline{X}$ is normally distributed with mean $\\mu$ and variance $\\frac{\\sigma^2}{n}$. This yields\n\n$$\nP\\left( \\mu - z_{1-\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}} \\le \\overline{X} \\le \\mu + z_{1-\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}} \\right) = 1-\\alpha\n$$\n\nwhere $z_{1-\\frac{\\alpha}{2}}$ is the $1-\\frac{\\alpha}{2}$ quantile of the standard normal distribution.\n\nBy rearranging the inequalities inside the probability statement above, we obtain the following confidence interval formula\n\n$$\n\\bar{x} \\pm z_{1-\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}}\n$$\n\nIn practice, $\\sigma^2$ is rarely known; hence, it is tempting to replace $\\sigma$ by $s$ (sample standard deviation) in the formula above. **Such a replacement is generally fine for large samples**.\n\nIn addition, the validity of the normality assumption should also be considered. In cases where the data is not normally distributed, the probability statement above only approximately holds. However, as $n \\rightarrow \\infty$, it quickly becomes precise due to the central limit theorem.\n\n```{julia}\nusing Distributions, HypothesisTests\n\nd = [\n    53.35674558144255,\n    53.45887516134873,\n    52.282838627926125,\n    52.98746570643515,\n    51.993167774733486,\n    53.373333606198,\n    55.75410538860477,\n    50.279496381439365,\n    53.6359586914001,\n    53.517705831707495,\n    53.70044994508253,\n    54.15592592604583,\n    53.55054914606446,\n    52.37319589109419,\n    53.4900750059897,\n    52.939458524079605,\n    52.16761562743534,\n    50.87140009591033,\n    53.144919157924924,\n    52.09084035473537,\n]\nxBar, n = mean(d), length(d)\nsig = 1.2\nalpha = 0.1\nz = quantile(Normal(), 1 - alpha / 2)\n\nprintln(\"Calculating formula: \", (xBar - z * sig / sqrt(n), xBar + z * sig / sqrt(n)))\nprintln(\"Using confint() function: \", confint(OneSampleZTest(xBar, sig, n); level=1 - alpha))\n```\n\n::: {.callout-note}\n\nAssume $U \\sim N(0, 1)$, and $X \\sim N(\\mu, \\sigma)$, we have\n\n$$\nU = \\frac{X-\\mu}{\\sigma}\n$$\n\n$$\nX = \\mu + U\\sigma\n$$\n\n$$\nP(-z_{1-\\frac{\\alpha}{2}} \\le U \\le z_{1-\\frac{\\alpha}{2}}) = 1-\\alpha\n$$\n\n$$\nP(\\mu - z_{1-\\frac{\\alpha}{2}}\\sigma \\le X \\le \\mu + z_{1-\\frac{\\alpha}{2}}\\sigma) = 1-\\alpha\n$$\n\n:::\n\n#### Population variance unknown\n\nIn cases where the population variance is unknown, if we replace $\\sigma$ by the sample standard deviation $s$, we can use the T-distribution instead of the normal distribution to obtain the confidence interval for the mean\n\n$$\n\\bar{x} \\pm t_{1-\\frac{\\alpha}{2}, n-1} \\frac{s}{\\sqrt{n}}\n$$\n\nwhere $t_{1-\\frac{\\alpha}{2}, n-1}$ is the $1-\\frac{\\alpha}{2}$ quantile of a T-distribution with $n-1$ degrees of freedom.\n\nFor small samples, the replacement of $z_{1-\\frac{\\alpha}{2}}$ by $t_{1-\\frac{\\alpha}{2}, n-1}$ significantly affects the width of the confidence interval, as for the same value of $\\alpha$, the T case is wider.\n\nHowever, as $n \\rightarrow \\infty$, we have, $t_{1-\\frac{\\alpha}{2}, n-1} \\rightarrow z_{1-\\frac{\\alpha}{2}}$.\n\n```{julia}\nusing Distributions, HypothesisTests\n\nd = [\n    53.35674558144255,\n    53.45887516134873,\n    52.282838627926125,\n    52.98746570643515,\n    51.993167774733486,\n    53.373333606198,\n    55.75410538860477,\n    50.279496381439365,\n    53.6359586914001,\n    53.517705831707495,\n    53.70044994508253,\n    54.15592592604583,\n    53.55054914606446,\n    52.37319589109419,\n    53.4900750059897,\n    52.939458524079605,\n    52.16761562743534,\n    50.87140009591033,\n    53.144919157924924,\n    52.09084035473537,\n]\nxBar, n = mean(d), length(d)\ns = std(d)\nalpha = 0.1\nt = quantile(TDist(n - 1), 1 - alpha / 2)\n\nprintln(\"Calculating formula: \", (xBar - t * s / sqrt(n), xBar + t * s / sqrt(n)))\nprintln(\"Using confint() function: \", confint(OneSampleTTest(xBar, s, n); level=1 - alpha))\n```\n\n### Two sample confidence intervals for the difference in means\n\nIt is often of interest to estimate the difference between the population means, $\\mu_1 - \\mu_2$.\n\nFirst we collect two random samples, $x_{i,1}, ..., x_{i,n}$ for $i = 1,2$, each with the sample mean $\\bar{x}_i$ and sample standard deviation $s_i$. In addition, the difference of sample means, $\\bar{x}_1 - \\bar{x}_2$ serves as a point estimate for the difference in population means, $\\mu_1 - \\mu_2$.\n\nA confidence interval for $\\mu_1 - \\mu_2$ around the point estimate $\\bar{x}_1 - \\bar{x}_2$ is then constructed via the same process seen previously\n\n$$\n\\bar{x}_1 - \\bar{x}_2 \\pm K_\\alpha s_\\text{err}\n$$\n\n#### Population variances known\n\nIn cases where the population variances are known, we may explicitly compute\n\n$$\nVar(\\overline{X}_1 - \\overline{X}_2) = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\n$$\n\nHence, the standard error is given by\n\n$$\ns_\\text{err} = \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\n$$\n\nWhen combined with the assumption that the data is normally distributed, we can derive the following confidence interval\n\n$$\n\\bar{x}_1 - \\bar{x}_2 \\pm z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\n$$\n\n#### Population variances unknown and assumed equal\n\nIn cases where the population variances are unknown but assumed equal, denoted by $\\sigma^2$. Based on this assumption, it is sensible to use both sample variances to estimate $\\sigma^2$. This estimated variance using both samples is known as the pooled sample variance, and is given by\n\n$$\nS_p^2 = \\frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1+n_2-2}\n$$\n\nIn fact, it is a weighted average of the sample variances of the individual samples.\n\nIn this case, it can be shown that\n\n$$\nT = \\frac{\\overline{X}_1 - \\overline{X}_2 - (\\mu_1 - \\mu_2)}{S_\\text{err}}\n$$\n\nis distributed as a T-distribution with $n_1+n_2-2$ degreees of freedom, where the standard error is\n\n$$\nS_\\text{err} = S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n$$\n\nHence, we arrive at the following confidence interval\n\n$$\n\\bar{x}_1 - \\bar{x}_2 \\pm t_{1-\\frac{\\alpha}{2}, n_1+n_2-2} S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n$$\n\n```{julia}\nusing CSV, Distributions, HypothesisTests\n\nd1 = [\n    53.35674558144255,\n    53.45887516134873,\n    52.282838627926125,\n    52.98746570643515,\n    51.993167774733486,\n    53.373333606198,\n    55.75410538860477,\n    50.279496381439365,\n    53.6359586914001,\n    53.517705831707495,\n    53.70044994508253,\n    54.15592592604583,\n    53.55054914606446,\n    52.37319589109419,\n    53.4900750059897,\n    52.939458524079605,\n    52.16761562743534,\n    50.87140009591033,\n    53.144919157924924,\n    52.09084035473537,\n]\n\nd2 = [\n    48.23239011,\n    52.2735178,\n    52.07209917,\n    51.8813638,\n    50.89860065,\n    53.13910845,\n    50.88296219,\n    49.80725709,\n    49.04791179,\n    50.91491626,\n    50.73578183,\n    47.6154076,\n    50.89317122,\n    52.95593896,\n    51.90831274,\n    52.22159829,\n    51.60575821,\n    49.96704471,\n]\n\nxBar1, xBar2 = mean(d1), mean(d2)\nn1, n2 = length(d1), length(d2)\nalpha = 0.05\nt = quantile(TDist(n1 + n2 - 2), 1 - alpha / 2)\n\ns1, s2 = std(d1), std(d2)\nsP = sqrt(((n1 - 1) * s1^2 + (n2 - 1) * s2^2) / (n1 + n2 - 2))\n\nprintln(\"Calculating formula: \", (xBar1 - xBar2 - t * sP * sqrt(1 / n1 + 1 / n2), xBar1 - xBar2 + t * sP * sqrt(1 / n1 + 1 / n2)))\nprintln(\"Using confint() function: \", confint(EqualVarianceTTest(d1, d2); level=1 - alpha))\n```\n\n#### Population variances unkown and not assumed equal\n\nIn cases where the population variances are unknown and not assumed equal, the estimate for $S_\\text{err}$ is given by\n\n$$\nS_\\text{err} = \\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}\n$$\n\nHence, in this case the statistic $T = \\frac{(\\overline{X}_1 - \\overline{X}_2) - (\\mu_1 - \\mu_2)}{S_\\text{err}}$ is adapted to\n\n$$\nT = \\frac{\\overline{X}_1 - \\overline{X}_2 - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}}\n$$\n\nIt turns out that the above formula is only T-distributed with $n_1+n_2-2$ degrees of freedom **if the variances are equal**, otherwise, it isn't.\n\nHowever, the *Satterthwaite approximation* suggests that $T\\ \\ \\widetilde{approx}\\ \\ t(v)$, where the degrees of freedom $v$ is\n\n$$\nv = \\frac{\\left(\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}\\right)^2}{\\frac{\\left(s_1^2/n_1\\right)^2}{n_1-1} + \\frac{\\left(s_2^2/n_2\\right)^2}{n_2-1}}\n$$\n\nHence, we arrive at the confidence interval\n\n$$\n\\bar{x}_1 - \\bar{x}_2 \\pm t_{1-\\frac{\\alpha}{2}, v} \\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}\n$$\n\n```{julia}\nusing CSV, Distributions, HypothesisTests\n\nd1 = [\n    53.35674558144255,\n    53.45887516134873,\n    52.282838627926125,\n    52.98746570643515,\n    51.993167774733486,\n    53.373333606198,\n    55.75410538860477,\n    50.279496381439365,\n    53.6359586914001,\n    53.517705831707495,\n    53.70044994508253,\n    54.15592592604583,\n    53.55054914606446,\n    52.37319589109419,\n    53.4900750059897,\n    52.939458524079605,\n    52.16761562743534,\n    50.87140009591033,\n    53.144919157924924,\n    52.09084035473537,\n]\n\nd2 = [\n    48.23239011,\n    52.2735178,\n    52.07209917,\n    51.8813638,\n    50.89860065,\n    53.13910845,\n    50.88296219,\n    49.80725709,\n    49.04791179,\n    50.91491626,\n    50.73578183,\n    47.6154076,\n    50.89317122,\n    52.95593896,\n    51.90831274,\n    52.22159829,\n    51.60575821,\n    49.96704471,\n]\n\nxBar1, xBar2 = mean(d1), mean(d2)\ns1, s2 = std(d1), std(d2)\nn1, n2 = length(d1), length(d2)\nalpha = 0.05\n\nv = (s1^2 / n1 + s2^2 / n2)^2 / ((s1^2 / n1)^2 / (n1 - 1) + (s2^2 / n2)^2 / (n2 - 1))\nt = quantile(TDist(v), 1 - alpha / 2)\n\nprintln(\"Calculating formula: \", (xBar1 - xBar2 - t * sqrt((s1^2 / n1 + s2^2 / n2)), xBar1 - xBar2 + t * sqrt((s1^2 / n1 + s2^2 / n2))))\nprintln(\"Using confint() function: \", confint(UnequalVarianceTTest(d1, d2); level=1 - alpha))\n```\n\n#### More on the Satterthwaite approximation\n\nObserve that both sides of the \"distributed as\" ($\\sim$) symbol are random variables which depdend on the same random experiment.\n\nHence, the statement can be presented generally, as a case of the following format:\n\n$$\nX(\\omega) \\sim F_{h(\\omega)}\n$$\n\nwhere $\\omega$ is a point in the sample space.\n\nHere, $X(\\omega)$ is a random variabe, and $F$ is a distribution that depends on a parameter $h$, which depends on $\\omega$.\n\nIn our case of the Satterthwaite approximation, $h$ is $v$, defined above.\n\n::: {.callout-note title=\"Why both sides of the $\\sim$ symbol are random variables which depend on the same random experiment\"}\n\nWhen we had finished a random experiment, we obtained a sample $\\omega$ from the sample space.\n\nThen, we can derive some statistic ($X(\\omega)$) from the sample, which means that $X$ is a random variable. In addition, under the same experiment, $X$ should be distributed as some distribution $F$, which may be defined by some parameter $h$, saying the degrees of freedom. Obviously, $h$ itself depends on the same sample obtained from the sample space before.\n\n:::\n\nBy using the inverse probability transformation, the above formula is equivalent to\n\n$$\nF_{h(\\omega)}\\left(X(\\omega)\\right) \\sim \\text{Uniform}(0, 1)\n$$\n\nThis means that the Satterthwaite approximation is a better approximative distribution close to the true distribution which $T$ calculated in the case of population variances unknown and not assumed equal is distributed as in comparison with the alternative of treating $h$ as simply dependent on the number of observations made ($n_1+n_2-2$).\n\nWe can make a simple validation using Q-Q plot: $t(v)$ is more similar with the true distribution of $T$ than $t(n_1+n_2-2)$\n\n```{julia}\nusing Distributions, Statistics, Random, CairoMakie\n\nRandom.seed!(0)\n\nmu1, sig1, n1 = 0, 2, 8\nmu2, sig2, n2 = 0, 30, 15\n\ndist1 = Normal(mu1, sig1)\ndist2 = Normal(mu2, sig2)\n\nN = 10^6\ntdArray = Array{Tuple{Float64,Float64}}(undef, N)\n\nfunc(s1, s2, n1, n2) = (s1^2 / n1 + s2^2 / n2)^2 / ((s1^2 / n1)^2 / (n1 - 1) + (s2^2 / n2)^2 / (n2 - 1))\n\nfor i in 1:N\n    x1Data = rand(dist1, n1)\n    x2Data = rand(dist2, n2)\n    x1Bar, x2Bar = mean(x1Data), mean(x2Data)\n    s1, s2 = std(x1Data), std(x2Data)\n    tStatistics = (x1Bar - x2Bar) / sqrt(s1^2 / n1 + s2^2 / n2)\n    tdArray[i] = (tStatistics, func(s1, s2, n1, n2))\nend\nsort!(tdArray, by=first)\n\ninvVal(v, i) = quantile(TDist(v), i / (N + 1))\n\nxCoords = Array{Float64}(undef, N)\nyCoords1 = Array{Float64}(undef, N)\nyCoords2 = Array{Float64}(undef, N)\n\nfor i in 1:N\n    xCoords[i] = first(tdArray[i])\n    yCoords1[i] = invVal(last(tdArray[i]), i)\n    yCoords2[i] = invVal(n1 + n2 - 2, i)\nend\n\nfig, ax = qqplot(xCoords, yCoords1; color=:blue, label=\"Calculated v\", qqline=:identity)\nqqplot!(ax, xCoords, yCoords2; color=:red, label=\"Fixed v\")\naxislegend(ax; position=:lt)\nfig\n```\n\n### Confidence intervals for proportions\n\nIn certain inference settings the parameter of interest is a population proportion.\n\nWhen carrying out inference for a proportion we assume that there exists some unknown population proportion $p \\in (0, 1)$.\n\nWe then sample an i.i.d. sample of observations $I_1, ..., I_n$, where for the $i$'th observation, $I_i = 0$ if the event in question does not happen, and $I_i = 1$ if the event occurs.\n\nA natural estimator for the proportion is then the sample mean of $I_1, ..., I_n$, which we denote\n\n$$\n\\hat{p} = \\frac{\\sum_{i=1}^{n}I_i}{n}\n$$\n\nPlease note that $\\overline{X} \\sim N(\\mu, \\frac{\\sigma^2}{n})$ as $n \\rightarrow \\infty$, so is $\\hat{p}$.\n\nNow observe that each $I_i$ is a Bernoulli random variable with success probability $p$. Under the i.i.d assumption this means that the numerator of the above formula (i.e. $\\sum_{i=1}^{n}I_i$) is binomially distributed with parameters $n$ and $p$. Hence\n\n$$\nE\\left[\\sum_{i=1}^{n}I_i\\right] = np\n$$\n\nand\n\n$$\n\\text{Var}(\\sum_{i=1}^{n}I_i) = np(1-p)\n$$\n\nSo we have\n\n$$\nE(\\hat{p}) = p\n$$\n\nand\n\n$$\n\\text{Var}(\\hat{p}) = \\frac{p(1-p)}{n}\n$$\n\nHence, $\\hat{p}$ is an **unbiased** and **consistent** estimator of $p$. That is, **on average** $\\hat{p}$ estimates $p$ perfectly (unbiased), and if **more** observations are collected the variance of the estimator vanishes and then $\\hat{p} \\rightarrow p$ (consistent).\n\nFurthermore, we can use the central limit theorem to create a normal approximation for the distribution of $\\hat{p}$ and yield an approximate condidence interval.\n\nDue to the fact $\\hat{p} \\sim N(p, \\frac{p(1-p)}{n})$ as $n \\rightarrow \\infty$, we have\n\n$$\n\\tilde{Z}_n = \\frac{\\hat{p}-p}{\\sqrt{\\frac{p(1-p)}{n}}}\n$$\n\nwhich is a random variable that approximately follows a standard normal distribution. The approximation becomes exact as $n$ grows. Due to the fact that $\\hat{p}$ is an unbiased and consistent estimator of $p$, it's reasonable that we replace $p$ with $\\hat{p}$, and then we have\n\n$$\n\\hat{Z}_n = \\frac{\\hat{p}-p}{\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}}\n$$\n\nSo we have\n\n$$\nP(z_\\frac{\\alpha}{2} \\le \\hat{Z}_n \\le z_{1-\\frac{\\alpha}{2}}) \\approx 1-\\alpha\n$$\n\nand then along with the fact that $z_\\frac{\\alpha}{2} = -z_{1-\\frac{\\alpha}{2}}$ as follows\n\n$$\n\\begin{align}\n1-\\alpha &\\approx P(-z_{1-\\frac{\\alpha}{2}} \\le \\hat{Z}_n \\le z_{1-\\frac{\\alpha}{2}}) \\\\\n&= P(-z_{1-\\frac{\\alpha}{2}} \\le \\frac{\\hat{p}-p}{\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}} \\le z_{1-\\frac{\\alpha}{2}}) \\\\\n&= P(\\hat{p} - z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\le p \\le \\hat{p} + z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}})\n\\end{align}\n$$\n\nWe thus arrive the following approximate confidence interval for proportions\n\n$$\n\\hat{p} \\pm z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n$$\n\nSimilarly, we have the confidence interval for the case of two populations\n\n$$\n\\hat{p}_1 - \\hat{p}_2 \\pm z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n}}\n$$\n\n```{julia}\nusing CSV, DataFrames, CategoricalArrays, Distributions\n\ndat = CSV.read(\"./data/purchaseData.csv\", DataFrame)\nprintln(\"Levels of Grade: \", levels(dat.Grade))\nprintln(\"Data points: \", nrow(dat))\n\nn = sum(.!(ismissing.(dat.Grade)))\nprintln(\"Non-missing data points: \", n)\ndat2 = dropmissing(dat[:, [:Grade]], :Grade)\n\ngradeInQuestion = \"E\"\nindicatorVector = dat2.Grade .== gradeInQuestion\nnumSuccess = sum(indicatorVector)\nphat = numSuccess / n\nserr = sqrt(phat * (1 - phat) / n)\n\nalpha = 0.05\nconfidencePercent = 100 * (1 - alpha)\nzVal = quantile(Normal(), 1 - alpha / 2)\nconfInt = (phat - zVal * serr, phat + zVal * serr)\n\nprintln(\"\\nOut of $n non-missing observations, $numSuccess are at level $gradeInQuestion.\")\nprintln(\"Hence a point estimate for the proportion of grades at level $gradeInQuestion is $phat.\")\nprintln(\"A $confidencePercent% confidence interval for the proportion of level $gradeInQuestion is:\\n$confInt\")\n```\n\n#### Sample size planing\n\nDenote the confidence interval of the proportion as $\\hat{p} \\pm E$ where $E$ is the margin of error or half the width of the confidence interval, denoted by\n\n$$\nE = z_{1-\\frac{\\alpha}{2}}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n$$\n\nWe may often want to plan an experiment or a sampling scheme such that $E$ is not too wide.\n\nSay we want $E \\le \\epsilon$ with the confidence probability $1-\\alpha$, we have\n\n$$\nE = z_{1-\\frac{\\alpha}{2}}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\le \\epsilon\n$$\n\nGiven that $x(1-x)$ is maximized at $x=1/2$ with a maximal value of $1/4$. Hence,\n\n$$\nE \\le \\frac{z_{1-\\frac{\\alpha}{2}}}{2\\sqrt{n}} \\le \\epsilon\n$$\n\nFinally, we have\n\n$$\nn \\ge \\frac{z_{1-\\frac{\\alpha}{2}}^2}{4\\epsilon^2}\n$$\n\n#### Validity of the approximation\n\nIn many cases, this confidence interval approximation for the proportion works well, however for small sample sizes $n$ or values of $p$ near $0$ or $1$, this is often too crude of an approximation. A consequence is that one may obtain a confidence interval that isn't actually a $1-\\alpha$ confidence interval, but rather has a different coverage probability.\n\nOne common rule of thumb used to decide if the confidence interval for the proportion is valid is to require that both the product $np$ and the product $n(1-p)$ be at least $10$.\n\nHere, we expore some combinations of $n$ and $p$. For each combination, we use $N$ Monte Carlo expriments and calculate the following\n\n$$\n(1-\\alpha) - \\frac{1}{N} \\sum_{k=1}^{N}\\mathbf{1}\\left\\{p \\in \\left[\\hat{p} - z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}, \\hat{p} + z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\right]\\right\\}\n$$\n\nThis estimated difference of the actual converage probability of the confidence interval and the desired confidence level $1-\\alpha$ is a measure of the accuracy of the confidence level. We expect this difference to be almost $0$ if the approximation is good. Otherwise, a higher absolute difference is oberved.\n\n```{julia}\nusing Random, Distributions, CairoMakie\n\nN = 5e3\nalpha = 0.05\nconfLevel = 1 - alpha\nz = quantile(Normal(), 1 - alpha / 2)\n\nfunction randCI(n, p)\n    sample = rand(n) .< p\n    pHat = sum(sample) / n\n    serr = sqrt(pHat * (1 - pHat) / n)\n    (pHat - z * serr, pHat + z * serr)\nend\ncover(p, ci) = ci[1] <= p && p <= ci[2]\n\npGrid = 0.1:0.01:0.9\nnGrid = 5:1:50\nerrs = zeros(length(nGrid), length(pGrid))\n\nfor i in 1:length(nGrid)\n    for j in 1:length(pGrid)\n        Random.seed!(1234)\n        n, p = nGrid[i], pGrid[j]\n        coverageRatio = sum([cover(p, randCI(n, p)) for _ in 1:N]) / N\n        errs[i, j] = confLevel - coverageRatio\n    end\nend\n\nfig, ax1, hm1 = heatmap(pGrid, nGrid, errs'; colormap=to_colormap([\"white\", \"black\"]))\nax2, hm2 = heatmap(fig[2, 1], pGrid, nGrid, errs' .<= 0.04; colormap=to_colormap([\"black\", \"white\"]))\nColorbar(fig[1, 2], hm1)\nfig\n```\n\n### Confidence interval for the variance of a normal population\n\nFirst, a point estimator of the population variance is the sample variance\n\n$$\ns^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(X_i-\\overline{X})^2\n$$\n\nWe have known that $\\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi_{n-1}^2$. So denoting the $\\gamma$-quantile of this distribution via $\\chi_{\\gamma,n-1}^2$, we have\n\n$$\nP(\\chi_{\\frac{\\alpha}{2},n-1}^2 \\le \\frac{(n-1)s^2}{\\sigma^2} \\le \\chi_{1-\\frac{\\alpha}{2},n-1}^2) = 1-\\alpha\n$$\n\nThen we have\n\n$$\nP(\\frac{(n-1)s^2}{\\chi_{1-\\frac{\\alpha}{2},n-1}^2} \\le \\sigma^2 \\le \\frac{(n-1)s^2}{\\chi_{\\frac{\\alpha}{2},n-1}^2}) = 1-\\alpha\n$$\n\nNote: this equality holds only if the data is normally distributed.\n\n```{julia}\nusing Distributions\n\ndat = rand(Normal(0, 1), 1000)\nn, s, alpha = length(dat), std(dat), 0.05\n\nci = ((n - 1) * s^2 / quantile(Chisq(n - 1), 1 - alpha / 2),\n    (n - 1) * s^2 / quantile(Chisq(n - 1), alpha / 2))\n\nprintln(\"Point estimate for the variance: \", s^2)\nprintln(\"Confidence interval for the variance: \", ci)\n```\n\n#### Sensitivity of the normal assumption\n\nAs mentioned before, this confidence interval for the variance of a normal population only holds for normally distributed population and is more sensitive about the normal assumption than the other confidence intervals constructed before.\n\nWe'll find that the sample variance distributions of a normal distribution and a logistic distribution are quite different, though they have the same pupulation mean $\\mu$ and population variance $\\sigma^2$, and the PDF's curve shape of the logistic distribution is somewhat similar with the normal.\n\nThe logistic distribution is defined by the location and scale parameters $\\mu$ and $\\eta$, and the PDF is\n\n$$\nf(x) = \\frac{e^{-\\frac{x-\\mu}{\\eta}}}{\\eta(1+e^{-\\frac{x-\\mu}{\\eta}})^2}\n$$\n\nwith mean $\\mu$ and variance $\\sigma^2 = \\eta^2\\pi^2/3$.\n\n```{julia}\nusing Distributions, CairoMakie\n\nmu, sig = 2, 3\neta = sqrt(3) * sig / pi\n\nn, N = 15, 10^7\ndNormal = Normal(mu, sig)\ndLogistic = Logistic(mu, eta)\nxGrid = -8:0.1:12\n\nsNormal = [var(rand(dNormal, n)) for _ in 1:N]\nsLogistic = [var(rand(dLogistic, n)) for _ in 1:N]\n\nfig = Figure()\nax1 = Axis(fig[1, 1];\n    xlabel=\"x\",\n    ylabel=\"Density\")\nlines!(ax1, xGrid, pdf.(dNormal, xGrid); color=:blue, label=\"Normal\")\nlines!(ax1, xGrid, pdf.(dLogistic, xGrid); color=:red, label=\"Logistic\")\naxislegend(ax1)\nax2 = Axis(fig[2, 1],\n    xlabel=\"Sample Variance\",\n    ylabel=\"Density\",\n    limits=(0, 30, 0, 0.14))\nstephist!(ax2, sNormal; bins=200, color=:blue, normalization=:pdf, label=\"Normal\")\nstephist!(ax2, sLogistic; bins=200, color=:red, normalization=:pdf, label=\"Logistic\")\naxislegend(ax2)\nfig\n```\n\nIn addition, we can check the actual confidence interval coverage under different model assumptions:\n\n```{julia}\nusing Distributions, CairoMakie\n\nmu, sig = 2, 3\neta = sqrt(3) * sig / pi\nn, N = 15, 10^4\ndNormal = Normal(mu, sig)\ndLogistic = Logistic(mu, eta)\nalphaUsed = 0.001:0.001:0.1\n\nfunction alphaSimulator(dist, n, alpha)\n    popVar = var(dist)\n    coverageCount = 0\n    for _ in 1:N\n        sVar = var(rand(dist, n))\n        Lo = (n - 1) * sVar / quantile(Chisq(n - 1), 1 - alpha / 2)\n        Up = (n - 1) * sVar / quantile(Chisq(n - 1), alpha / 2)\n        coverageCount += Lo < popVar && popVar < Up\n    end\n    1 - coverageCount / N\nend\n\nfig, ax = ablines(0, 1; color=:green, label=\"y = x\")\nscatter!(ax, alphaUsed, alphaSimulator.(dNormal, n, alphaUsed); color=:blue, label=\"Normal\")\nscatter!(ax, alphaUsed, alphaSimulator.(dLogistic, n, alphaUsed); color=:red, label=\"Logistic\")\naxislegend(ax; position=:lt)\nfig\n```\n\n### Bootstrap confidence intervals\n\nBootstrap, also called empirical bootstrap, is a useful technique which relies on resampling from the observed data $x_1, ..., x_n$ **with replacement** in order to empirically construct **the distribution of the point estimator** for some unknown population parameters.\n\nOne way in which this resampling can be conducted is to apply the inverse probability transform on the empirical cumulative distribution function.\n\n::: {.callout-note}\n\n1. Obtain a sample: $\\mathbf{X} = (x_1, ..., x_n)$.\n\n2. Obtain the empirical cumulative distribution function $F_X(\\mathbf{X})$ based on $\\mathbf{X}$.\n\n3. Use the inverse probability transformation: $\\mathbf{U} = F_X(\\mathbf{X}) \\Longrightarrow \\mathbf{X} = F_X^{-1}(\\mathbf{U})$.\n\n4. Resample $\\mathbf{X}$ via resampling $\\mathbf{U} \\sim U(0, 1)$ with replacement.\n\nIt seems that we sample a large number of \"new\" samples from the population, and then we can get a large number of point estimators, which then are used to construct the empirical distribution of the point estimator. Finally, to get the $1-\\alpha$ confidence interval, we can just get the quantiles of $\\frac{\\alpha}{2}$ and $1-\\frac{\\alpha}{2}$.\n\nBootstrap requires that the number of observations is not too small.\n\n:::\n\nHowever, from an implementation perspective, a simpler alternative is to consider the data points $x_1, ..., x_n$, and then randomly sample $n$ discrete uniform indexes, $j_1, ..., j_n$ each in the range $\\{1, ..., n\\}$. The resampled data denoted by $x^* = (x_1^*, ..., x_n^*)$ is then\n\n$$\nx^* = (x_{j_1}, ..., x_{j_n})\n$$\n\nThat is, each point in the resampeld data is a random observation from the original data, where we allow to **sample with replacement**.\n\nIn Julia, $x^* = \\text{rand}(\\mathbf{X}, n)$, where we use the `rand()` method to uniformally sample $n$ random copies of elements in $\\mathbf{X}$ with replacement.\n\nThe idea of empirical bootstrap is now to repeat the resampling a large number of times, say $N$, and for each resampled data vector, $x^*(1), ..., x^*(N)$ to compute the parameter estimate. If the parameter estimate is denoted by the function $h: R^n \\mapsto R$, then we end up with values\n\n$$\n\\begin{align}\nh^*(1) &= h(x_1^*(1), ..., x_n^*(1)) \\\\\nh^*(2) &= h(x_1^*(2), ..., x_n^*(2)) \\\\\n&\\vdots \\\\\nh^*(N) &= h(x_1^*(N), ..., x_n^*(N))\n\\end{align}\n$$\n\nFinally, a bootstrap confidence interval is determined by computing the respective lower and upper $(\\frac{\\alpha}{2}, 1-\\frac{\\alpha}{2})$ quantiles of the sequence $h^*(1), ..., h^*(N)$.\n\n```{julia}\nusing Random, Distributions, CairoMakie\n\nRandom.seed!(0)\n\nsampleData = [\n    53.35674558144255,\n    53.45887516134873,\n    52.282838627926125,\n    52.98746570643515,\n    51.993167774733486,\n    53.373333606198,\n    55.75410538860477,\n    50.279496381439365,\n    53.6359586914001,\n    53.517705831707495,\n    53.70044994508253,\n    54.15592592604583,\n    53.55054914606446,\n    52.37319589109419,\n    53.4900750059897,\n    52.939458524079605,\n    52.16761562743534,\n    50.87140009591033,\n    53.144919157924924,\n    52.09084035473537,\n]\nn, N = length(sampleData), 10^6\nalpha = 0.1\n\n# sampling with replacement\nbootstrapSampleMeans = [mean(rand(sampleData, n)) for _ in 1:N]\nLmean = quantile(bootstrapSampleMeans, alpha / 2)\nUmean = quantile(bootstrapSampleMeans, 1 - alpha / 2)\n\nprintln(\"Bootstrap confidence interval for the mean: \", (Lmean, Umean))\n\nfig, ax = stephist(bootstrapSampleMeans; bins=1000, color=:blue, normalization=:pdf, label=\"Sample means\")\nvlines!(ax, [Lmean, Umean]; color=:red, label=\"90% CI\")\naxislegend(ax)\nfig\n```\n\nSimply, we can carry out a computational experiment to show that if the number of sample observations is not very large, then the coverage probability of bootstrapped confidence interval is only approximately $1-\\alpha$, but not exactly. However, as the sample size $n$ grows, the coverage probability converges to the desired $1-\\alpha$.\n\n```{julia}\nusing Random, Distributions\n\nRandom.seed!(0)\n\nlambda = 0.1\ndist = Exponential(1 / lambda)\nactualMedian = median(dist)\n\nM = 10^3\nN = 10^4\nnRange = 2:2:20\nalpha = 0.05\n\nfor n in nRange\n    coverageCount = 0\n    for _ in 1:M\n        sampleData = rand(dist, n)\n        bootstrapSampleMeans = [median(rand(sampleData, n)) for _ in 1:N]\n        Lo = quantile(bootstrapSampleMeans, alpha / 2)\n        Up = quantile(bootstrapSampleMeans, 1 - alpha / 2)\n        coverageCount += Lo < actualMedian && actualMedian < Up\n    end\n    println(\"n = \", n, \"\\t coverage = \", coverageCount / M)\nend\n```\n\n### Prediction intervals\n\nA prediction interval tells us a predicted range that **a single next observation** of data is expected to fall within with some level of confidence. This differs from a confidence interval which indicates how confident we are of a particular **parameter** that we are trying to estimate.\n\nIn brief, a prediction interval is constructed based on **the distribution of a population** itself from which we sample observations, while a confidence interval is constructed based on **the distribution of a particular parameter** which we are trying to estimate (e.g. $\\overline{X} \\sim N(\\mu, \\frac{\\sigma^2}{n})$).\n\nSuppose we have a sequence of data points (observations) $x_1, x_2, x_3, ...$, which come from a normal distribution and are assumed i.i.d. Further assume that we observed the first $n$ data points $x_1, ..., x_n$, but have not yet observed $X_{n+1}$. Note that we use lower case $x$ for values observed and upper case $X$ for yet unobserved random variables.\n\nIn this case, a $1-\\alpha$ prediction interval for the single future observation $X_{n+1}$ is given by\n\n$$\n\\bar{x} - t_{1-\\frac{\\alpha}{2},n-1}s\\sqrt{1+\\frac{1}{n}} \\le X_{n+1} \\le \\bar{x} + t_{1-\\frac{\\alpha}{2},n-1}s\\sqrt{1+\\frac{1}{n}}\n$$\n\nwhere $\\bar{x}$ and $s$ are respectively the sample mean and sample standard deviation computed from $x_1, ..., x_n$.\n\nNote that as the number of observations $n$ grows, the bounds of the prediction interval decreases towards\n\n$$\n\\bar{x} - z_{1-\\frac{\\alpha}{2}}s \\le X_{n+1} \\le \\bar{x} - z_{1-\\frac{\\alpha}{2}}s\n$$\n\nand finally has an expected width close to $2z_{1-\\frac{\\alpha}{2}}\\sigma$.\n\n::: {.callout-note}\n\nSuppose $X$ is a normally distributed variable (i.e. $X \\sim N(\\mu, \\sigma^2)$), and then we have $U = \\frac{X-\\mu}{\\sigma} \\sim N(0, 1)$.\n\nFor $U$, we have a $1-\\alpha$ prediction interval $-z_{1-\\frac{\\alpha}{2}} \\le U \\le z_{1-\\frac{\\alpha}{2}}$, and then we have $\\mu - z_{1-\\frac{\\alpha}{2}}\\sigma \\le X \\le \\mu + z_{1-\\frac{\\alpha}{2}}\\sigma$, which is a $1-\\alpha$ prediction interval of $X$ in the case of population mean and population variance known.\n\nSimply, a prediction interval is such an interval constructed based on the distribution of observations and needs to cover a $1-\\alpha$ proportion of this distribution.\n\n:::\n\n```{julia}\nusing Random, Statistics, Distributions, CairoMakie\n\nRandom.seed!(0)\n\nmu, sig = 50, 5\ndist = Normal(mu, sig)\nalpha = 0.01\nnMax = 40\n\nobservations = rand(dist, 1)\npiLarray, piUarray = [], []\n\nfor _ in 2:nMax\n    xNew = rand(dist)\n    push!(observations, xNew)\n\n    xBar, sd = mean(observations), std(observations)\n    n = length(observations)\n    tVal = quantile(TDist(n - 1), 1 - alpha / 2)\n    delta = tVal * sd * sqrt(1 + 1 / n)\n    piL, piU = xBar - delta, xBar + delta\n\n    push!(piLarray, piL)\n    push!(piUarray, piU)\nend\n\nfig, ax = scatter(1:nMax, observations; color=:blue, label=\"Observations\")\nscatterlines!(2:nMax, piUarray; marker=:cross, color=:red, markercolor=:black, label=\"Prediction Interval\")\n\nscatterlines!(2:nMax, piLarray; marker=:cross, color=:red, markercolor=:black)\naxislegend(ax)\nax.xlabel = \"Number of observations\"\nax.ylabel = \"Value\"\nfig\n```\n\n### Credible intervals\n\nThe concept of credible intervals comes from the field of Bayesian statistics.\n\nIn general, we often need to find an interval $[l, u]$ such that given some probability density $f(x)$, the interval satisfies\n\n$$\n\\int_l^u f(x)dx = 1-\\alpha\n$$\n\nThis is needed for confidence intervals, prediction intervals, and credible intervals.\n\nHowever, as long as $\\alpha < 1$, there is more than one interval $[l, u]$ that satisfies the above formula.\n\nIn certain cases, there is a \"natural\" interval. For example, for a symmetric distribution, using equal quantiles is natural (i.e. $l = z_{\\frac{\\alpha}{2}}$ and $u = z_{1-\\frac{\\alpha}{2}}$). In such cases, the mean is also the median and further if the density is unimodal (has a single maximum) the mean and median are also the mode.\n\nHowever, consider asymmetric distribution, there isn't an immediate \"natrual\" choice for $l$ and $u$. There are three types of intervals we often use:\n\n1. **Classic interval:** this type of interval has the mode of the density (assuming the density is unimodal) at its center between $l$ and $u$. An alternative is to use mean or median at the center. That is, assuming the centrality measure (mode, mean, or median) is $m$, we have $[l, u] = [m-E, m+E]$. One way to define $E$ is via\n\n$$\nE = \\max\\{\\epsilon \\ge 0: \\int_{m-\\epsilon}^{m+\\epsilon} f(x)dx \\le 1-\\alpha\\}\n$$\n\n2. **Equal tail interval:** this type of interval simply sets $l$ and $u$ as the $\\frac{\\alpha}{2}$ and $1-\\frac{\\alpha}{2}$ quantiles, respectively. Namely,\n\n$$\n\\frac{\\alpha}{2} = \\int_{-\\infty}^l f(x)dx\n$$\n\nand\n\n$$\n1 - \\frac{\\alpha}{2} = \\int_{u}^{\\infty} f(x)dx\n$$\n\n3. **Highest density interval:** this type of interval seeks to cover the part of the support that is most probable. A consequence is that if the density is unimodal then this highest density interval is also the narrowest possible confidence interval. We can crudely does so by starting with a high-density value and decreasing it gradually while seeking for the associated interval $[l, u]$. An alternative would be to gradually increment $l$ each time finding a corresponding $u$ that satisfies the above formula and within this search to choose the interval that minimizes the width $u-l$.\n\nFor a symmetric and unimodal distribution, all three of these confidence intervals agree.\n\nWe now explain the credible intervals. In the Bayesian setting, we treat the unknown parameter $\\theta$ as a random variable, this is totally different from the classical case where we treat the unknown parameter $\\theta$ as a fixed value. The process of inference is based on observing data, $x = (x_1, ..., x_n)$ which has a distribution depending on the unknown parameter $\\theta$, and fusing it with the prior distribution $f(\\theta)$ that the unknown parameter $\\theta$ is distributed as before observing data $x = (x_1, ..., x_n)$ to obtain the posterior distribution $f(\\theta|x)$ which the unknown parameter $\\theta$ is distributed as after observing data $x = (x_1, ..., x_n$.\n\nHere too, as in the frequentist case, we may wish to describe an interval where it is likely that our parameter lies. Then for a fixed confidence interval $1-\\alpha$, seek $[l, u]$, such that\n\n$$\n\\int_l^u f(\\theta|x)d\\theta = 1-\\alpha\n$$\n\nThere is a basic difference between confidence intervals in the frequentist setting and credible intervals in the Bayesian setting:\n\nFor a given $1-\\alpha$ interval $[L, U]$,\n\n1. In the frequentist setting, the unknown parameter $\\theta$ is a fixed value while $L$ and $U$ are random variables depending on observed data $X$ which is a random variable. So this is why we often say the confidence interval $[L, U]$ will cover the unknown but fixed parameter $\\theta$ under the confidence level $1-\\alpha$.\n\n2. In the Bayesian setting, the unknown parameter $\\theta$ is a random variable while $L$ and $U$ are deterministic values. So we will see the unknown parameter $\\theta$ will fall within the credible interval $[L, U]$ with the probability $1-\\alpha$.\n\n```{julia}\nusing Distributions, CairoMakie\n\nalpha, beta = 8, 2\ndat = [2, 1, 0, 0, 1, 0, 2, 2, 5, 2, 4, 0, 3, 2, 5, 0]\n\nnewAlpha, newBeta = alpha + sum(dat), beta + length(dat)\npost = Gamma(newAlpha, newBeta)\n\nxGrid = quantile(post, 0.01):0.001:quantile(post, 0.99)\nsignificance = 0.9\nhalfAlpha = (1 - significance) / 2\n\ncoverage(l, u) = cdf(post, u) - cdf(post, l)\n\nfunction classicCI(dist)\n    l, u = mode(dist), mode(dist)\n    while coverage(l, u) < significance\n        l -= 0.00001\n        u += 0.00001\n    end\n    (l, u)\nend\nequalTailCI(dist) = (quantile(dist, halfAlpha), quantile(dist, 1 - halfAlpha))\nfunction highestDensityCI(dist)\n    height = 0.999 * maximum(pdf.(dist, xGrid))\n    l, u = mode(dist), mode(dist)\n    while coverage(l, u) < significance\n        range = filter(theta -> pdf(dist, theta) > height, xGrid)\n        l, u = minimum(range), maximum(range)\n        height -= 0.00001\n    end\n    (l, u)\nend\n\nl1, u1 = classicCI(post)\nl2, u2 = equalTailCI(post)\nl3, u3 = highestDensityCI(post)\n\nprintln(\"Classical: \", (l1, u1), \"\\tWidth: \", u1 - l1, \"\\tCoverage: \", coverage(l1, u1))\nprintln(\"Equal tails: \", (l2, u2), \"\\tWidth: \", u2 - l2, \"\\tCoverage: \", coverage(l2, u2))\nprintln(\"Highest density: \", (l3, u3), \"\\tWidth: \", u3 - l3, \"\\tCoverage: \", coverage(l3, u3))\n\nfig, ax = lines(xGrid, pdf.(post, xGrid); color=:black, label=\"Gamma Posterior Distribution\")\nrangebars!(ax, [-0.00025], [l1], [u1]; direction=:x, whiskerwidth=10, color=:blue, label=\"Classic CI\")\nrangebars!(ax, [-0.0005], [l2], [u2]; direction=:x, whiskerwidth=10, color=:red, label=\"Equal Tail CI\")\nrangebars!(ax, [-0.00075], [l3], [u3]; direction=:x, whiskerwidth=10, color=:green, label=\"Highest Density CI\")\naxislegend(ax)\nfig\n```\n\n## Hypothesis testing\n\nTo perform a hypothesis testing, first, we need to partition the parameter space $\\Theta$ as two hypotheses:\n\n* Null hypothesis: $H_0: \\theta \\in \\Theta_0$\n\n* Alternative hypothesis: $H_1: \\theta \\in \\Theta_1$\n\nAnd then, we need to determine the test statistic used to perform the hypothesis testing. Once this is done, we can calculate the test statistic and then get the rejection region (e.g. $(-\\infty, ICDF(\\frac{\\alpha}{2}))$ and $(ICDF(1-\\frac{\\alpha}{2}), +\\infty)$ for a two-sided hypothesis test) or the p-value ($CDF(\\text{the value of test statistic})$) under certain confidence level $1-\\alpha$ under $H_0$ (the distribution of the test statistic is often known under $H_0$ but it's usually unknown under $H_1$).\n\nFinally, make a statement (rejecting or not rejecting $H_0$) under the assumption of null hypothesis based on some confidence level.\n\nSo there are several key concepts concerning a hypothesis testing: two alternative hypotheses, confidence level, test statistic, the distribution of the test statistic under the null hypothesis, rejection region or $p$-value.\n\n### Single sample hypothesis tests for the mean\n\nAssume that the observations $X_1, ..., X_n$ are **normally** distributed and we want to know whether this sample is from a population with $\\mu = \\mu_0$ or not ($\\mu \\ne \\mu_0$).\n\nIn this case, we set up the hypothesis as two-sided (which means that $\\mu < \\mu_0$ and $\\mu > \\mu_0$ are both plausible) and the confidence level $1-\\alpha$.\n\n#### Population variance known (Z-Test)\n\nAssume that $\\sigma$ is known. Under $H_0$, $\\overline{X}$ follows a normal distribution with mean $\\mu_0$ and variance $\\frac{\\sigma^2}{n}$. Hence, it holds that under $H_0$ the Z-statistic\n\n$$\nZ = \\frac{\\overline{X}-\\mu_0}{\\sigma/n}\n$$\n\nfollows a standard normal distribution ($Z \\sim N(0, 1)$).\n\nIn this case, under the null hypothesis, $Z$ is a standard normal random variable, and hence to carry out a hypothesis test we observe its position relative to a standard normal distribution. Specifically, we check if it lies within the rejection region or not, and if it does, we reject the null hypothesis, otherwise we don't. In addition, we can also calculate the $p$-value ($p = 2P(Z > |z|)$). If $p$-value is less than some pre-designated significance level $\\alpha$, then we can reject $H_0$ or we don't.\n\n```{julia}\nusing Random, Distributions, HypothesisTests\n\nRandom.seed!(0)\n\nmu0 = 25\nmu1, sigma = 27, 2\nn = 100\nd = rand(Normal(mu1, sigma), n)\nxBar = mean(d)\n\ntestStatistic = (xBar - mu0) / (sigma / sqrt(n))\npVal = 2 * ccdf(Normal(), testStatistic)\nprintln(\"Manual hypothesis testing: \\nz-statistic: \", testStatistic, \"\\np-value: \", pVal, \"\\n\")\n\nOneSampleZTest(xBar, sigma, n, mu0)\n```\n\n#### Population variance unknown (T-Test)\n\nWhen the population variance is unknown, then $\\overline{X}$ does not subject to $N(\\mu, \\frac{s}{\\sqrt{n}})$. But we know that\n\n$$\nT = \\frac{\\overline{X} - \\mu_0}{S/\\sqrt{n}} \\sim T(n-1)\n$$\n\nunder the null hypothesis.\n\nThe observed test statistic from the data is then\n\n$$\nt = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}\n$$\n\nand the corresponding $p$-value for a two-sided test is\n\n$$\np = 2P(T_{n-1} > |t|)\n$$\n\nwhere $T_{n-1}$ is a random variable distributed according to a T-distribution with $n-1$ degrees of freedom.\n\n```{julia}\nusing Random, Distributions, HypothesisTests\n\nRandom.seed!(0)\n\nmu0, mu1, sigma = 51, 53, 2\ndist = Normal(mu1, sigma)\nd = rand(dist, 20)\n\nxBar, n, s = mean(d), length(d), std(d)\ntStatistic = (xBar - mu0) / (s / sqrt(n))\npVal = 2 * ccdf(TDist(n - 1), abs(tStatistic))\n\nprintln(\"Manually calculated t-statistic: \", tStatistic)\nprintln(\"Manually calculated p-value: \", pVal)\n\nOneSampleTTest(d, mu0)\n```\n\n#### Non-parametric sign test\n\nThe validity of Z-Test or T-Test relies heavily on the assumption that the sample $X_1, ..., X_n$ is comprised of independent normal random variables with variance known or unknown. This is because only under this assunmption does $\\overline{X} \\sim N(\\mu, \\frac{\\sigma^2}{n})$ or $T \\sim T(n-1)$.\n\nFor the non-parametric sign test, ***non-parametric* implies that the distribution of the test statistic does not depend on any particular distributional assumption for the population**.\n\nFor the sign test, we begin by denoting the random variables\n\n$$\nX^+ = \\sum_{i=1}^{n}\\mathbf{1}\\{X_i > \\mu_0\\} \\quad\\text{and}\\quad X^- = \\sum_{i=1}^{n}\\mathbf{1}\\{X_i < \\mu_0\\} = n - X^+\n$$\n\nwhere $\\mathbf{1}$ is the indicator function. The variable $X^+$ is a count of the number of observations that exceed $\\mu_0$, and similarly $X^-$ is a count of the number of observations that are below $\\mu_0$.\n\nObserve that under $H_0: \\mu = \\mu_0$, it holds that $P(X_i > \\mu_0) = P(X_i < \\mu_0) = 1/2$. Note that here we are actually taking $\\mu_0$ as the median of the distribution and assuming that $P(X_i = \\mu_0) = 0$ as is the case for a continuous distribution.\n\nHence, under $H_0$, the random variables $X^+$ and $X^-$ both follow a binomial $(n, 1/2)$ distribution. Given the symmetry of this binomial distribution we define the test statistic to be\n\n$$\nU = max\\{X^+, X^-\\}\n$$\n\nHence, with observed data, and an observed test statistic $u$, the $p$-value can be calculated via\n\n$$\np = 2P(B > u)\n$$\n\nwhere $B \\sim B(n, 1/2)$.\n\n```{julia}\nusing Random, Distributions\n\nRandom.seed!(0)\n\nmu0, mu1, sigma = 51, 53, 2\ndist = Normal(mu1, sigma)\nd = rand(dist, 20)\nn = length(d)\n\nxPos = sum(d .> mu0)\ntestStat = max(xPos, n - xPos)\n\nbinom = Binomial(n, 0.5)\npVal = 2 * ccdf(binom, testStat)\n\nprintln(\"mu0: \", mu0, \"\\nmu1: \", mu1)\nprintln(\"Binomial mean: \", mean(binom), \"\\nTest statistic: \", testStat)\nprintln(\"p-value: \", pVal)\n```\n\n#### Sign test vs. T-test\n\nWhen the normality assumption holds, the T-test is often more powerful than the sign test. That is, for a fixed $\\alpha$, the probability of detecting $H_1$ is higher for the T-test than for the sign test if $H_1 \\ne H_0$. This makes it a more effective test to use, if the data can be assumed normally distributed.\n\n```{julia}\nusing Random, Distributions, CairoMakie\n\nRandom.seed!(0)\n\nActualMuRange = 51:0.02:55\nsigma = 1.2\nmu0 = 53\nn, N = 50, 10^4\npowerT, powerU = [], []\n\nfor ActualMu in ActualMuRange\n    dist = Normal(ActualMu, sigma)\n    rejectT, rejectU = 0, 0\n\n    for _ in 1:N\n        d = rand(dist, n)\n        xBar, stdDev = mean(d), std(d)\n\n        tStatistics = (xBar - mu0) / (stdDev / sqrt(n))\n        pValT = 2 * ccdf(TDist(n - 1), abs(tStatistics))\n\n        xPos = sum(d .> mu0)\n        uStat = max(xPos, n - xPos)\n        pValSign = 2 * ccdf(Binomial(n, 0.5), uStat)\n\n        rejectT += pValT < 0.05\n        rejectU += pValSign < 0.05\n    end\n\n    push!(powerT, rejectT / N)\n    push!(powerU, rejectU / N)\nend\n\nfig, ax = lines(ActualMuRange, powerT; color=:blue, label=\"T test\")\nlines!(ax, ActualMuRange, powerU; color=:red, label=\"Sign test\")\nhlines!(ax, [0.05]; color=:green, label=\"Alpha\")\nvlines!(ax, [mu0 - sigma, mu0 + sigma]; color=:black, label=\"mu0 ± sigma\")\naxislegend(ax; position=:rb)\nax.xlabel = \"Different values of muActual\"\nax.ylabel = \"Proportion of times H0 rejected (power)\"\nfig\n```\n\nFrom the result, **under the normality assumption**, we obsered that\n\n* When $H_1 = H_0$, then the power is $\\alpha$ (i.e. the probability of rejecting $H_0$).\n\n* T-test is more powerful than sign test when $H_1 \\ne H_0$.\n\n* When $H_1 \\to H_0$, sign test will make higher $\\alpha$ error than T-test.\n\n* When $H_1$ is away from $H_0$ by one $\\sigma$ or more, then the powers of T-test and sign test are really similar.\n\n### Two sample hypothesis tests for comparing means\n\nWe now present some common hypothesis tests for the inference on **the difference in means of two populations**.\n\nCommonly, we wish to investigate if the population difference, $\\Delta_0$, takes on a specific value.\n\nHence, we may wish to set up a **two-sided** hypothesis test as\n\n$$\nH_0: \\mu_1 - \\mu_2 = \\Delta_0 \\quad\\text{and}\\quad H_1: \\mu_1 - \\mu_2 \\ne \\Delta_0\n$$\n\nor one could formulate a **one-sided** hypothesis test, such as\n\n$$\nH_0: \\mu_1 - \\mu_2 \\le \\Delta_0 \\quad\\text{and}\\quad H_1: \\mu_1 - \\mu_2 > \\Delta_0\n$$\n\nor the reverse if desired.\n\nIt is common to consider $\\Delta_0 = 0$, in which case the hypothesis test can be stated as $H_0: \\mu_1 = \\mu_2$, and $H_1: \\mu_1 \\ne \\mu_2$.\n\nFor the tests introduced below, we assume that the observations $X_1^{(1)}, ..., X_n^{(1)}$ from population $1$ and $X_1^{(2)}, ..., X_n^{(2)}$ from population $2$ are all **normally distributed**, where $X_i^{(j)}$ has mean $\\mu_j$ and variance $\\sigma_j^2$.\n\nThe testing methodology then differs based on the following three cases:\n\n1. The population variances $\\sigma_1$ and $\\sigma_2$ are **known**.\n\n2. The population variances $\\sigma_1$ and $\\sigma_2$ are **unknown but assumed equal**.\n\n3. The population variances $\\sigma_1$ and $\\sigma_2$ are **unknown and not assumed equal**.\n\nIn each of these cases, the test statistic is given by\n\n$$\n\\frac{(\\overline{X}_1 - \\overline{X}_2) - \\Delta_0}{S_{err}}\n$$\n\nwhere $\\overline{X}_j$ is the sample mean of $X_1^{(j)}, ..., x_n^{(j)}$, and the standard error $S_{err}$ varies according to the case ($1-3$).\n\n#### Population variances known\n\nWhen the population variances $\\sigma_1^2$ and $\\sigma_2^2$ are known, we have $\\overline{X}_1 \\sim N(\\mu_1, \\frac{\\sigma_1^2}{n_1})$ and $\\overline{X}_2 \\sim N(\\mu_2, \\frac{\\sigma_2^2}{n_2})$, and then $\\overline{X}_1 - \\overline{X}_2 \\sim N(\\mu_1 - \\mu_2, \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2})$. Hence we set\n\n$$\nS_{err} = \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\n$$\n\nIn this case, the test statistic follows a standard normal distribution under $H_0$, so we have\n\n$$\nz = \\frac{(\\bar{x}_1 - \\bar{x}_2) - \\Delta_0}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\n$$\n\n```{julia}\nusing Random, Distributions\n\nRandom.seed!(0)\n\nmu1, sigma1 = 10, 2\nmu2, sigma2 = 12, 1.5\n\nd1 = rand(Normal(mu1, sigma1), 20)\nd2 = rand(Normal(mu2, sigma2), 30)\n\nxBar1, n1 = mean(d1), length(d1)\nxBar2, n2 = mean(d2), length(d2)\n\n# to test whether μ1 = μ2 (two-sided)\ndelta0 = 0\n\ntestStat = (xBar1 - xBar2 - delta0) / sqrt(sigma1^2 / n1 + sigma2^2 / n2)\npVal = 2 * ccdf(Normal(), abs(testStat))\n\nprintln(\"μ1 = \", mu1, \"\\nμ2 = \", mu2)\nprintln(\"Manually calculated test statistic: \", testStat)\nprintln(\"Manually calculated p-value: \", pVal)\n```\n\n#### Population variances unknown and assumed equal\n\nIn case the population variances are **unknown and assumed equal** ($\\sigma^2 := \\sigma_1^2 = \\sigma_2^2$), the pooled sample variance (weighted arithmetic mean), $S_p^2$ is used to estimate $\\sigma^2$ based on both samples. It is given by\n\n$$\nS_p^2 = \\frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1+n_2-2}\n$$\n\nwhere $S_j^2$ is the sample variance of sample $j$. It can be shown that under $H_0$, if we set\n\n$$\nS_{err} = \\sqrt{\\frac{S_p^2}{n_1} + \\frac{S_p^2}{n_2}} = S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n$$\n\nthe test statistic is distributed as a T-distribution with $n_1+n_2-2$ degrees of freedom.\n\nFor this case, the observed test statistic is\n\n$$\nt = \\frac{(\\bar{x}_1 - \\bar{x}_2) - \\Delta_0}{S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n$$\n\n```{julia}\nusing Random, Distributions, HypothesisTests\n\nRandom.seed!(0)\n\nmu1, sigma1 = 10, 2\nmu2, sigma2 = 12, 2\n\nd1 = rand(Normal(mu1, sigma1), 20)\nd2 = rand(Normal(mu2, sigma2), 30)\n\nxBar1, s1, n1 = mean(d1), std(d1), length(d1)\nxBar2, s2, n2 = mean(d2), std(d2), length(d2)\n\n# to test whether μ1 = μ2 (two-sided)\ndelta0 = 0\n\nsP = sqrt(((n1 - 1) * s1^2 + (n2 - 1) * s2^2) / (n1 + n2 - 2))\ntestStat = (xBar1 - xBar2 - delta0) / (sP * sqrt(1 / n1 + 1 / n2))\npVal = 2 * ccdf(TDist(n1 + n2 - 2), abs(testStat))\n\nprintln(\"Manually calculated test statistic: \", testStat)\nprintln(\"Manually calculated p-value: \", pVal)\n\nEqualVarianceTTest(d1, d2, delta0)\n```\n\n#### Population variances unknown and not assumed equal\n\nWhere the population variances are unknown and not assumed equal ($\\sigma_1^2 \\ne \\sigma_2^2$), we set\n\n$$\nS_{err} = \\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}\n$$\n\nThen the observed test statistic is given by\n\n$$\nt = \\frac{(\\bar{x}_1 - \\bar{x}_2) - \\Delta_0}{\\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}}\n$$\n\nThe distribution of the test statistic does not follow an exact T-distribution with $n_1+n_2-2$ degrees of freedom. Instead, we use the *Satterthwaite approxmation*, and determine the degrees of freedom via\n\n$$\nv = \\frac{(s_1^2n_1 + s_2^2n_2)^2}{\\frac{(s_1^2/n_1)^2}{n_1-1} + \\frac{(s_2^2/n_2)^2}{n2-1}}\n$$\n\n```{julia}\nusing Random, Distributions, HypothesisTests\n\nRandom.seed!(0)\n\nmu1, sigma1 = 10, 2\nmu2, sigma2 = 12, 1.5\n\nd1 = rand(Normal(mu1, sigma1), 20)\nd2 = rand(Normal(mu2, sigma2), 30)\n\nxBar1, s1, n1 = mean(d1), std(d1), length(d1)\nxBar2, s2, n2 = mean(d2), std(d2), length(d2)\n\n# to test whether μ1 = μ2 (two-sided)\ndelta0 = 0\n\nv = (s1^2 / n1 + s2^2 / n2)^2 / ((s1^2 / n1)^2 / (n1 - 1) + (s2^2 / n2)^2 / (n2 - 1))\ntestStat = (xBar1 - xBar2 - delta0) / sqrt(s1^2 / n1 + s2^2 / n2)\npVal = 2 * ccdf(TDist(v), abs(testStat))\n\nprintln(\"Manually calculated degrees of freedom, v: \", v)\nprintln(\"Manually calculated test statistic: \", testStat)\nprintln(\"Manually calculated p-value: \", pVal)\nUnequalVarianceTTest(d1, d2, delta0)\n```\n\n### Analysis of variance (ANOVA or F-test)\n\nAs mentioned above, Z-test or T-test can handle the problems of comparing means of two populations. However, what if there are more than two populations that need to be compared? You may say that we can compare each pair of them, but in fact, this will reduce the hypothesis test power.\n\nWhen we have more than two populatons to be compared, we call each population a treatment or a group. It is of interest to see if vairous \"treatments\" have an effect on some mean value or not.\n\nMore formally, assume that there is some *overall mean* $\\mu$ and there are $L \\ge 2$ treatments, where each treatment may potentially alter the mean by $\\tau_i$. In this case, the mean of the population under treatment $i$ can be represented by $\\mu_i = \\mu + \\tau_i$, with $\\mu$ an overall mean, and\n\n$$\n\\sum_{i=1}^{L} \\tau_i = 0\n$$\n\nThis condition on the parameters $\\tau_1, ..., \\tau_L$ ensures that given $\\mu_1, ..., \\mu_L$, the overall mean $\\mu$ and $\\tau_1, ..., \\tau_L$ are well defined.\n\nGiven $\\mu_1, ..., \\mu_L$, we have\n\n$$\n\\mu = \\frac{1}{L} \\sum_{i=1}^{L} \\mu_i \\quad\\text{and}\\quad \\tau_i = \\mu_i - \\mu\n$$\n\n**The question is then: *Do the treatments have any effect or not?***\n\nSuch a question is presented via the hypothesis formulation:\n\n$$\nH_0: \\tau_1 = \\tau_2 = \\cdots = \\tau_L = 0 \\quad\\text{vs.}\\quad H_1: \\exists i\\ |\\ \\tau_i \\ne 0\n$$\n\nNote that $H_0$ is equivalent to the statement that $\\mu_1 = \\mu_2 = \\cdots = \\mu_L$, indicating that the treatments do not have an effect. Furthermore, $H_1$ states that there exists an $i$ with $\\tau_i \\ne 0$ is equivalent to the case where there exist at least two treatments, $i$ and $j$ such that $\\mu_i \\ne \\mu_j$.\n\nAssume that we collect observations as follows:\n\n$$\n\\begin{align}\n\\text{Treatment 1: } &x_{11}, x_{12}, ..., x_{1n_1} \\\\\n\\text{Treatment 2: } &x_{21}, x_{22}, ..., x_{2n_2} \\\\\n&\\vdots \\\\\n\\text{Treatment L: } &x_{L1}, x_{L2}, ..., x_{Ln_L}\n\\end{align}\n$$\n\nwhere $n_1, n_2, ..., n_L$ are the sample sizes for each treatment.\n\nThen denote the total number of observations via\n\n$$\nm = \\sum_{i=1}^{L} n_i\n$$\n\nWe also consider the sample means for each treatment\n\n$$\n\\bar{x}_i = \\frac{1}{n_i} \\sum_{i=1}^{n_i} x_{ij}\n$$\n\nand the overall sample mean\n\n$$\n\\bar{x} = \\frac{1}{m} \\sum_{i=1}^{L} \\sum_{i=1}^{n_i} x_{ij} = \\sum_{i=1}^{L} \\frac{n_i}{m} x_{ij}\n$$\n\nIn ANOVA, the statistical model assumes that the observations of each treatment come from an underlying model of the following form:\n\n$$\nX_i = \\mu_i + \\epsilon = \\mu + \\tau_i + \\epsilon \\quad\\text{where}\\quad \\epsilon \\sim N(0, \\sigma^2)\n$$\n\nwhere $X_i$ is the model for the $i$th treatment, and $\\epsilon$ is some noise term with common unknown variance across all treatment groups, independent across measurements.\n\n#### Decomposing sum of squares\n\nA key idea of ANOVA is the decomposition of the total variability into two components: the variablity between the treatments, and the variability within the treatments.\n\nThe total sum of squares ($SS_\\text{Total}$) is a measure of the total variability of all observations, and is calculated as follows:\n\n$$\nSS_\\text{Total} = \\sum_{i=1}^{L} \\sum_{j=1}^{n_i} (x_{ij} - \\bar{x})^2\n$$\n\nwhere $\\bar{x}$ is the overall mean.\n\nNow we decompose $SS_\\text{Total}$ into two parts:\n\n$$\n\\begin{align}\n\\sum_{i=1}^{L} \\sum_{j=1}^{n_i} (x_{ij} - \\bar{x})^2 &= \\sum_{i=1}^{L} \\sum_{j=1}^{n_i} (x_{ij} - \\bar{x}_i + \\bar{x}_i - \\bar{x})^2 \\\\\n&= \\sum_{i=1}^{L} \\sum_{j=1}^{n_i} \\left((x_{ij} - \\bar{x}_i)^2 + 2(x_{ij} - \\bar{x}_i)(\\bar{x}_i - \\bar{x}) + (\\bar{x}_i - \\bar{x})^2\\right) \\\\\n&= \\underbrace{\\sum_{i=1}^{L} \\sum_{j=1}^{n_i} (x_{ij} - \\bar{x}_i)^2}_{SS_\\text{Error}} + \\underbrace{\\sum_{i=1}^{L} n_i (\\bar{x}_i - \\bar{x})^2}_{SS_\\text{Treatment}}\n\\end{align}\n$$\n\nNote $\\sum_{i=1}^{n_i} (x_{ij} - \\bar{x}_i) = 0$. So we have\n\n$$\nSS_\\text{Total} = SS_\\text{Error} + SS_\\text{Treatment}\n$$\n\nNote that $SS_\\text{Error}$ is also known as the sum of variability within the groups, and that $SS_\\text{Treatment}$ is also known as the variability between the groups.\n\nThe decomposition holds under both $H_0$ and $H_1$, and hence allows us to construct a test statistic. Intuitively, under $H_0$, both $SS_\\text{Error}$ and $SS_\\text{Treatment}$ should contribute to $SS_\\text{Total}$ **in the same manner (once properly normalized)**. Alternatively, under $H_1$, it is expected that $SS_\\text{Treatment}$ would contribute **more heavily** to the total variability.\n\nThe test statistic we constructed is called F-statistic, defined as the ratio of $SS_\\text{Treatment}$ and $SS_\\text{Error}$ normalized by their respective degrees of freedom $L-1$ and $m-L$:\n\n$$\nF = \\frac{SS_\\text{Treatment}/(L-1)}{SS_\\text{Error}/(m-L)}\n$$\n\nThese normalized quantities are, respectively, denoted by $MS_\\text{Treatment}$ and $MS_\\text{Error}$ standing for \"Mean Squared\". Hence, $F = \\frac{MS_\\text{Treatment}}{MS_\\text{Error}}$\n\nUnder $H_0$, the F-statistic follows an F-distribution with $L-1$ degrees of freedom for the numerator and $m-L$ degrees of freedom for the denominator. Intuitively, under $H_0$, we expect the numerator and denominator to have similar values, hence expect $F$ to be around $1$ (indeed most of the mass of $F$ distributions is concentrated around $1$). However, if $MS_\\text{Treatment}$ is significantly larger, then it indicates that $H_0$ may not hold. Hence, the approach of the F-test is to reject $H_0$ if the F-statistic is geater than the $1-\\alpha$ quantile of the respective F-distribution. Similarly, the $p$-value for an observed F-statistic $f_o$ is given by\n\n$$\np = P(F_{L-1, m-L} > f_o)\n$$\n\nwhere $F_{L-1, m-L}$ is an F-distributed random variable with $L-1$ numerator degrees of freedom and $m-L$ denominator degrees of freedom.\n\n```{julia}\nusing Random, Distributions, DataFrames, GLM, CategoricalArrays\n\nRandom.seed!(0)\n\nallData = [rand(Normal(10, 1), 20), rand(Normal(11, 2), 30), rand(Normal(11, 2), 25)]\n\n# manual ANOVA\n# the decomposition of the sum of squares\n# F-test\nnArray = length.(allData)\nd = length(nArray)\n\nxBarTotal = mean(vcat(allData...))\nxBarArray = mean.(allData)\n\nssBetween = sum([nArray[i] * (xBarArray[i] - xBarTotal)^2 for i in 1:d])\nssWithin = sum([sum([(ob - xBarArray[i])^2 for ob in allData[i]]) for i in 1:d])\n\ndfBetween = d - 1\ndfError = sum(nArray) - d\n\nmsBetween = ssBetween / dfBetween\nmsError = ssWithin / dfError\n\nfStat = msBetween / msError\npVal = ccdf(FDist(dfBetween, dfError), fStat)\n\nprintln(\"Maunal ANOVA: \\nF-Statistic: \", fStat, \"\\np-value: \", pVal)\n\n# ANOVA using GLM package which requires the DataFrames package\nnArray = length.(allData)\nd = length(nArray)\n\ntreatment = vcat([fill(k, nArray[k]) for k in 1:d]...)\nresponse = vcat(allData...)\ndf = DataFrame(Response=response, Treatment=categorical(treatment))\nmodelH0 = lm(@formula(Response ~ 1), df)\nmodelH1 = lm(@formula(Response ~ 1 + Treatment), df)\nres = ftest(modelH1.model, modelH0.model)\n\nprintln(\"GLM ANOVA: \\nF-Statistic: \", res.fstat[2], \"\\np-value: \", res.pval[2])\n```\n\n#### More on the distribution of the F-Statistic\n\nHere we use the Monte Carlo simulation to generate the distribution of the F-Statistic for two different cases ($H_0$ and $H_1$).\n\nUnder $H_0$, the distribution of F-Statistic obtained via Monte Carlo should be exactly the same as the analytical F-distribution with the same numerator and denominator degrees of freedom, but it's not for the distribution of F-Statistic under $H_1$.\n\n```{julia}\nusing Random, Distributions, CairoMakie\n\nRandom.seed!(0)\n\nfunction anovaFStat(allData)\n    xBarArray = mean.(allData)\n    nArray = length.(allData)\n    xBarTotal = mean(vcat(allData...))\n    Le = length(nArray)\n\n    ssBetween = sum([nArray[i] * (xBarArray[i] - xBarTotal)^2 for i in 1:Le])\n    ssWithin = sum([sum([(ob - xBarArray[i])^2 for ob in allData[i]]) for i in 1:Le])\n\n    return ((ssBetween / (Le - 1)) / (ssWithin / (sum(nArray) - Le)))\nend\n\ncase1 = [13.4, 13.4, 13.4, 13.4, 13.4]\ncase2 = [12.7, 11.8, 13.4, 12.7, 12.9]\nstdDevs = [2, 2, 2, 2, 2]\nnumObs = [24, 15, 13, 23, 9]\nLe = length(case1)\n\nN = 10^5\n\nmcFstatsH0 = Array{Float64}(undef, N)\nfor i in 1:N\n    mcFstatsH0[i] = anovaFStat([rand(Normal(case1[j], stdDevs[j]), numObs[j]) for j in 1:Le])\nend\n\nmcFstatsH1 = Array{Float64}(undef, N)\nfor i in 1:N\n    mcFstatsH1[i] = anovaFStat([rand(Normal(case2[j], stdDevs[j]), numObs[j]) for j in 1:Le])\nend\n\nfig, ax = stephist(mcFstatsH0; bins=100, color=:blue, normalization=:pdf, label=\"H0\")\nstephist!(ax, mcFstatsH1; bins=100, color=:red, normalization=:pdf, label=\"H1\")\n\ndfBetween = Le - 1\ndfError = sum(numObs) - Le\nxGrid = 0:0.01:10\n\nlines!(ax, xGrid, pdf.(FDist(dfBetween, dfError), xGrid); color=:green, label=\"F-Statistic analytic\")\n\ncritVal = quantile(FDist(dfBetween, dfError), 0.95)\n\nvlines!(ax, [critVal]; color=:black, linestyle=:dash, label=\"Critical value boundary\")\n\naxislegend(ax)\nax.xlabel = \"F-value\"\nax.ylabel = \"Density\"\nfig\n```\n\nAnalysis presented here is just the *one-way ANOVA*, which means that we have only one treatment category having different treatment levels. Often we may have two treatment categories, each of which has different treatment levels (*two-way ANOVA*). This can be extended to higher dimensional extensions, which are often considered in *block factorial design*. In addition, once we reject $H_0$, we often want to known which specific treatments have an effect and in which way. This involves *multiple comparisons*.\n\n### Independence and goodness of fit\n\n#### Testing for an indenpendent sequence\n\nHere, we'll use the *Wald-Wolfowitz runs test* to perform hypothesis testing for checking if a sequence of random variables is i.i.d. (indenpendent and identically distributed).\n\nConsider a sequence of data points $x_1, ..., x_n$ with sample mean $\\bar{x}$. For simplicity, assume that no point equals the sample mean. Now transform the sequence to $y_1, ..., y_n$ via $y_i = x_i - \\bar{x}$.\n\nWe now consider the signs of $y_i$. For example, in a dataset with $20$ observations, once considering the signs we may have a sequence such as\n\n$$\n+-+-----++--+---++++\n$$\n\nindicating that the first is positive (greater than the mean), the second is negative (less than the mean), and so on.\n\nNote that we assume no exact $0$ for $y_i$ and if such exist we can arbitrarily assign them to be either positive or negative. We then create a random variable $R$ by counting the number of *runs* in this sequence, **where a run is a consecutive sequence of points having the same sign**. In our example, the runs (visually separated by white space) are\n\n$$\n+\\ -\\ +\\ -----\\ ++\\ --\\ +\\ ---\\ ++++\n$$\n\nHance $R = 9$.\n\nThe essence of the Wald-Wolfowitz runs test is an approximation of the distribution of $R$ under $H_0$. The null hypothesis is that the data is i.i.d. Under $H_0$, $R$ can be shown to be approximately follow a **normal** distribution with mean $\\mu$ and variance $\\sigma^2$, where\n\n$$\n\\mu = 2 \\frac{n_+n_-}{n} + 1,\\qquad \\sigma^2 = \\frac{(\\mu - 1)(\\mu - 2)}{n - 1}\n$$\n\nHere $n_+$ is the number of positive signs and $n_-$ is the number of negative signs. Note that $n_+$ and $n_-$ are also random variables. Clearly, $n_+ + n_- = n$, the total number of observations. With such values at hand the test creates the $p$-value via\n\n$$\n2P(Z > \\left|\\frac{R - \\mu}{\\sigma}\\right|)\n$$\n\nwhere $Z$ is a standard normal variable.\n\nIn the following code, we'll validate that the random variable $R$ (i.e. runs) is approximately distributed as a normal distribution with mean $\\mu$ and variance $\\sigma$ defined above under $H_0$ (i.e. the data points are i.i.d.). In other words, the distribution of the $p$-value is $U(0, 1)$.\n\n```{julia}\nusing Random, StatsBase, Distributions, CairoMakie\n\nRandom.seed!(0)\n\n# we'll use a Monte Carlo simulation to obtain the empirical distribution of the p-value\n# sample size: n\n# experimental numbers: N\nn, N = 10^3, 10^6\n\n# calculate the p-value for each sample\nfunction waldWolfowitz(data)\n    n = length(data)\n    sgns = data .> mean(data)\n    nPlus, nMinus = sum(sgns), n - sum(sgns)\n    wwMu = 2 * nPlus * nMinus / n + 1\n    wwVar = (wwMu - 1) * (wwMu - 2) / (n - 1)\n\n    R = 1\n    for i in 1:(n-1)\n        R += sgns[i] != sgns[i+1]\n    end\n\n    zStat = abs((R - wwMu) / sqrt(wwVar))\n    2 * ccdf(Normal(), zStat)\nend\n\n# repeat the process for N times\npVals = [waldWolfowitz(rand(Normal(), n)) for _ in 1:N]\n# here, we calculate the hypothesis testing power\n# under H0, the power is equal to α\nfor alpha in [0.001, 0.005, 0.01, 0.05, 0.1]\n    pva = sum(pVals .< alpha) / N\n    println(\"For alpha = $(alpha), p-value area = $(pva)\")\nend\n\nfig = Figure(size=(500, 800))\n# we can see that the distributio of the p-value is U(0, 1)\n# the reason why spikes of high density appear is that\n# we are approximating a discrete random variable R (can only have positive integers) with a normal random variable\nax1 = Axis(fig[1, 1]; xlabel=\"p-value\", ylabel=\"Frequency\")\nhist!(ax1, pVals; bins=5 * n)\n\npGrid = 0:0.001:1\n# get the ECDF using the ecdf function from StatsBase package\nFhat = ecdf(pVals)\n\n# the ECDF indicates that the distribution is almost uniform\nax2 = Axis(fig[2, 1]; xlabel=\"p-value\", ylabel=\"ECDF\")\nlines!(ax2, pGrid, Fhat.(pGrid))\nfig\n```\n\n### More on power\n\n#### Parameters affecting power\n\nEstimate the hypothesis testing powers under different scenarios:\n\n```{julia}\nusing Random, Distributions, KernelDensity, CairoMakie\n\nRandom.seed!(1)\n\n# calculate T-statistic by sampling n observations from a given normal distribution\nfunction tSat(mu0, mu, sig, n)\n    sample = rand(Normal(mu, sig), n)\n    xBar = mean(sample)\n    s = std(sample)\n    (xBar - mu0) / (s / sqrt(n))\nend\n\nmu0, mu1A, mu1B = 20, 22, 24\nsig, n = 7, 5\nN = 10^6\nalpha = 0.05\n\n# the underlying mean equals the mean under the null hypothesis\n# the power is α\ndataH0 = [tSat(mu0, mu0, sig, n) for _ in 1:N]\n# the underlying mean is increased\ndataH1A = [tSat(mu0, mu1A, sig, n) for _ in 1:N]\n# the underlying mean is increased further\ndataH1B = [tSat(mu0, mu1B, sig, n) for _ in 1:N]\n# increase the sample size\ndataH1C = [tSat(mu0, mu1B, sig, 2 * n) for _ in 1:N]\n# the underlying variance is decreased\ndataH1D = [tSat(mu0, mu1B, sig / 2, 2 * n) for _ in 1:N]\n\n# calculate the quantile of alpha\ntCrit = quantile(TDist(n - 1), 1 - alpha)\n# estimate the power\nestPwr(sample) = sum(sample .> tCrit) / N\n\nprintln(\"Rejection boundary: \", tCrit)\nprintln(\"Power under H0 (equal α): \", estPwr(dataH0))\nprintln(\"Power under H1A (increase μ): \", estPwr(dataH1A))\nprintln(\"Power under H1B (increase μ further): \", estPwr(dataH1B))\nprintln(\"Power under H1C (increase sample size n to 2n): \", estPwr(dataH1C))\nprintln(\"Power under H1D (decrease σ to σ/2): \", estPwr(dataH1D))\n\nkH0 = kde(dataH0)\nkH1A = kde(dataH1A)\nkH1B = kde(dataH1B)\nkH1C = kde(dataH1C)\nkH1D = kde(dataH1D)\n\nxGrid = -10:0.1:15\n\nfig, ax = lines(xGrid, pdf(kH0, xGrid); color=:blue, label=\"Distribution under H0\")\nlines!(ax, xGrid, pdf(kH1A, xGrid); color=:red, label=\"Distribution under H1A\")\nlines!(ax, xGrid, pdf(kH1B, xGrid); color=:green, label=\"Distribution under H1B\")\nlines!(ax, xGrid, pdf(kH1C, xGrid); color=:orange, label=\"Distribution under H1C\")\nlines!(ax, xGrid, pdf(kH1D, xGrid); color=:purple, label=\"Distribution under H1D\")\nvlines!(ax, [tCrit]; color=:black, label=\"Critical value boundary\")\nax.xlabel = L\"\\Delta = \\mu - \\mu_0\"\nax.ylabel = \"Density\"\naxislegend(ax)\nfig\n```\n\nUnder a normal population, $\\mu$, $\\sigma$, $\\alpha$, and sample size all have an effect on the power. But in practice, if keeping $\\alpha$ constant, it is only the sample size can be controlled.\n\nAs a consequence, we need to know what is the sample size for our expected power.\n\n#### Power curves\n\nAs mentioned before, if keeeping $\\alpha$ constant, it is only the sample size can be controlled. Therefore, underlying a given $\\alpha$, we must know\n\n* To obtain a given power, how many observations (i.e. sample size $n$) do we need?\n\n* For a few of available sample sizes, what's the largest power we can obtain?\n\n**A *power curve* is a plot of the power as a function of certain parameters we are interested in.**\n\nFor example, under the hypothesis test setup:\n\n$$\nH_0: \\mu = \\mu_0\\ \\ \\ \\ \\text{and}\\ \\ \\ \\ H_1: \\mu > \\mu_0\n$$\n\nwe want to estimate the power of a one-sided T-test for different scenarios combining the mean $\\mu$ and the sample size $n$ under normality assumption.\n\n```{julia}\nusing Random, Distributions, CairoMakie\n\n# calculate T-statistic by sampling n observations from a given normal distribution\nfunction tSat(mu0, mu, sig, n)\n    sample = rand(Normal(mu, sig), n)\n    xBar = mean(sample)\n    s = std(sample)\n    (xBar - mu0) / (s / sqrt(n))\nend\n\n# estimate the statistical power under a given scenario (μ, n)\nfunction powerEstimate(mu0, mu1, sig, n, alpha, N)\n    Random.seed!(0)\n    sampleH1 = [tSat(mu0, mu1, sig, n) for _ in 1:N]\n    critVal = quantile(TDist(n - 1), 1 - alpha)\n    sum(sampleH1 .> critVal) / N\nend\n\nmu0 = 20\nsig = 5\nalpha = 0.05\nN = 10^4\nrangeMu1 = 16:0.1:30\nnList = [5, 10, 20, 30]\n\npowerCurves = [powerEstimate.(mu0, rangeMu1, sig, n, alpha, N) for n in nList]\n\nfig, ax = lines(rangeMu1, powerCurves[1]; color=:blue, label=\"n = $(nList[1])\")\nlines!(ax, rangeMu1, powerCurves[2]; color=:red, label=\"n = $(nList[2])\")\nlines!(ax, rangeMu1, powerCurves[3]; color=:green, label=\"n = $(nList[3])\")\nlines!(ax, rangeMu1, powerCurves[4]; color=:purple, label=\"n = $(nList[4])\")\nax.xlabel = L\"\\mu\"\nax.ylabel = \"Power\"\naxislegend(ax; position=:lt)\nfig\n```\n\nAs seen in the above figure, under a given power, if $\\mu$ is away from $\\mu_0$ further, then we can take less observations. On the other hand, for a given $\\mu$, if we hope to increase the power, we need to increase the sample size.\n\nAnother point to note is that the x-axis could be adjusted to represent the difference $\\Delta = \\mu - \\mu_0$. Furthermore, one could make the axis scale invariant by dividing $\\Delta$ by the standard deviation.\n\n#### Distribution of the $p$-value\n\nFor a uniform $U(0, 1)$ random variable, we have its CDF $F(x) = x, x \\in [0, 1]$, and CCDF $F_c(x) = 1 - x, x \\in [0, 1]$.\n\n$p$-value is defined as $p = P(S > u)$, where $S$ is a random variable representing the test statistic, $u$ is the observed test statistic, and $p$ is the $p$-value of the observed test statistic.\n\nTo discuss the distribution of the $p$-value, denote the random variable of the $p$-value by $P$. Hence $P = 1 - F(S)$, where $F(\\cdot)$ is the CDF of the test statistic under $H_0$. Note that $P$ is just a transformation of the test statistic random variable $S$. Assume that $S$ is continuous and assume that $H_0$ holds, hence $P(S < u) = F(u)$. we now have\n\n$$\n\\begin{align}\nP(P > x) &= P(1 - F(S) > x) \\\\\n&= P(F(S) < 1 - x) \\\\\n&= P(S < F^{-1}(1-x)) \\\\\n&= F(F^{-1}(1-x)) \\\\\n&= 1 - x\n\\end{align}\n$$\n\nObviously, the CDF of the random variable $P$ is $F(x) = P(P < x) = x, x \\in [0, 1]$, so the distribution of the $p$-value is $U(0, 1)$ under $H_0$.\n\nIf $H_0$ does not hold, then $P(S < u) \\ne F(u)$ and the derivation above fails. In such a case, the distribution of the $p$-value is no longer uniform. In fact, in such a case, if the setting is such that the power of the test statistic increases, then we expect the distribution of the $p$-value to be more concentrated around $0$ than a uniform distribution.\n\n```{julia}\nusing Random, Distributions, KernelDensity, CairoMakie\n\nRandom.seed!(1)\n\nfunction pval(mu0, mu, sig, n)\n    sample = rand(Normal(mu, sig), n)\n    xBar = mean(sample)\n    s = std(sample)\n    tStatistics = (xBar - mu0) / (s / sqrt(n))\n    # under H0, t ∼ T(n - 1)\n    ccdf(TDist(n - 1), tStatistics)\nend\n\nmu0, mu1A, mu1B = 20, 23, 26\nsig, n, N = 7, 5, 10^6\n\npValsH0 = [pval(mu0, mu0, sig, n) for _ in 1:N]\npValsH1A = [pval(mu0, mu1A, sig, n) for _ in 1:N]\npValsH1B = [pval(mu0, mu1B, sig, n) for _ in 1:N]\n\nalpha = 0.05\nestPwr(pVals) = sum(pVals .< alpha) / N\n\nprintln(\"Power under H0: \", estPwr(pValsH0))\nprintln(\"Power under H1A: \", estPwr(pValsH1A))\nprintln(\"Power under H1B: \", estPwr(pValsH1B))\n\nfig, ax = stephist(pValsH0; bins=100, normalization=:pdf, color=:blue, label=\"Under H0\")\nstephist!(ax, pValsH1A; bins=100, normalization=:pdf, color=:red, label=\"Under H1A\")\nstephist!(ax, pValsH1B; bins=100, normalization=:pdf, color=:green, label=\"Under H1B\")\nvlines!(ax, [alpha]; color=:black, label=\"α\", linestyle=:dash)\naxislegend(ax)\nax.xlabel = \"p-value\"\nax.ylabel = \"Density\"\nfig\n```\n\n## Appendices\n\n### Base conversions\n\n1. Base $10$ to base $k$:\n\n- 整数部分：除 $k$ 取余，逆序写出，直到**商**为 $0$。\n\n- 小数部分：乘 $k$ 取整，顺序写出，直到**小数部分**为 $0$ 或达到指定的精度为止。\n\nNote: $\\frac{1}{2} \\frac{1}{1} \\frac{0}{0} \\frac{.}{} \\frac{1}{-1} \\frac{0}{-2} \\frac{1}{-3}$.\n\n### Misc\n\n1. The largest integer which can be represented by `Int32` is $2^{31} - 1$.\n\n对于 `Int32` 来说，最高位为**符号位**，因此最大的二进制数为 $\\underbrace{1...1}_{\\text{31 1's}}$，即所能表示的最大整数为 $1\\cdot 2^0 + 1\\cdot 2^1 + ... + 1\\cdot 2^{30}$，为了表示方便，我们将其加 $1$，变为 $1\\underbrace{0...0}_{\\text{31 0's}}$，再减 $1$，即 $2^{31} - 1$。\n","srcMarkdownNoYaml":"\n\n## References\n\n1. **Statistics with Julia** by Yoni Nazarathy (2021).\n\n## Pseudorandom number generation\n\nFor pseudorandom number generation, there is some deterministic (non-random and well defined) sequence $\\{x_n\\}$, specified by\n\n$$\nx_{n+1} = f(x_n, x_{n-1}, ...)\n$$\n\noriginating from some specified *seed* $x_0$. The mathematical function $f(\\cdot)$ is designed to yield desirable properties for the sequence $\\{x_n\\}$ that make it appear random.\n\nThose properties include:\n\n1.  Elements $x_i$ and $x_j$ for $i \\neq j$ should appear statistically independent. That is, knowing the value of $x_i$ should not yield any information about the value $x_j$.\n\n2.  The distribution of $\\{x_n\\}$ should appear uniform. That is, there shouldn't be values (or ranges of values) where elements of $\\{x_n\\}$ occur more frequently than others.\n\n3.  The range covered by $\\{x_n\\}$ should be well defined.\n\n4.  The sequence should repeat itself as rarely as possible.\n\nIn Julia, the main player for pseudorandom number generation is the function `rand()`, which generates a random number in each call without giving any arguments once a seed is set (it is usually set to the current time by default). You can set the seed yourself by using the `Random.seed!()` function from the Random package.\n\n```{julia}\nusing Random\n\nRandom.seed!(2023)\nprintln(\"Seed 2023: \", rand(), \"\\t\", rand(), \"\\t\", rand())\nRandom.seed!(2024)\nprintln(\"Seed 2024: \", rand(), \"\\t\", rand(), \"\\t\", rand())\nRandom.seed!(2023)\nprintln(\"Seed 2023: \", rand(), \"\\t\", rand(), \"\\t\", rand())\n```\n\nAs can be seen from the output, setting the same seed will generate the same sequence.\n\n### Creating a simple pseudorandom number generator\n\nHere, we create a *Linear Congruential Generator* (LCG). The function $f(\\cdot)$ used here is just a linear transformation modulo $m$: $x_{n+1} = (ax_n + c) \\mod m$.\n\nHere, we pick $m = 2^{32}$, $a = 69069$, $c = 1$, which yields sensible performance.\n\n```{julia}\nusing DataFrames, AlgebraOfGraphics, CairoMakie\n\na, c, m = 69069, 1, 2^32\nnext(x) = (a * x + c) % m\n\nN = 10^6\nvec = Array{Float64,1}(undef, N)\n\nx = 2024  # Seed\nfor i in 1:N\n    global x = next(x)\n    vec[i] = x / m  # Scale x to [0, 1]\nend\ndf = DataFrame(x=1:N, y=vec)\n\nfig = Figure()\np1 = data(first(df, 5000)) * mapping(:x, :y) * visual(Scatter, markersize=3)\np2 = data(df) * mapping(:y) * visual(Hist, bins=50, normalization=:pdf)\ndraw!(fig[1, 1], p1, axis=(xlabel=\"n\", ylabel=L\"\\mathbf{x_n}\"))\ndraw!(fig[1, 2], p2, axis=(xlabel=\"x\", ylabel=\"Density\"))\nfig\n```\n\n### More about Julia's pseudorandom number generator\n\nIn addition to `rand()`, we can also use `randn()` to generate **normally distributed** random numbers.\n\nAfter invoking `using Random`, the following functions are available:\n\n- `Random.seed!()`\n\n- `randsubseq()`\n\n- `randstring()`\n\n- `randcycle()`\n\n- `bitrand()`\n\n- `randperm()` and `shuffle()` for permutations\n\nIn addition, in Julia, we can create an object representing a pseudorandom number generator implemented via a specified algorithm, for example, the *Mersenne Twister* pseudorandom number generator, which is considerably more complicated than the LCG described above. In Julia, we can create such an object of the *Mersenne Twister* pseudorandom number generator by calling `rng = MersenneTwister(seed)`, and then pass the `rng` to `rand()` to let it use the given pseudorandom number generator to generate pseudorandom numbers.\n\n## Monte Carlo simulation\n\nThe core idea of Monte Carlo simulation lies in building a mathematical relationship between an unknown quantity to be estimated and the probability of a certain event, which can be estimated by statistical sampling. Then, we can get an estimate of this unknown quantity.\n\nWe can use this idea to estimate the value of $\\pi$.\n\n```{julia}\nusing DataFrames, AlgebraOfGraphics, CairoMakie\n\nline_df = DataFrame(x=[0, 0, 1, 1, 0],\n    y=[0, 1, 1, 0, 0])\n\nx = range(0, 1, length=1000)\nquarter_circle_df = DataFrame(x=x,\n    y=@. sqrt(1 - x^2))\n\nrect = data(line_df) * mapping(:x, :y) * visual(Lines)\nquarter_circle = data(quarter_circle_df) * mapping(:x, :y) * visual(Lines)\ndraw(rect + quarter_circle, axis=(limits=(0, nothing, 0, nothing),))\n```\n\nAs can be seen from the above figure, we know:\n\n1.  The area of the unit square is 1;\n\n2.  The area of the first quadrant of the unit circle is $\\pi / 4$;\n\n3.  Then, if we randomly throw a ball within the unit square, the probability of the event that this ball falls into the area of the first quadrant of the unit circle is $\\pi / 4$. Further, we know that the probability of this event can be estimated by its frequency if we repeat this experiment infinitely many times; therefore, we can estimate the value of $\\pi$ by the following formula:\n\n$$\n\\hat{\\pi} = 4 \\frac{\\text{The number of times falling in }x^2 + y^2 \\leq 1}{\\text{Total number of times}}\n$$\n\n```{julia}\nusing Random, LinearAlgebra, AlgebraOfGraphics, CairoMakie, DataFrames\n\nRandom.seed!(1234)\n\nN = 10^5\ndf = DataFrame([(x=rand(), y=rand()) for _ in 1:N])\ntransform!(df, [:x, :y] => ByRow((x, y) -> ifelse(norm([x, y]) <= 1, \"in\", \"out\")) => :flag)\npi_estimate = 4 * count(df.flag .== \"in\") / N\nprintln(\"π estimate: \", pi_estimate)\n\nfig = Figure()\np = data(df) * mapping(:x, :y, color=:flag) * visual(Scatter, markersize=1)\ndraw!(fig, p, axis=(limits=(0, nothing, 0, nothing),))\nfig\n```\n\n## `Distributions` and related packages for probability distributions\n\n### Introduction\n\nPackages:\n\n- `Statistics` (built-in)\n\n- `StatsBase`\n\n- `Distributions`\n\n#### Weighted vectors\n\nThe `StatsBase` package provides the \"weighted vector\" object via `Weights()`, which allows for an array of values to be given probabilistic weights.\n\nAn alternative of `Weights()` is to use the `Categorical` distribution supplied by the `Distributions` package.\n\nTogether with `Weights()`, you can use the `sample()` function from `StatsBase` to generate observations.\n\n```{julia}\nusing StatsBase, Random\n\nRandom.seed!(1234)\n\ngrades = 'A':'E'\nweights = Weights([0.1, 0.2, 0.1, 0.2, 0.4])\n\nN = 10^6\nd = sample(grades, weights, N)\n[count(i -> i == g, d) for g in grades] / N\n```\n\n#### Distribution type objects\n\nThe `Distributions` package allows us to create distribution type objects based on **what family they belong to**. Then these distribution type objects can be used as arguments for other functions.\n\n```{julia}\nusing Distributions, CairoMakie\n\ndist = TriangularDist(0, 2, 1)  # Triangular distribution\nx = 0:0.01:2\nu = 0:0.01:1\n\nfig = Figure(size=(800, 250))\nlines!(Axis(fig[1, 1], xlabel=\"x\", ylabel=\"f(x)\"), x, pdf.(dist, x))  # PDF\nlines!(Axis(fig[1, 2], xlabel=\"x\", ylabel=\"F(x)\"), x, cdf.(dist, x))  # CDF\nlines!(Axis(fig[1, 3], xlabel=\"u\", ylabel=L\"\\mathbf{F^{-1}(u)}\"), u, quantile.(dist, u))  # ICDF\nfig\n```\n\n```{julia}\nprintln(\"Parameters: \", params(dist))\nprintln(\"Central descriptors: \", mean(dist), \", \", median(dist))\nprintln(\"Dispersion descriptos: \", var(dist), \", \", std(dist))\nprintln(\"Higher-order moment shape descriptors: \", skewness(dist), \", \", kurtosis(dist))\nprintln(\"Range: \", minimum(dist), \", \", maximum(dist))\nprintln(\"Mode: \", mode(dist), \", \", modes(dist))  # Value(s) of x where PMF or PDF is maximized\n```\n\n## Univariate distributions\n\n### Families of discrete distributions\n\n#### Discrete uniform distribution\n\n```{julia}\nusing StatsBase, CairoMakie\n\nfaces, N = 1:6, 10^6\n\nmcEstimate = counts(rand(faces, N), faces) / N  # rand(faces, N) is identical to rand(DiscreteUniform(1, 6), N)\ntheory = [1 / 6 for _ in faces]\n\nfig, ax = stem(faces, mcEstimate, label=\"Estimate\",\n    color=:black, stemcolor=:black,\n    stemwidth=6, markersize=18,\n    axis=(xlabel=\"x\", ylabel=\"f(x)\"))\nstem!(ax, faces, theory, label=\"Theory\",\n    color=:red, stemcolor=:red)\nylims!(ax, nothing, 0.25)\naxislegend(ax)\nfig\n```\n\n#### Binomial distribution\n\n```{julia}\nusing StatsBase, Distributions, CairoMakie\n\nbinomialRV(n, p) = sum(rand(n) .< p)\n\np, n, N = 0.25, 10, 10^6\n\nb_dist = Binomial(n, p)\nx = 0:n\nb_pmf = pdf.(b_dist, x)\nest_data = [binomialRV(n, p) for _ in 1:N]\nest_pmf = counts(est_data, 0:n) / N\n\nfig, ax = stem(x, est_pmf, label=\"Estimate\",\n    color=:black, stemcolor=:black,\n    stemwidth=6, markersize=18,\n    axis=(xlabel=\"x\", ylabel=\"f(x)\"))\nstem!(ax, x, b_pmf, label=\"Theory\",\n    color=:red, stemcolor=:red)\naxislegend(ax)\nfig\n```\n\n#### Geometric distribution\n\nConsider an **infinite** sequence of independent trials, each with sucess probability $p$, and let $X$ be the **first** trial that is successful. Then the PMF is:\n\n$$\nP(X=x) = p(1-p)^{x-1}\n$$\n\nfor $x = 1, 2, ...$.\n\nAn alternative version is to count **the number of failures until success**. Obviously, we have $\\tilde{X} = X - 1$. Then the PMF is:\n\n$$\nP(\\tilde{X} = x) = p(1-p)^x\n$$\n\nfor $x = 0, 1, 2, ...$.\n\nIn `Distributions` package, `Geometric` stands for the distribution of $\\tilde{X}$.\n\n```{julia}\nusing StatsBase, Distributions, CairoMakie\n\nfunction geometricRV(p)\n    x = 0\n    while true\n        if rand() < p\n            return x\n        end\n        x += 1\n    end\nend\n\np = 0.25\nx = 0:25\nN = 10^6\n\ng_dist = Geometric(p)\ng_pmf = pdf.(g_dist, x)\nmcEstimate = counts([geometricRV(p) for _ in 1:N], x) / N\n\nfig, ax = stem(x, mcEstimate, label=\"Estimate\",\n    color=:black, stemcolor=:black,\n    stemwidth=6, markersize=18,\n    axis=(xlabel=\"x\", ylabel=\"f(x)\"))\nstem!(ax, x, g_pmf, label=\"Theory\",\n    color=:red, stemcolor=:red)\naxislegend(ax)\nfig\n```\n\n#### Negative binomial distribution\n\n$X$ stands for **the number of trials** until the $r$-th success. The PMF is:\n\n$$\nP(X=x) = \\binom{x-1}{r-1} p^r (1-p)^{x-r}\n$$\n\nfor $x = r, r+1, r+2, ...$.\n\nSimilarly to the geometric distribution, we usually count **the number of failures** until the $r$-th success. The PMF is:\n\n$$\nP(\\tilde{X} = x) = \\binom{x+r-1}{x} p^r (1-p)^x\n$$\n\nfor $x = 0, 1, 2, ...$.\n\n```{julia}\nusing StatsBase, Distributions, CairoMakie\n\nfunction nbRV(r, p)\n    x = 0\n    success = 0\n    while true\n        if success == r\n            return x\n        end\n        if rand() < p\n            success += 1\n        else\n            x += 1\n        end\n    end\nend\n\nr = 5\np = 0.25\nx = 0:60\nN = 10^6\n\nnb_dist = NegativeBinomial(r, p)\nnb_pmf = pdf.(nb_dist, x)\nmcEstimate = counts([nbRV(r, p) for _ in 1:N], x) / N\n\nfig, ax = stem(x, mcEstimate, label=\"Estimate\",\n    color=:black, stemcolor=:black,\n    stemwidth=6, markersize=18,\n    axis=(xlabel=\"x\", ylabel=\"f(x)\"))\nstem!(ax, x, nb_pmf, label=\"Theory\",\n    color=:red, stemcolor=:red)\naxislegend(ax)\nfig\n```\n\n::: {.callout-note title=\"Summary\"}\n\nSo far, we've seen that binomial distribution, Bernoulli distribution (0-1 distribution, two-point distribution), geometric distribution, and negative binomial distribution all involve **Bernoulli trials** which has exactly **two** possible outcomes, \"success\", and \"failure\", where the probability of success is the same every time the experiment is conducted.\n\nIn a word:\n\n- Binomial distribution ($X \\sim B(n, p)$): $X$ indicates the number of successes in $n$ Bernoulli experiments.\n\n- Bernoulli distribution ($X \\sim B(1, p)$): $X$ indicates the number of successes in $1$ Bernoulli experiments.\n\n- Geometric distribution ($X \\sim Ge(p)$): $X$ indicates the number of total Bernoulli experiments until the **first** success.\n\n- Negative binomial distribution ($X \\sim Nb(r, p)$): $X$ indicates the number of total Bernoulli experiments until the $r$-th success.\n\nObviously, a binomial distribution or a negative binomial distribution can be divided into $n$ Bernoulli distributions or $r$ geometric distributions, respectively.\n\n:::\n\n#### Hypergeometric distribution\n\nHypergeometric distribution means **sampling without replacement**, which means the probability of success changes for each subsequent sample.\n\nThe PMF is:\n\n$$\np(x) = \\frac{\\binom{M}{x} \\binom{N-M}{n-x}}{\\binom{N}{n}}\n$$\n\nfor $x = max(0, n+M-N), ..., min(n, M)$, where $N$ (the population size), $M$ (the number of successes), and $n$ (the sample size) are all parameters.\n\nNote: $max(0, n+M-N)$: if $n \\gt N-M$ (i.e., $n$ is greater than **the number of failures**), then at least $n - (N-M)$ successes must occur.\n\n```{julia}\nusing Distributions, CairoMakie\n\nN, M, n = 500, 100, 60\nx = max(0, n - (N - M)):min(n, M)\n\nh_dist = Hypergeometric(M, N - M, n)  # the 1st is the number of successes; the 2nd is the number of failures; the 3rd is the sample size\nh_pmf = pdf.(h_dist, x)\n\nstem(x, h_pmf,\n    color=:black, stemcolor=:black,\n    axis=(xlabel=\"x\", ylabel=\"f(x)\"))\n```\n\n#### Poisson distribution\n\nThe **Poisson process** is a **stochastic process** (random process) which can be used to model **occurrences of events over time** or more generally in space.\n\nIn a Poisson process, during an infinitesimally small time interval, $\\Delta t$, it is assumed that as $\\Delta t \\rightarrow 0$:\n\n1. There is an occurrence with probability $\\lambda \\Delta t$ and no occurrence with probability $1 - \\lambda \\Delta t$.\n\n2. The chance of 2 or more occurences during an interval of length $\\Delta t$ tends to $0$.\n\nHere, $\\lambda \\gt 0$ is the **intensity** of the Poisson process, and has the property that when multiplied by an interval of length $T$, **the mean occurrences** during the interval is $\\lambda T$.\n\nFor a Poisson process over the time interval $[0, T]$, the Poisson distribution can be used to describe **the number of occurrences**. The PMF is:\n\n$$\nP(x\\text{ Poisson process occurrences during interval }[0, T]) = \\frac{(\\lambda T)^x}{x!} e^{-\\lambda T}\n$$\n\nfor $x = 0, 1, 2, ...$.\n\nWhen the interval is $[0, 1]$, then we have the PMF:\n\n$$\np(x) = \\frac{\\lambda ^x}{x!} e^{-\\lambda}\n$$\n\nfor $x = 0, 1, 2, ...$, where $\\lambda$ is the mean of occurences.\n\nIn addition, **the times between occurrences** in the Poisson process are exponentially distributed.\n\nThe Poisson process has many elegant analytic properties. One such property is to consider the random variable $N \\ge 0$ such that\n\n$$\n\\prod_{i=1}^{N} U_i \\ge e^{-\\lambda} \\gt \\prod_{i=1}^{N+1} U_i\n$$\n\nwhere $U_1, U_2, ...$ is a sequence of i.i.d uniform $(0, 1)$ random variables and $\\prod_{i=1}^{0} \\equiv 1$.\n\nIt turns out that $N$ is Poisson distributed with mean $\\lambda$.\n\n```{julia}\nusing StatsBase, Distributions, CairoMakie\n\nfunction pRV(lambda)\n    N, p = 0, 1\n    while p >= MathConstants.e^(-lambda)\n        N += 1\n        p *= rand()\n    end\n    return N - 1\nend\n\nx = 0:20\nlambda = 5.5\nN = 10^6\n\np_dist = Poisson(lambda)\np_pmf = pdf.(p_dist, x)\n\nmcEstimate = counts([pRV(lambda) for _ in 1:N], x) / N\n\nfig, ax = stem(x, mcEstimate, label=\"Estimate\",\n    color=:black, stemcolor=:black,\n    stemwidth=6, markersize=18,\n    axis=(xlabel=\"x\", ylabel=\"f(x)\"))\nstem!(ax, x, p_pmf, label=\"Theory\",\n    color=:red, stemcolor=:red)\naxislegend(ax)\nfig\n```\n\n### Families of continuous distributions\n\n#### Continuous uniform distribution\n\n```{julia}\nusing Distributions, CairoMakie\n\nx = 0:0.1:2π\nN = 10^6\n\nc_unif_dist = Uniform(0, 2π)\nc_unif_pmf = pdf.(c_unif_dist, x)\nest_data = rand(N) * 2π  # Equivalent to rand(c_unif_dist, N)\n\nfig, ax = stephist(est_data, normalization=:pdf, color=:black, label=\"Estimate\")\nlines!(ax, x, c_unif_pmf, color=:red, label=\"Theory\")\nylims!(ax, nothing, 0.3)\naxislegend(ax)\nfig\n```\n\n#### Exponential distribution\n\nAs mentioned before, the exponential distribution is often used to model random durations between occurrences in the Poisson process.\n\nA **non-negative** random variable $X$, expoentially distributed with a rate parameter $\\lambda$, has PDF:\n\n$$\nf(x) = \\lambda e^{-\\lambda x}\n$$\n\nAs can be verified, the mean is $\\frac{1}{\\lambda}$, the variance is $\\frac{1}{\\lambda ^2}$, and the CCDF is $\\bar{F}(x) = e^{-\\lambda x}$.\n\nIn addition, exponential random variables possess **a lack of memory** property:\n\n$$\nP(X>t+s|X>t) = P(X>s)\n$$\n\nWhile geometric random variables also have such a property, this hints at the fact that **exponential random variables are the continuous analogs of geometric random variables**.\n\n::: {.callout}\n\nSuppose that X is an exponential random variable, and $Y = \\lfloor X \\rfloor$, where $\\lfloor \\cdot \\rfloor$ represents the **floor function**. Then we'll know that $Y$ is a geometric random variable:\n\n$$\np_Y (y) = P(\\lfloor X \\rfloor = y) = \\int_y^{y+1} \\lambda e^{-\\lambda x} \\mathrm{d}x = (e^{-\\lambda})^y (1-e^{-\\lambda})\n$$\n\nfor $y = 0, 1, 2, ...$.\n\nIf we set $p = 1 - e^{-\\lambda}$, then $Y$ is a geometric random variable (representing the number of failures until the first success) which starts at $0$ and has the success parameter $p$.\n\n:::\n\n**Note:** the parameter of `Exponential` is $\\frac{1}{\\lambda}$, instead of $\\lambda$.\n\nExponential distribution:\n\n```{julia}\nusing Distributions, CairoMakie\n\nlambda = 1\nx = 0:0.01:10\n\nexp_dist = Exponential(1 / lambda)\nexp_pmf = pdf.(exp_dist, x)\n\nlines(x, exp_pmf, color=:black)\n```\n\nThe PMF of the floor of an exponential random variable is a geometric distribution:\n\n```{julia}\nusing StatsBase, Distributions, CairoMakie\n\nlambda = 1\nN = 10^6\nx = 0:6\n\nexp_dist = Exponential(1 / lambda)\nfloor_data = counts(convert.(Int, floor.(rand(exp_dist, N))), x) / N\n\ngeom_dist = Geometric(1 - MathConstants.e^-lambda)\n\nfig, ax = stem(x, floor_data, label=\"Floor of Exponential\",\n    color=:black, stemcolor=:black,\n    stemwidth=6, markersize=18,\n    axis=(xlabel=\"x\", ylabel=\"f(x)\"))\nstem!(ax, x, geom_dist, label=\"Geometric\",\n    color=:red, stemcolor=:red)\naxislegend(ax)\nfig\n```\n\n#### Gamma distribution\n\nThe gamma distribution is commonly used to model **asymmetric non-negative** data.\n\n**It generalizes the *exponential distribution* and the *chi-squared distribution*.**\n\nConsider such an example, where **the lifetimes of light bulbs** are **exponentially distributed** with mean $\\frac{1}{\\lambda}$. Now imagine that we are lighting a room continuously with a single light bulb, and that we replace the bulb with a new one when it burns out. If we start at time $0$, what is **the distribution of time** until $n$ bulbs are replaced?\n\nOne way to describe this time is by the random variable $T$, where\n\n$$\nY = X_1 + X_2 + ... + X_n\n$$\n\nand $X_i$ are i.i.d. exponential random variables representing the lifetimes of light bulbs. It turns out that the distribution of $T$ is a gamma distribution.\n\nAt a first glance, this is quite similar with the case, where the random variable of geometric distribution indicates the total number of Bernoulli trials until the first success, while the random variable of negative binomial distribution indicates the total number of Bernoulli trials until the $r$-th success, and we have $Y = X_1 + X_2 + ... + X_r$, where $Y$ is a random variable of negative binomial distribution, and $X_i$ are i.i.d. geometric random variables.\n\nThe PDF of the gamma distribution is proportional to $x^{\\alpha - 1} e^{-\\lambda x}$, where $\\alpha$ is called the **shape parameter**, and $\\lambda$ is called the **rate parameter**.\n\nIn order to normalize $x^{\\alpha - 1} e^{-\\lambda x}$, we need to divide by $\\int_0^\\infty x^{\\alpha -1} e^{-\\lambda x} \\mathrm{d}x$, which can be represented by $\\frac{\\Gamma (\\alpha)}{\\lambda ^\\alpha}$, where $\\Gamma (\\cdot)$ is called the gamma function.\n\nThen, the PDF of the gamma distribution is:\n\n$$\nf(x) = \\frac{\\lambda ^\\alpha}{\\Gamma (\\alpha)} x^{\\alpha - 1} e^{-\\lambda x}\n$$\n\ni.e., $X \\sim Ga(\\alpha, \\lambda)$.\n\nIn the light bulbs case, we have $T \\sim Ga(n, \\lambda)$, where $\\alpha = n$.\n\nIt can also be evaluated that $E[X] = \\frac{\\alpha}{\\lambda}$ and $Var(X) = \\frac{\\alpha}{\\lambda ^2}$.\n\n::: {.callout-note title=\"Squared coefficient of variation\"}\n\n**Squared coefficient of variation** is often used for non-negative random variables:\n\n$$\nSCV = \\frac{Var(X)}{[E(X)]^2}\n$$\n\nThe SCV is a **normalized** or **unit-less** version of the variance.\n\nThe lower it is, the less variability in the random variable.\n\nIt can be seen that for a gamma random variable, the SCV is $\\frac{1}{\\alpha}$.\n\nFor the light bulbs case, the SCV is $\\frac{1}{n}$, which indicates for large $n$, i.e., more light bulbs, there is less variability.\n\n:::\n\n```{julia}\nusing Distributions, CairoMakie\n\nlambda = 1 / 3\nN = 10^6\nbulbs = [1, 10, 50]  # α = 1 is exponential\nx = 0:0.1:20\ncolors = [:blue, :red, :green]\n\n# Theoretical gamma PDFs\n# For each case, we set the rate parameter at λn, so that the mean time until all light bulbs run out is n/(λn) = 1/λ, independent of n\n# For the rate parameter, like Exponential, Gamma also accepts 1/λ, not λ\nga_dists = [Gamma(n, 1 / (n * lambda)) for n in bulbs]\n\n# Generate exponentially distributed pseudorandom numbers by using the inverse probability transformation\nfunction approxBySumExp(dist::Gamma)\n    n = Int64(shape(dist))  # shape() is used to get the shape parameter α\n    [sum(-(1 / (n * lambda)) * log.(rand(n))) for _ in 1:N]  # Generate n exponentially distributed pseudorandom numbers, and then add them up to generate N gamma distributed pseudorandom numbers\nend\n\nest_data = approxBySumExp.(ga_dists)\n\nfig = Figure()\nax = Axis(fig[1, 1])\nfor i in 1:length(bulbs)\n    label = string(\"Shape = \", round(shape(ga_dists[i]), digits=2), \", Scale = \", round(Distributions.scale(ga_dists[i]), digits=2))  # The inverse of the rate parameter is called the scale parameter. Of coourse, you can also use the rate() function to get the rate parameter (λ)\n    stephist!(ax, est_data[i], normalization=:pdf, color=colors[i], label=label, bins=50)\nend\nfor i in 1:length(bulbs)\n    lines!(ax, x, pdf.(ga_dists[i], x), color=colors[i])\nend\nxlims!(ax, 0, 20)\nylims!(ax, 0, 1)\naxislegend(ax)\nfig\n```\n\n**Note:** in the above code, we generate the exponentially distributed pseudorandom numbers by using the **inverse probability transformation**: $F(x) = P(X \\le x) = 1 - e^{-\\lambda x} \\Longrightarrow U = F(X) \\Longrightarrow U = 1 - e^{-\\lambda X} \\Longrightarrow X = -\\frac{1}{\\lambda} \\log(1-U) \\Longrightarrow X = -\\frac{1}{\\lambda} \\log U$ (since we'll use the `rand` function to generate uniformly distributed pseudorandom numbers in $[0, 1]$, it's reasonable that replacing $1-U$ with $U$).\n\n#### Beta distribution\n\nThe beta distribution is a commonly used distribution **when seeking a parameterized shape over a finite support**.\n\nThe PDF is:\n\n$$\nf(x) = \\frac{x^{\\alpha - 1} (1-x)^{\\beta -1}}{B(\\alpha, \\beta)}\n$$\n\nfor $x \\in [0, 1]$. Both $\\alpha$ and $\\beta$ are shape parameters.\n\n```{julia}\nusing Distributions, CairoMakie\n\nx = 0:0.01:1\n\nfig, ax = lines(x, pdf.(Beta(2, 2), x), label=\"α = β = 2\")\nlines!(ax, x, pdf.(Beta(1, 1), x), label=\"α = β = 1\")  # U(0, 1)\naxislegend(ax)\nfig\n```\n\n**Note:** you can use **mathematical special functions** like beta or gamma function calling `beta` or `gamma` provided by the `SpecialFunctions` package. In addition, `QuadGK` provides the `quadgk` function to integrate one-dimensional function.\n\n#### Weibull distribution\n\nFor a random variable $T$, representing **the lifetime of an individual or a component**, an interesting quantity is **the instantaneous chance of failure** at any time, given that the component has been operating **without failure** up to time $x$.\n\nThe instantaneous chance of failure at time $x$ can be expressed as\n\n$$\nh(x) = \\lim_{\\Delta \\to 0} \\frac{1}{\\Delta} P(T \\in [x, x+\\Delta] | T \\gt x)\n$$\n\nAlternatively, by using the conditional probability ($P(T \\in [x, x+\\Delta] | T \\gt x) = \\frac{P(T \\in [x, x+\\Delta])}{P(T \\gt x)} = \\frac{P(T \\in [x, x+\\Delta])}{1 - P(X \\le x)}$) and noticing that the PDF $f(x)$ satisfies $f(x)\\Delta \\approx P(x \\le T \\lt x + \\Delta)$ for small $\\Delta$, we can express $h(x)$ as\n\n$$\nh(x) = \\lim_{\\Delta \\to 0} \\frac{f(x)\\Delta}{\\Delta (1-F(x))} = \\frac{f(x)}{1-F(x)}\n$$\n\nHere the function $h(\\cdot)$ is called **the hazard rate function**, which is often used in **reliability analysis** and **survival analysis**. It's a common method of viewing the distribution for lifetime random variables $T$.\n\nIn fact, we can reconstruct the CDF of $T$ as\n\n$$\nF(x) = 1 - \\exp(-\\int_0^x h(t)\\mathrm{d}t)\n$$\n\nThe **Weibull distribution** is defined through the hazard rate function of the form $h(x) = \\lambda x^{\\alpha - 1}$. Where $\\lambda$ is **positive**, and $\\alpha$ takes on **any real value**.\n\nThe parameter $\\alpha$ gives the Weibull distribution different modes of behavior:\n\n- $\\alpha = 1$: the hazard rate is constant, in which case the Weibull distribution is actually an exponential distribution with rate $\\lambda$.\n\n- $\\alpha > 1$: the hazard rate increases over time. This depicts a situation of \"aging components\", i.e., the longer a components has lived, the higher the instantaneous chance of failure. This is sometimes called **Increasing Failure Rate (IFR)**.\n\n- $\\alpha < 1$: this is an opposite case against $\\alpha > 1$. This is sometimes called **Decreasing Failure Rate (DFR)**.\n\nFor Weibull distribution, we have\n\n$$\nF(x) = 1 - \\exp(-\\int_0^x h(t)\\mathrm{d}t)\n$$\n\nand\n\n$$\nh(x) = \\lambda x^{\\alpha - 1}\n$$\n\nThen the CDF and PDF are\n\n$$\nF(x) = 1 - e^{-\\frac{\\lambda}{\\alpha} x^\\alpha}\n$$\n\nand\n\n$$\nf(x) = \\lambda x^{\\alpha - 1} e^{-\\frac{\\lambda}{\\alpha} x^\\alpha}\n$$\n\nNote that in Julia, the distribution is parameterized via\n\n$$\nf(x) = \\frac{\\alpha}{\\theta} (\\frac{x}{\\theta})^{\\alpha - 1} e^{-(\\frac{x}{\\theta})^\\alpha} = \\alpha \\theta ^{-\\alpha} x^{\\alpha - 1} e^{-\\theta ^{-\\alpha} x^\\alpha}\n$$\n\nwhere the bijection from $\\lambda$ to $\\theta$ is\n\n$$\n\\lambda = \\alpha \\theta ^{-\\alpha}\n$$\n\nand\n\n$$\n\\theta = (\\frac{\\alpha}{\\lambda})^{\\frac{1}{\\alpha}}\n$$\n\nIn this case, $\\theta$ is called the **scale** parameter, and $\\alpha$ is the **shape** parameter.\n\n```{julia}\nusing Distributions, CairoMakie\n\nalphas = [0.5, 1, 1.5]\ngiven_lambda = 2\nx = 0.01:0.01:10\ncolors = [:blue, :red, :green]\n\nactual_lambda(dist::Weibull) = shape(dist) * Distributions.scale(dist)^(-shape(dist))\ntheta(lambda, alpha) = (alpha / lambda)^(1 / alpha)\n\nwb_dists = [Weibull(alpha, theta(given_lambda, alpha)) for alpha in alphas]\n\nhazardA(dist, x) = pdf(dist, x) / ccdf(dist, x)\nhazardB(dist, x) = actual_lambda(dist) * x^(shape(dist) - 1)\n\n# We usually use the hazard rate function to view the Weibull distribution\nhazardsA = [hazardA.(dist, x) for dist in wb_dists]\nhazardsB = [hazardB.(dist, x) for dist in wb_dists]\n\nprintln(\"Maximum difference between two implementations of hazard: \",\n    maximum(maximum.(hazardsA - hazardsB)))\n\nfig = Figure()\nax = Axis(fig[1, 1],\n    xlabel=\"x\",\n    ylabel=\"Instantaneous failure rate\")\nfor i in 1:length(hazardsA)\n    label = string(\"λ = \", round(actual_lambda(wb_dists[i]), digits=2), \", α = \", round(shape(wb_dists[i]), digits=2))\n    lines!(ax, x, hazardsA[i], color=colors[i], label=label)\nend\nxlims!(ax, 0, 10)\nylims!(ax, 0, 10)\naxislegend(ax)\nfig\n```\n\n#### Normal distribution\n\nThe normal distribution (also known as Gaussian distribution) is defined by two parameters, $\\mu$ and $\\sigma ^2$, which are the mean and variance respectively.\n\nThe PDF is\n\n$$\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma ^2}}\n$$\n\nThe normal distribution usually comes with the standard form with $\\mu = 0$ and $\\sigma ^2 = 1$. The PDF is\n\n$$\nf(u) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{u^2}{2}}\n$$\n\nThe CDF of the standard normal distribution (the CDF of the normal distribution is not available as a simple expression) is\n\n$$\n\\Phi (u) = \\int_{-\\infty}^u \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{t^2}{2}} \\mathrm{d}t = \\frac{1}{2} (1 + \\mathrm{erf}(\\frac{x}{\\sqrt{2}})\n$$\n\nwhere $\\mathrm{erf}(\\cdot)$ is a mathematical special function, called **error function**, and defined as\n\n$$\n\\mathrm{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_0^x e^{-t^2} \\mathrm{d}t\n$$\n\nFor any general normal random variable with mean $\\mu$ and variance $\\sigma ^2$, the CDF is available via $\\Phi(\\frac{x-\\mu}{\\sigma})$.\n\n```{julia}\nusing Distributions, Calculus, SpecialFunctions, CairoMakie\n\nx = -5:0.01:5\n\n# erf function from the SpecialFunctions package\nphiA(x) = 0.5 * (1 + erf(x / sqrt(2)))  # Calculate Φ(u) using the error function\nphiB(x) = cdf(Normal(), x)  # Calculate Φ(u)\n\nprintln(\"Maximum difference between two CDF implementations: \",\n    maximum(phiA.(x) - phiB.(x)))\n\nnormalDensity(x) = pdf(Normal(), x)\n\n# Calculate the numerical derivatives from the Calculus package\nd0 = normalDensity.(x)\nd1 = derivative.(normalDensity, x)  # We'll know that x = 0 is the unique extremum\nd2 = second_derivative.(normalDensity, x)  # We'll know x=±1 are two inflection points\n\nfig, ax = lines(x, d0, color=:red, label=\"f(x)\")\nlines!(x, d1, color=:blue, label=\"f'(x)\")\nlines!(x, d2, color=:green, label=\"f''(x)\")\naxislegend(ax)\nfig\n```\n\n#### Rayleigh distribution\n\nConsider an exponentially distributed random variable $X$, with rate parameter $\\lambda = \\frac{\\sigma ^{-2}}{2}$, where $\\sigma > 0$.\n\nLet $R = \\sqrt{X}$, and then we have\n\n$$\nF_R(y) = P(R \\le y) = P(\\sqrt{X} \\le y) = P(X \\le y^2) = F_X(y^2) = 1 - exp(-\\frac{y^2}{2\\sigma ^2})\n$$\n\nand by differentiating, we get\n\n$$\nf_R(y) = \\frac{y}{\\sigma ^2} exp(-\\frac{y^2}{2\\sigma ^2})\n$$\n\nwhich is called the PDF of **Rayleigh distribution**.\n\nThe mean of a Rayleigh random variable is $\\sigma \\sqrt{\\frac{\\pi}{2}}$.\n\nAs mentioned before, we have $U \\sim U(0, 1) \\xrightarrow{X = -\\frac{1}{\\lambda}\\log U} X \\sim Exp(\\lambda) \\xrightarrow{R=\\sqrt{X}, \\lambda = \\frac{\\sigma ^{-2}}{2}} R \\sim Rl(\\sigma)$\n\nIn addition, if $N_1$ and $N_2$ are two i.i.d. **normally distributed** random variables, each with $\\mu = 0$ and std. $\\sigma$, then $\\tilde{R} = \\sqrt{N_1^2 + N_2^2}$ is also Rayleigh distributed just as $R$ above.\n\nTherefore, we have three ways to generate Rayleigh distributed random variables:\n\n```{julia}\nusing Distributions, CairoMakie\n\nN = 10^6\nsigma = 1.5\n\n# U(0, 1) ⟶ Exp(λ) ⟶ Rl(σ)\nrlA = sqrt.(-(2 * sigma^2) * log.(rand(N)))\n\n# From two i.i.d. normally distributed random variables\nnormal_dist = Normal(0, sigma)\nrlB = sqrt.(rand(normal_dist, N) .^ 2 + rand(normal_dist, N) .^ 2)\n\nrlC = rand(Rayleigh(sigma), N)\n\nmean.([rlA, rlB, rlC, sigma * sqrt(π / 2)])\n```\n\nA common way to generate **normal random variables**, called the **Box-Muller Transform**, is to use the relationship $R = \\sqrt{N_1^2 + N_2^2}$.\n\nThe relationship between the pair $(N_1, N_2)$ and their polar coordinate counterpart $(\\theta, R)$ is\n\n$$\n\\begin{cases}\n   N_1 = R\\cos(\\theta) \\\\\n   N_2 = R\\sin(\\theta)\n\\end{cases}\n$$\n\nwhere $\\theta$ is a **uniformly distributed** random variable on $[0, 2\\pi]$, and $R$ is a **Rayleigh distributed** random variable with parameter $\\sigma$.\n\nGiven this, we can first generate $\\theta$ and $R$, and then transform them via the above formula into $N_1$ and $N_2$. Often, $N_2$ is not needed. Hence, in practice, given two independent uniform $(0, 1)$ random variables $U_1$ and $U_2$, we set $Z = \\sqrt{-2\\sigma ^2 \\log U_1} \\cdot \\cos(2\\pi U_2)$. Here $Z$ is a normally distributed random variable with $\\mu = 0$ and std. $\\sigma$.\n\nGenerate $N(0, 1)$:\n\n```{julia}\nusing Distributions, CairoMakie\n\nZ(sigma) = sqrt(-2 * sigma * log(rand())) * cos(2 * pi * rand())\n\nfig, ax = hist([Z(1) for _ in 1:10^6], bins=50,\n    normalization=:pdf, label=\"MC estimate\")\nlines!(-4:0.01:4, pdf.(Normal(), -4:0.01:4),\n    label=\"PDF\", color=:red, linewidth=3)\naxislegend(ax)\nfig\n```\n\n#### Cauchy distribution\n\nThe PDF is\n\n$$\nf(x) = \\frac{1}{\\pi \\gamma (1 + (\\frac{x - x_0}{\\gamma})^2)}\n$$\n\nwhere $x_0$ is the location parameter at which the peak is observed, and $\\gamma$ is the scale parameter.\n\n**Note:** the mean and variance are undefined for Cauchy distribution.\n\n#### Summary\n\n![A brief summay of univariate distributions](./figures/Summary_of_Univariate_Distributions.jpg){.lightbox fig-alt=\"Click to see a larger version of the image\" fig-align=\"center\"}\n\n## Multivariate distributions\n\nConsider $\\mathbf{X} = (X_1, ..., X_n)$ as a random vector with multiple random variables, defined in the same probability space.\n\n### Covarianve and correlation coefficient\n\nCovariance: $Cov(X, Y) = E[(X-\\mu_X)(Y-\\mu_Y)] = E[XY] - \\mu_X \\mu_Y$.\n\nCorrelation coefficient: $\\rho_{XY} = \\frac{Cov(X, Y)}{\\sigma_X \\sigma_Y}$, where $-1 \\le \\rho_{XY} \\le 1$.\n\nThe correlation coefficient is a normalized covariance standing for the **linear** correlation relationship between $X$ and $Y$.\n\n### Expectation vector and covariance matrix\n\nConsider a random vector $X = [X_1, ..., X_n]^\\top$:\n\nThe expectation vector is defined as\n\n$$\n\\mu_{\\mathbf{X}} = [E(X_1), ..., E(X_n)]^\\top\n$$\n\nThe covariance matrix is defined as\n\n$$\n\\Sigma_\\mathbf{X} = Cov(\\mathbf{X}) = E[(\\mathbf{X} - \\mu_\\mathbf{X})(\\mathbf{X} - \\mu_\\mathbf{X})^\\top]\n$$\n\nAs can be verified, the $i,j$-th element of $\\Sigma_\\mathbf{X}$ is $Cov(\\mathbf{X}_i, \\mathbf{X}_j)$, and hence the diagonal elements are the variances.\n\n### Affine transformation\n\nFor any collection of random variables,\n\n$$\nE[X_1+ ... + X_n] = E[X_1] + ... + E[X_n]\n$$\n\nFor **uncorrelated** random variables,\n\n$$\nVar(X_1 + ... + X_n) = \\sum_{i} Var(X_i)\n$$\n\nMore generally, if we allow the random variables to be correlated, then\n\n$$\nVar(X_1 + ... + X_n) = \\sum_{i} Var(X_i) + 2\\sum_{i < j} Cov(X_i, X_j)\n$$\n\nObviously, the right-hand side is the sum of the elements of the matrix $Cov(\\mathbf{X})$.\n\nThe above is a special case of the affine transformation, where we take a random vector $\\mathbf{X} = [X_1, ..., X_n]^\\top$ with covariance matrix $\\Sigma_\\mathbf{X}$, and an $m \\times n$ matrix $\\mathbf{A}$ and $m$ vector $\\mathbf{b}$. We then set\n\n$$\n\\mathbf{Y} = \\mathbf{A}\\mathbf{X} + \\mathbf{b}\n$$\n\nThen, the new random vector $\\mathbf{Y}$ has expectation and covariance\n\n$$\nE[\\mathbf{Y}] = \\mathbf{A}E[\\mathbf{X}] + \\mathbf{b}\\ \\ \\ \\ \\text{and}\\ \\ \\ \\ Cov(\\mathbf{Y}) = \\mathbf{A}\\Sigma_\\mathbf{X}\\mathbf{A}^\\top\n$$\n\nThe above case can be retrieved by setting $\\mathbf{A} = [1, ..., 1]$, and $\\mathbf{b} = \\mathbf{0}$.\n\n### The Cholesky decomposition and generating random vectors\n\nNow we want to create an $n$-dimensional random vector $\\mathbf{Y}$ with some specified expectation vector $\\mu_\\mathbf{Y}$ and covariance matrix $\\Sigma_\\mathbf{Y}$, which are known.\n\nFirst, we can generate a random vector $\\mathbf{X}$ with $\\mu_\\mathbf{X} = \\mathbf{0}$ and identity-covariance matrix $\\Sigma_\\mathbf{X} = \\mathbf{I}$ (e.g., a sequence of $n$ i.i.d. N(0, 1) random variables).\n\nThen, by applying the affine transformation $\\mathbf{Y} = \\mathbf{A}\\mathbf{X} + \\mathbf{b}$, we have $\\mu_\\mathbf{Y} = \\mathbf{b}$ and a matrix $\\mathbf{A}$ which satisfies $\\Sigma_\\mathbf{Y} = \\mathbf{A}\\mathbf{A}^\\top$. The Cholesky decomposition will help us get $\\mathbf{A}$ from $\\Sigma_\\mathbf{Y} = \\mathbf{A}\\mathbf{A}^\\top$.\n\n```{julia}\nusing Distributions, LinearAlgebra, Random, CairoMakie\n\nRandom.seed!(1)\n\nN = 10^5\n\nmuY = [15; 20]\nSigY = [6 4; 4 9]\n\nA = cholesky(SigY).L  # The Cholesky decomposition; get the lower triangular form\n\nrngGens = [() -> rand(Normal()),\n    () -> rand(Uniform(-sqrt(3), sqrt(3))),\n    () -> rand(Exponential()) - 1]  # Expectation 0; variance 1\n\nlabels = [\"Normal\", \"Uniform\", \"Exponential\"]\ncolors = [:blue, :red, :green]\n\nrv(rng) = A * [rng(), rng()] + muY\n\nds = [[rv(rng) for _ in 1:N] for rng in rngGens]\n\nprintln(\"E1\\tE2\\tVar1\\tVar2\\tCov\")\nfor d in ds\n    println(round(mean(first.(d)), digits=2), \"\\t\", round(mean(last.(d)), digits=2), \"\\t\",\n        round(var(first.(d)), digits=2), \"\\t\", round(var(last.(d)), digits=2), \"\\t\",\n        round(cov(first.(d), last.(d)), digits=2))\nend\n\nfig = Figure()\nax = Axis(fig[1, 1],\n    xlabel=L\"X_1\",\n    ylabel=L\"X_2\")\nfor i in 1:length(ds)\n    scatter!(ax, first.(ds[i]), last.(ds[i]), color=colors[i], label=labels[i], markersize=2)\nend\naxislegend(ax, position=:rb)\nfig\n```\n\n### Bivariate normal distribution\n\n$$\n\\mu_\\mathbf{XY} = \\left[\\begin{matrix} \\mu_\\mathbf{X} \\\\ \\mu_\\mathbf{Y} \\end{matrix}\\right]\n$$\n\n$$\n\\Sigma_\\mathbf{XY} = \\left[\\begin{matrix} \\sigma_\\mathbf{X}^2 & \\sigma_\\mathbf{X}\\sigma_\\mathbf{Y}\\rho \\\\ \\sigma_\\mathbf{X}\\sigma_\\mathbf{Y}\\rho & \\sigma_\\mathbf{Y}^2\\end{matrix} \\right]\n$$\n\n```{julia}\nusing Distributions, CairoMakie\n\nmeanVect = [27.1554, 26.1638]\ncovMat = [16.1254 13.047; 13.047 12.3673]\n\nbiNorm = MvNormal(meanVect, covMat)  # Multivariate normal distribution\n\nN = 10^3\npoints = rand(biNorm, N)\n\nsupport = 15:0.5:40\nz = [pdf(biNorm, [x, y]) for x in support, y in support]\n\nfig = Figure(size=(900, 400))\nax2 = Axis(fig[1, 1],\n    xlabel=\"x\",\n    ylabel=\"y\")\nscatter!(ax2, points[1, :], points[2, :], markersize=4, color=:black)\ncontour!(support, support, z, levels=[0.001, 0.005, 0.02], color=:red, linewidth=2)\nax3 = Axis3(fig[1, 2],\n    xlabel=\"x\",\n    ylabel=\"y\",\n    zlabel=\"z\")\nsurface!(support, support, z)\ncolsize!(fig.layout, 1, Auto(0.65))\nfig\n```\n\n## Processing and summarizing data\n\n### Processing data\n\nData cleaning.\n\n### Summarizing data\n\nDescriptive statistics.\n\n#### Single sample\n\nGiven a set of observations $x_1, x_2, ..., x_n$.\n\n1. **Sample mean**: measure of centrality.\n\n- Arithmetic mean:\n\n$$\n\\bar{x} = \\frac{\\displaystyle\\sum_{i=1}^{n} x_i}{n}\n$$\n\n- Geometric mean:\n\n$$\n\\bar{x}_g = \\sqrt[n]{\\displaystyle\\prod_{i=1}^n x_i}\n$$\n\nUseful for **averaging growth factors**.\n\ne.g., if we start with an original base level say $L$ with growths of $x_1$, $x_2$, and $x_3$ in three consecutive periods, then after three periods, we have\n\n$$\n\\text{Value after three periods} = L\\cdot x_1\\cdot x_2\\cdot x_3 = L\\cdot \\bar{x}_g^3\n$$\n\nHere, the average growth factor is $\\bar{x}_g$.\n\n- Harmonic mean:\n\n$$\n\\bar{x}_h = \\frac{n}{\\displaystyle\\sum_{i=1}^n \\frac{1}{x_i}}\n$$\n\nUseful for **averaging rates or speeds**.\n\nAssume that you are on a brisk hike, walking $5$ km up a mountain and then $5$ km back down.\n\nSay your speed going up is $x_1 = 5 \\text{ km/h}$, and your speed going down is $x_2 = 10 \\text{ km/h}$.\n\nYou travel up for $1$ h, and down for $0.5$ h and hence your total travel time is $1.5$ h.\n\nWhat is your average speed for the whole journey?\n\nThe avearge speed shoud be $\\frac{10 \\text{ km}}{1.5 \\text{ h}} = 6.6\\bar{6} \\text{ km/h}$.\n\nThis is not the arithmetic mean which is $\\frac{x_1 + x_2}{2} = \\frac{5 \\text{ km/h}+ 10 \\text{ km/h}}{2} = 7.5 \\text{ km/h}$ but rather equals the harmonic mean.\n\n2. Variance: a measure of dispersion.\n\n- Sample variance: the dispersion degree of sample observations away from the sample mean.\n\n$$\ns^2 = \\frac{\\displaystyle\\sum_{i=1}^n (x_i - \\bar{x})^2}{n-1} = \\frac{\\displaystyle\\sum_{i=1}^n x_i^2 - n\\bar{x}^2}{n-1}\n$$\n\nNote that the denominator is $n-1$ instead of $n$, which is the population variance ($s^2$ defined in the above way is an unbiased estimator of the population variance).\n\n- Sample standard deviation: $s = \\sqrt{s^2}$.\n\n- Standard error: $\\frac{s}{\\sqrt{n}}$ (the dispersion degree of sample mean away from the population mean).\n\nAnother breed of descriptive statistics is based on order statistics. This term is used to describe the sorted sample, denoted by\n\n$x_{(1)} \\le x_{(2)} \\le ... \\le x_{(n)}$\n\nBased on the order statistics, we can define a variety of statistics.\n\n1. minimum: $x_{(1)}$.\n\n2. maximum: $x_{(n)}$.\n\n3. median: which in case of $n$ being odd is $x_{(\\frac{n+1}{2})}$; in case of $n$ being even is the arithmetic mean of $x_{(\\frac{n}{2})}$ and $x_{(\\frac{n}{2} + 1)}$. A measure of centrality. It is not influenced by very high or very low measurements.\n\n4. $\\alpha$-quantile: which is $x_{(\\widetilde{\\alpha n})}$, where $\\widetilde{\\alpha n}$ denotes a rounding of $\\alpha n$ to the nearest element of $\\{1, ..., n\\}$.\n\n$\\alpha = 0.25$ and $\\alpha = 0.75$ is called the first quantile and the third quantile, the difference of which is called the inter-quantile range (IQR), which is a measure of dispersion.\n\n5. range: $x_{(n)} - x_{(1)}$, which is also a measure of dispersion.\n\nA measure of centrality: mean (arithmetic mean, geometric mean, harmonic mean), median (i.e., $0.5$-quantile).\n\nA measure of dispersion: variance (sample variance, sample standard deviation, standard error), IQR, range.\n\nIn Julia, packages `Statistics` together with `StatsBase` is usually used to perform descriptive statistics:\n\n```{julia}\nusing Statistics, StatsBase, Distributions\n\nd = rand(Exponential(1 / 2), 10^6)\n\nprintln(\"Sample arithmetic mean, sample geometric mean, sample harmonic mean: \", (mean(d), geomean(d), harmmean(d)))\nprintln(\"Sample variance, sample standard deviation, sample standard error: \", (var(d), std(d), sem(d)))\nprintln(\"Minimum, maximum, range: \", (minimum(d), maximum(d), maximum(d) - minimum(d)))\nprintln(\"95th percentile, 0.95 quantile, IQR: \", (percentile(d, 95), quantile(d, 0.95), iqr(d)), \"\\n\")\n\nsummarystats(d)\n```\n\n#### Observations in pairs\n\nWhen data is configured in the form of pairs, $(x_1, y_1), ..., (x_n, y_n)$, we often consider the (1) **sample covariance**\n\n$$\n\\widehat{cov}_{x,y} = \\frac{\\displaystyle\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{n-1}\n$$\n\nor its normalized form - (2) **correlation coefficient** (Pearson correlation coefficient)\n\n$$\n\\hat{\\rho}_{x,y} = \\frac{\\widehat{cov}_{x,y}}{s_x s_y}\n$$\n\nWe often represent the variances and covariances in the (3) **sample covariance matrix**\n\n$$\n\\hat{\\Sigma} = \\left[ \\begin{matrix} \\widehat{cov}_{x,x} & \\widehat{cov}_{x,y} \\\\ \\widehat{cov}_{x,y} & \\widehat{cov}_{y,y} \\end{matrix} \\right] = \\left[ \\begin{matrix} s_x^2 & \\hat{\\rho}_{x,y} s_x s_y \\\\ \\hat{\\rho}_{x,y} s_x s_y & s_y^2 \\end{matrix} \\right]\n$$\n\n```{julia}\nusing CSV, DataFrames, Statistics\n\nd = CSV.read(\"./data/temperatures.csv\", DataFrame)\n\nx = d.Brisbane\ny = d.GoldCoast\n\ncovXY = cov(x, y)\nsigX = std(x)\nsigY = std(y)\nrhoXY = covXY / (sigX * sigY)\n\nprintln(\"covXY: \", covXY, \"\\n\",\n    \"sigX: \", sigX, \"\\n\",\n    \"sigY: \", sigY, \"\\n\",\n    \"rhoXY: \", rhoXY)\n\nmeanVect = [mean(x), mean(y)]\ncovMat = [sigX^2 covXY\n    covXY sigY^2]\n\nprintln(\"meanVect: \", meanVect)\nprintln(\"covMat: \", covMat)\n```\n\n#### Observations in vectors\n\nThe data is represented by an $n\\times p$ matrix, $\\mathbf{X}$, where **the rows are observations** and **the columns are features**.\n\n$$\n\\mathbf{X} = [\\mathbf{X_1}, ..., \\mathbf{X_p}]\n$$\n\n$\\mathbf{X_j}$ represents the $j$-th feature.\n\nBasically, we can summarize the data matrix $\\mathbf{X}$ by these statistics:\n\n- Sample mean vector\n\n$$\n\\bar{\\mathbf{x}} = [\\bar{x}_1, ..., \\bar{x}_p]^\\top\n$$\n\n- Sample standard deviation vector\n\n$$\n\\mathbf{s} = [s_1, ..., s_p]^\\top\n$$\n\nWith these two statistics, we often standardize or normalize the data by creating a new $n\\times p$ matrix $\\mathbf{Z}$ with entries\n\n$$\nz_{ij} = \\frac{x_{ij} - \\bar{x}_j}{s_j}, i = 1, ..., n,\\ \\ j = 1, ..., p\n$$\n\nalso called z-scores.\n\nThe normalized data has the attribute that each column has a $0$ sample mean and a unit standard deviation. Hence the first- and second-order information of the $j$-th feature is lost when moving from $\\mathbf{X}$ to $\\mathbf{Z}$.\n\nIt can be created via\n\n$$\n\\mathbf{Z} = (\\mathbf{X} - \\mathbf{1}\\mathbf{\\bar{x}}^\\top)diag(\\mathbf{s})^{-1}\n$$\n\nwhere $diag(\\cdot)$ creates a diagonal matrix from a vector by using the `Diagonal` function, and then get the inverse matrix by using the grammar `D^-1`, both of which are from the `LinearAlgebra` package.\n\nIn Julia this can be calculated using the `zscore` function.\n\n- Sample covariance matrix\n\n$$\n\\begin{align}\n\\hat{\\Sigma} & = \\frac{1}{n-1} (\\mathbf{X} - \\mathbf{1}\\mathbf{\\bar{x}}^\\top)^\\top (\\mathbf{X} - \\mathbf{1}\\mathbf{\\bar{x}}^\\top) \\\\\n             & = \\frac{1}{n-1} \\mathbf{X}^\\top (\\mathbf{I} - n^{-1} \\mathbf{1} \\mathbf{1}^\\top) \\mathbf{X}\n\\end{align}\n$$\n\n- Sample correlation matrix\n\nThe following only picks two columns called $x$ and $y$ to perform deduction:\n\n$$\n\\begin{align}\n\\hat{\\rho}_{x,y} & = \\frac{\\widehat{cov}_{x,y}}{s_x s_y} \\\\\n                 & = \\frac{\\displaystyle\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{(n-1) s_x s_y} \\\\\n                 & = \\frac{1}{n-1} \\displaystyle\\sum_{i=1}^n \\frac{x_i - \\bar{x}}{s_x} \\cdot \\frac{y_i - \\bar{y}}{s_y} \\\\\n                 & = \\frac{1}{n-1} \\displaystyle\\sum_{i=1}^n z_{ix} z_{iy} \\\\\n                 & = \\frac{1}{n-1} \\mathbf{Z}^\\top \\mathbf{Z}\n\\end{align}\n$$\n\nIn julia this can be performed via the `cor` function.\n\n```{julia}\nusing Statistics, StatsBase, LinearAlgebra, DataFrames, CSV\n\ndf = CSV.read(\"./data/3featureData.csv\", DataFrame, header=false)\nprintln(df, \"\\n\")\n\nX = Matrix(df)\nprintln(X, \"\\n\")\n\nn, p = size(X)\n\n# Sample mean vector\nxbarA = X' * ones(n) / n\nxbarB = [mean(X[:, j]) for j in 1:p]\nxbarC = sum(X, dims=1) / n\nprintln(\"Sample mean vector: \", \"\\n\", xbarA, \"\\n\", xbarB, \"\\n\", xbarC, \"\\n\")\n\n# Sample standard deviation vector\nsA = [std(X[:, j]) for j in 1:p]\nsB = std(X, dims=1)\nprintln(\"Sample standard deviation vector: \", \"\\n\", sA, \"\\n\", sB, \"\\n\")\n\nxbar = xbarB\ns = sA\n\n# Z-scores matrix\nZA = [((X[i, j] - mean(X[:, j])) / std(X[:, j])) for i in 1:n, j in 1:p]\nZB = (X - ones(n) * xbar') * Diagonal(s)^-1\nZC = hcat([zscore(X[:, j]) for j in 1:p]...)\nprintln(\"Z-scores matrix: \", \"\\n\", ZA, \"\\n\", ZB, \"\\n\", ZC, \"\\n\")\n\n# Sample covariance matrix\ncovA = (X - ones(n) * xbar')' * (X - ones(n) * xbar') / (n - 1)\ncovB = X' * (I - ones(n, n) / n) * X / (n - 1)\ncovC = [cov(X[:, i], X[:, j]) for i in 1:p, j in 1:p]\ncovD = [cor(X[:, i], X[:, j]) * std(X[:, i]) * std(X[:, j]) for i in 1:p, j in 1:p]\ncovE = cov(X)\nprintln(\"Sample covariance matrix: \", \"\\n\", covA, \"\\n\", covB, \"\\n\", covC, \"\\n\", covD, \"\\n\", covE, \"\\n\")\n\nZMat = ZC\n\n# Sample correlation coefficient matrix\ncorA = cov(X) ./ [std(X[:, i]) * std(X[:, j]) for i in 1:p, j in 1:p]\ncorB = cov(X) ./ (std(X, dims=1)' * std(X, dims=1))\ncorC = [cor(X[:, i], X[:, j]) for i in 1:p, j in 1:p]\ncorD = ZMat' * ZMat / (n - 1)\ncorE = cov(ZMat)\ncorF = cor(X)\nprintln(\"Sample correlation coefficient matrix: \", \"\\n\", corA, \"\\n\", corB, \"\\n\", corC, \"\\n\", corD, \"\\n\", corE, \"\\n\", corF, \"\\n\")\n```\n\n### Plots for single samples and time series\n\nHere, we focus on a single collection of observations, $x_1, ..., x_n$.\n\nIf the observations are obtained by **randomly sampling a population**, then **the order of the observations is inconsequential**.\n\nIf the observations represent measurement **over time** then we call the data **time-series**, and in this case, plotting the observations one after another is the standard way for considering temporal patterns in the data.\n\n#### Histograms\n\nConsidering frequencies of occurrences.\n\nFirst denote the support of the observations via $[l, m]$, where $l$ is the minimal observation and $m$ is the maximal observation.\n\nThen the interval $[l, m]$ is partitioned into a finite set of bins $B_1, ..., B_L$, and the frequency in each bin is recorded via\n\n$$\nf_j = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}{\\{x_i \\in B_j}\\}, \\ \\ \\ \\ \\text{for}\\ j = 1, ..., L\n$$\n\nHere $\\mathbf{1}\\{\\cdot\\}$ is $1$ for $x_i \\in \\B_j$, or $0$ if not.\n\nWe have that $\\sum f_j = 1$, and hence $f_i, ..., f_L$ is a PMF.\n\nA histogram is then just a visual representation of PMF. One way to plot the frequencies is via a **stem plot**. However, such a plot would not represent **the widths of the bins**. Instead we plot $h(x)$ defined as\n\n$$\nh(x) = \\sum_{j=1}^L \\frac{f_j}{|B_j|} \\mathbf{1}{\\{x_i \\in B_j}\\}\n$$\n\nwhere $|B_j|$ is the width of bin $j$. Hence $h(x)$ is actually a PDF.\n\nIn a word, calculate frequencies of occurrences in each bin $\\implies$ PMF; further normalized by bin widths $\\implies$ PDF.\n\n```{julia}\nusing Distributions, CairoMakie\n\nn = 2000\nd = rand(Normal(), n)\n\n# PMF\nfig, ax = hist(d, bins=20, normalization=:probability, color=:purple, label=\"PMF\")\n# PDF\nstephist!(ax, d, bins=20, normalization=:pdf, color=:red, label=\"PDF\")\naxislegend(ax)\nfig\n```\n\n#### Density plots and kernel density estimation\n\nA more modern and visually applealing alternative to histograms is the **smoothed histogram**, also known as a **density plot**, often generated via a **kernel density estimate**.\n\n1. Mixture model\n\nGenerating observations from a **mixture model** means that we sample from populations made up of heterogeneous sub-populations.\n\nEach sub-population has its own probability distribution and these are \"mixed\" in the process of sampling.\n\nAt first, a latent (un-observed) random variable determines which sub-population is used, and then a sample is taken from that sub-population.\n\nThat is if the $M$ sub-populations have densities $g_1(x), ..., g_M(x)$ with weights $p_1, ..., p_M$, and $\\sum p_i = 1$, then the density of the mixture is\n\n$$\nf(x) = \\sum_{i=1}^M p_i g_i(x)\n$$\n\n2. Kernel density estimate\n\nGiven a set of observations, $x_1, ..., x_n$, the KDE is the function\n\n$$\n\\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h} K(\\frac{x-x_i}{h})\n$$\n\nwhere $K(\\cdot)$ is some specified kernel function and $h > 0$ is the bandwidth parameter.\n\nThe kernel function is a function that satisfies the properties of a PDF. A typical example is the Gaussian kernel\n\n$$\nK(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}\n$$\n\nWith such a kernel, the estimate $\\hat{f}(x)$ is a PDF because it is a weighted superposition of scaled kernel fucntions centered about each of the observations.\n\nA very small bandwidth implies that the density\n\n$$\n\\frac{1}{h} K(\\frac{x-x_i}{h})\n$$\n\nis very concentrated around $x_i$.\n\nFor any value of $h$, it can be proved under general conditions that if the data is distributed according to some density $f(\\cdot)$, then $\\hat{f}(\\cdot)$ converges to $f(\\cdot)$ when the sample size grows.\n\n```{julia}\nusing Distributions, CairoMakie\n\nmu1, sigma1 = 10, 5\nmu2, sigma2 = 40, 12\n\ndist1, dist2 = Normal(mu1, sigma1), Normal(mu2, sigma2)\nmixRV(p) = (rand() <= p) ? rand(dist1) : rand(dist2)\n\nn = 2000\nd = [mixRV(0.3) for _ in 1:n]\n\n# PMF\nfig, ax = hist(d, bins=20, normalization=:probability, color=:skyblue, label=\"PMF\")\n# PDF\nstephist!(ax, d, bins=20, normalization=:pdf, color=:red, label=\"PDF\")\n# Smoothed PDF\ndensity!(ax, d, color=(:white, 0), label=\"Smoothed PDF\", strokecolor=:green, strokewidth=2)\naxislegend(ax)\nfig\n```\n\nIn a word, the KDE is a useful way to estimate the PDF of the unknown underlying distribution given some sample data.\n\n#### Empirical cumulative distribution function\n\nThe *Empirical Cumulative Distribution Function* (ECDF) can be viewed as an estimate of the underlying CDF.\n\nIn contrast to histograms and KDEs, ECDFs provide an unique representation of the data independent of tuning parameters.\n\nThe ECDF is a stepped function which, given $n$ data points, increases by $\\frac{1}{n}$ at each point.\n\nMathematically, given the sample, $x_1, ..., x_n$, the ECDF is given by\n\n$$\n\\hat{F}(t) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1} \\{x_i \\le t\\}\\ \\ \\ \\ \\text{where }\\mathbf{1}\\text{ is the indicator function}\n$$\n\nIn the case of i.i.d. data from an underlying distribution with CDF $F(\\cdot)$, the Glivenko-Cantelli theorem ensures that the ECDF $\\hat{F}(\\cdot)$ approaches $F(\\cdot)$ as the sample size grows.\n\n```{julia}\nusing Distributions, StatsBase, CairoMakie\n\nmu1, sigma1 = 10, 5\nmu2, sigma2 = 40, 12\n\ndist1, dist2 = Normal(mu1, sigma1), Normal(mu2, sigma2)\n\np = 0.3\n\nmixRV(p) = (rand() <= p) ? rand(dist1) : rand(dist2)\nmixCDF(x) = p * cdf(dist1, x) + (1 - p) * cdf(dist2, x)\n\nn = [30, 100]\n\ndata1 = [mixRV(p) for _ in 1:n[1]]\ndata2 = [mixRV(p) for _ in 1:n[2]]\n\nempiricalCDF1 = ecdf(data1)\nempiricalCDF2 = ecdf(data2)\n\nx = -10:0.1:80\n\nfig, ax = lines(x, empiricalCDF1, label=\"ECDF with n = $(n[1])\")\nlines!(x, empiricalCDF2, label=\"ECDF with n = $(n[2])\")\nlines!(x, mixCDF.(x), label=\"Underlying CDF\")\naxislegend(ax, position=:lt)\nfig\n```\n\n#### Normal probability plot\n\nSee @sec-qqplot for details.\n\n#### Visualizing time series\n\nIn cases where the time-series data appears to be **stationary** (a stationary sequence is one in which the distributional law of observations doesn't depend on the exact time. This means that there isn't an apparent trend nor a cyclic component.), then a histogram is immediately insightful; otherwise, plotting data points one after the other along the time axis is necessary.\n\n```{julia}\nusing DataFrames, CSV, Dates, CairoMakie\n\nd = CSV.read(\"./data/temperatures.csv\", DataFrame)\nbrisbane = d.Brisbane\ngoldcoast = d.GoldCoast\n\ndiff = brisbane - goldcoast\ndates = string.([Date(Year(d.Year[i]),\n    Month(d.Month[i]),\n    Day(d.Day[i]))\n                 for i in 1:nrow(d)])\n\nfortnight_range = 250:263\ndate_fortnight = dates[fortnight_range]\nbris_fortnight = brisbane[fortnight_range]\ngold_fortnight = goldcoast[fortnight_range]\n\nfig = Figure(size=(1100, 900))\n\nax1_slice_indexes = [1, 389, 777]\nax1 = Axis(fig[1, 1],\n    xlabel=\"Time\",\n    ylabel=\"Temperature\",\n    xticks=(ax1_slice_indexes, dates[ax1_slice_indexes]))\nseries!(ax1, stack(zip(brisbane, goldcoast)))\naxislegend(ax1, position=:rb)\n\nax2_slice_indexes = [1, 7, 14]\nax2 = Axis(fig[2, 1],\n    xlabel=\"Time\",\n    ylabel=\"Temperature\",\n    xticks=(ax2_slice_indexes, date_fortnight[ax2_slice_indexes]))\nseries!(ax2, stack(zip(bris_fortnight, gold_fortnight)))\nscatter!(1:length(date_fortnight), bris_fortnight)\nscatter!(1:length(date_fortnight), gold_fortnight)\naxislegend(ax2, position=:lb)\n\nax3_slice_indexes = [1, 389, 777]\nax3 = Axis(fig[3, 1],\n    xlabel=\"Time\",\n    ylabel=\"Temperature Difference\",\n    xticks=(ax3_slice_indexes, dates[ax3_slice_indexes]))\nseries!(ax3, reshape(diff, 1, length(diff)))\n\nax4 = Axis(fig[4, 1],\n    xlabel=\"Temperature Difference\",\n    ylabel=\"Frequency\")\nhist!(ax4, diff, bins=50)\n\nfig\n```\n\n#### Radial plot\n\nRadial plot is useful for presenting **time-series** or **cyclic** data.\n\nA variation of radial plot is the radar plot, which is often used to visualize the levels of different **categorical** variables on the one plot.\n\n```{julia}\nusing DataFrames, CSV, Dates, CairoMakie\n\nd = CSV.read(\"./data/temperatures.csv\", DataFrame)\nsubset!(d, :Year => x -> x .== 2015)\nbrisbane = d.Brisbane\ngoldcoast = d.GoldCoast\n\ndates = [Date(Year(d.Year[i]),\n    Month(d.Month[i]),\n    Day(d.Day[i]))\n         for i in 1:nrow(d)]\n\nx = 0:2pi/(length(brisbane)-1):2pi |> collect\nax_slice_indexes = [findfirst(Dates.month.(dates) .== m) for m in 1:12]\n\nfig = Figure(size=(600, 600))\nax = PolarAxis(fig[1, 1],\n    thetaticks=(x[ax_slice_indexes], Dates.monthabbr.(1:12)))\nseries!(ax, x, [brisbane goldcoast]')\n\nfig\n```\n\n### Plots for comparing two or more samples\n\n#### Quantile-Quantile (Q-Q) plot {#sec-qqplot}\n\nThe Q-Q plot checks if the distributional shape of two samples is the same or not.\n\nFor this plot, we require that the sample sizes are the same.\n\nThen the **ranked** quantiles of the first sample are plotted against the **ranked** quantiles of the second sample.\n\nIn the case where the samples have a similar distributional shape, the resulting plot appears like a collection of increasing points along a straight line.\n\n具体原理解释如下：\n\n给定一列数据 $x_1, ..., x_n$，假定其服从正态分布。现取一个正态分布作为模板，将其 PDF 下的面积等分成 $n$ 份，即每一块区域代表的概率都是相等的，都是 $\\frac{1}{n}$。如果现在要从这个正态分布中抽取一个随机数，在理想情况下，这个数出现在任何一个小区域内的概率都是相等的。也就是说，在该正态分布被分成 $n$ 等份后，如果我们要从其中抽出 $n$ 个随机数，在理想情况下，应该是刚好每个小区间都被抽出了一个数，并且我们预期这些数应该是每个小区间的中位数（二分位数）。\n\n在下图中，$n = 10$：\n\n```{julia}\nusing Distributions, CairoMakie\n\nd = Normal()\n\nn = 10\nx = -4:0.01:4\ny = pdf.(d, x)\nq = quantile.(d, collect(1/n:1/n:(n-1)/n))\n\nfig, ax = lines(x, y, color=:black)\nvlines!(ax, q, color=:red)\nfig\n```\n\n对于 $n = 10$ 来说，累积概率分位数分隔点分别为 $0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9$（$\\frac{1}{n}, \\frac{2}{n}, ..., \\frac{n-1}{n}$），对应的每个小区间的累积概率二分位数应为 $0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95$（$\\frac{i-0.5}{n}\\ \\ \\ \\ \\text{for}\\ i = 1,2, .., n$），再利用公式 $\\Phi^{-1}\\left(\\frac{i-0.5}{n}\\right)\\ \\ \\ \\ \\text{for}\\ i = 1,2, .., n$ 得到每个小区间相应的二分位数值。\n\n在理想情况下，**排完序**的实际观测值 $x_1, ..., x_n$ 应该和上述 $n$ 个二分位数值一致，即以实际值作为纵轴，理论值作为横轴，画出的这些点应该位于斜线 $y = x$ 上。\n\n值得注意的是，取理论分位数值这一步有很多方法，除了等分概率分布取二分位数之外，也有直接将概率分布等分为 $n+1$ 份，直接取对应的 $n$ 个分位数即可。\n\n```{julia}\nusing Random, Distributions, CairoMakie, Statistics\n\nRandom.seed!(1234)\n\nn = 2000\nmu, sigma = 10, 1\nrank = collect(1:2000)\n\nd = Normal(mu, sigma)\n\nempirical_data = rand(d, n) |> sort\ntheoretical_data = quantile.(Normal(), @. (rank - 0.5) / n)\n\n# x = σU + μ\nfig, ax = scatter(theoretical_data, empirical_data, color=:steelblue)\nablines!(ax, mu, sigma, color=:red)\nqqnorm!(Axis(fig[2, 1]), empirical_data, qqline=:fitrobust, color=:red, markercolor=:steelblue)\nfig\n```\n\nIf you want roughly to see if the two given data sets $x_1, ..., x_n$, and $y_1, ..., y_n$ have exactly the same distributional shape, you can just do the following:\n\n```{julia}\nusing Distributions, CairoMakie, Random\n\nRandom.seed!(1)\n\n# Plot the ranked quantiles of x against the ranked quantiles of y\nx = randn(2000) |> sort\ny = randn(2000) |> sort\n\nfig, ax = scatter(x, y, color=:steelblue)\nablines!(0, 1, color=:red)\nqqplot!(Axis(fig[2, 1]), x, y, qqline=:identity, color=:red, markercolor=:steelblue)\nfig\n```\n\n#### Box plot\n\nThe box plot, also known as a box and whisker plot, which displays the **first** and the **third** quantiles along with the **median**. The location of the **whiskers** is typically given by\n\n$$\n\\text{minimum} = Q1 - 1.5 IQR\\ \\ \\text{,}\\ \\ \\text{maximum} = Q3 + 1.5 IQR\n$$\n\nwhere IQR is the inter-quantile range. Observations that lie outside this range are called outliers.\n\n```{julia}\nusing CairoMakie\n\ncategories = repeat(1:3, outer=300)\nv = randn(900)\n\n# notch is used to test the significance of the difference between two medians under the 0.95 confidence interval\nboxplot(categories, v, show_notch=true, color=:cyan)\n```\n\n```{julia}\nusing CairoMakie\n\ncategories = repeat(1:3, inner=800)\ndodge = repeat(repeat(1:2, outer=3), inner=400)\nv = randn(2400)\n\nboxplot(categories, v, dodge=dodge, show_notch=true, color=map(d -> d == 1 ? :cyan : :magenta, dodge))\n```\n\n#### Violin plot\n\nIt is similar to the box plot, however, the shape of each sample is represented by a mirrored kernel density estimate of the data.\n\n```{julia}\nusing CairoMakie\n\ncategories = repeat(1:3, outer=300)\nv = randn(900)\n\nviolin(categories, v, color=:cyan, datalimits=extrema)\n```\n\n```{julia}\nusing CairoMakie\n\ncategories = repeat(1:3, inner=800)\ndodge = repeat(repeat(1:2, outer=3), inner=400)\nv = randn(2400)\n\nviolin(categories, v, dodge=dodge, color=map(d -> d == 1 ? :cyan : :magenta, dodge), datalimits=extrema)\n```\n\n```{julia}\nusing CairoMakie\n\ncategories = repeat(1:3, inner=800)\nside = repeat(repeat([:left, :right], outer=3), inner=400)\nv = randn(2400)\n\nviolin(categories, v, side=side, color=map(d -> d == :left ? :cyan : :magenta, side), datalimits=extrema)\n```\n\n### Plots for multivariate and high-dimensional data\n\nFor vectors of observations, $(x_{11}, ..., x_{1p}), ..., (x_{n1}, ..., x_{np})$, where $n$ is the number of observations and $p$ is the number of variables, or features. In case where $p$ is large the data is called **high dimensional**.\n\n#### Scatter plot matrix\n\nIt consists of taking each possible pair of variables and plotting a scatter plot for that pair.\n\nObviously, with $p$ variables, we need at least $\\frac{p^2-p}{2}$ scatters.\n\n```{julia}\nusing RDatasets, AlgebraOfGraphics, DataFrames, CairoMakie\n\ndf = dataset(\"datasets\", \"iris\")\n\nfeature_names = [\"Sepal Length\", \"Sepal Width\", \"Petal Length\", \"Petal Width\", \"Species\"]\n\nrename!(df, feature_names)\n\nfig = Figure(size=(1200, 1200))\nfor i in 1:4\n    for j in 1:4\n        scatter = data(df) * mapping(feature_names[i], feature_names[j], color=feature_names[5]) * visual(Scatter)\n        ax_scatter = Axis(fig[i, j],\n            xlabel=feature_names[i],\n            ylabel=feature_names[j])\n        grid = draw!(ax_scatter, scatter)\n        if i == 1 && j == 1\n            legend!(fig[i, j], grid; tellheight=false, tellwidth=false, halign=:left, valign=:top)\n        end\n    end\nend\nfig\n```\n\n#### Heat map with marginals\n\nIn cases of pairs of observations $(x_1, y_1), ..., (x_n, y_n)$, the bivariate data can be constructed into a bivariate histogram (shown in the form of heat map in the 2D plane) in a manner similar to the univariate histogram. In addition, we can also add two marginal histograms beside the heat map, which are two separate histograms, one for $x_1, ..., x_n$, and the other for $y_1, ..., y_n$.\n\n```{julia}\nusing Distributions, DataFrames, AlgebraOfGraphics, CairoMakie\n\nN = 10^6\nmeanVect = [27, 26]\ncovMat = [16 13; 13 12]\nbiNorm = MvNormal(meanVect, covMat)\nsimData = DataFrame(rand(biNorm, N)', [:x, :y])\n\nfig = Figure(size=(600, 600))\ngl = fig[1, 1] = GridLayout()\n\nax_x = Axis(gl[1, 1])\nhist!(ax_x, simData[!, :x], bins=50, normalization=:pdf)\n\nax_y = Axis(gl[2, 2])\nhist!(ax_y, simData[!, :y], bins=50, normalization=:pdf, direction=:x)\n\nfor ax in [ax_x, ax_y]\n    hidedecorations!(ax)\n    hidespines!(ax)\nend\n\nax_hm = Axis(gl[2, 1],\n    xlabel=\"x\",\n    ylabel=\"y\")\nhm = data(simData) * mapping(:x, :y) * AlgebraOfGraphics.density(npoints=50)\ngrid = draw!(ax_hm, hm)\ncolorbar!(gl[3, 1], grid; tellheight=true, tellwidth=true, vertical=false, flipaxis=false)\n\ncolgap!(gl, 0)\nrowgap!(gl, 0)\ncolsize!(gl, 2, Auto(0.25))\nrowsize!(gl, 1, Auto(0.25))\n\nfig\n```\n\n### Andrews plot\n\nThe idea of Andrews plot is to represent a data vector $(x_{i1}, ..., x_{ip})$ via a real-valued function. **For any individual vector, such a transformation cannot be generally useful; however, when comparing groups of vectors, it may yield a way to visualize structural differences in the data.**\n\nThe specific transformation rule that we present here creates a plot known as *Andrews plot*.\n\nHere, for the $i$-th data vector $(x_{i1}, ..., x_{ip})$, we create the function $f_i(\\cdot)$ defined on $[-\\pi, \\pi]$ via,\n\n$$\nf_i(t) = \\frac{x_{i1}}{\\sqrt{2}} + x_{i2}\\sin(t) + x_{i3}\\cos(t) + x_{i4}\\sin(2t) + x_{i5}\\cos(2t) + x_{i6}\\sin(3t) + x_{i7}\\cos(3t) + \\cdots\n$$\n\nwith the last term involving a $\\sin()$ if $p$ is even and a $\\cos()$ is $p$ is odd. For $i = 1, ..., n$, the functions $f_1(\\cdot), ..., f_n(\\cdot)$ are plotted.\n\nIn cases where each $i$ has an associated label from a small finite set, different colors or line patterns can be used.\n\n```{julia}\nusing RDatasets, AlgebraOfGraphics, DataFrames, StatsBase, CairoMakie\n\nfunction gen_uni_str(n::Int; exclude_strs::Vector{String}=String[], iter_n::Int=1000)\n    alphabet = [collect('a':'z'); collect('A':'Z')]\n    num_underscore = [collect('0':'9'); \"_\"]\n\n    for i in 1:iter_n\n        uni_str = join([rand(alphabet, 1); rand([alphabet; num_underscore], n - 1)])\n        if uni_str .∉ Ref(exclude_strs)\n            return uni_str\n        end\n    end\n    error(\"cannot generate an unique string against the given arguments\")\nend\n\nfunction andrewsplot(df::DataFrame, features::Vector{String}; npoints::Int=100, scale::Bool=true)\n    if nrow(df) < 1 || length(features) < 1\n        error(\"both the data frame and features must have at least 1 element\")\n    end\n\n    if npoints < 1\n        error(\"the npoints must be an integer greater than 0\")\n    end\n\n    tmp_df = transform(df, eachindex => \"row_number\")\n    transform!(tmp_df, :row_number => (x -> string.(x)) => :row_number)\n    n_vars = length(features)\n\n    if scale\n        # scale each column to mean 0 and std 1\n        # to ensure that all features contribute equally to the shape of the curve\n        scaled_df = DataFrame(hcat([zscore(tmp_df[!, j]) for j in features]...), features)\n    else\n        scaled_df = tmp_df[!, features]\n    end\n\n    if iseven(n_vars)\n        placeholder_column_name = gen_uni_str(12; exclude_strs=features)\n        scaled_df[!, placeholder_column_name] = zeros(nrow(scaled_df))\n        n_vars = n_vars + 1\n    end\n\n    fvs = Vector{Float64}(undef, nrow(scaled_df) * npoints)\n    fvs_index_pairs = [[(i - 1) * npoints + 1, min(i * npoints, length(fvs))] for i in 1:Int(ceil(length(fvs) / npoints))]\n    ob_index_pairs = [[(i - 1) * 2 + 1, min(i * 2, n_vars - 1)] for i in 1:Int(ceil((n_vars - 1) / 2))]\n    f_range = collect(range(-π, π; length=npoints))\n    for i in 1:nrow(scaled_df)\n        ob = Vector(scaled_df[i, :])\n        f_it0 = popfirst!(ob) / √2\n        for j in eachindex(f_range)\n            t = f_range[j]\n            f_it = f_it0\n            for multiplier in 1:length(ob_index_pairs)\n                x1, x2 = ob[ob_index_pairs[multiplier]]\n                f_it = f_it + x1 * sin(multiplier * t) + x2 * cos(multiplier * t)\n            end\n            fvs[fvs_index_pairs[i][1]+j-1] = f_it\n        end\n    end\n\n    fvs_df = DataFrame(andrew_plot_x=repeat(f_range; outer=nrow(scaled_df)),\n        andrew_plot_y=fvs,\n        row_number=repeat(1:nrow(scaled_df); inner=npoints))\n    transform!(fvs_df, :row_number => (x -> string.(x)) => :row_number)\n    return innerjoin(tmp_df, fvs_df; on=:row_number, renamecols=\"_raw\" => \"_new\")\nend\n\niris = dataset(\"datasets\", \"iris\")\nfeatures = [\"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\"]\ndf = andrewsplot(iris, features; scale=false)\np = data(df) * mapping(:andrew_plot_x_new, :andrew_plot_y_new; group=:row_number, color=:Species_raw) * visual(Lines)\ndraw(p; figure=(size=(800, 500),))\n```\n\n### Plots for the board room\n\n#### Pie chart\n\nUsed to convey relative proportions.\n\n```{julia}\nusing CairoMakie\n\nd = [36, 12, 68, 5, 42, 27]\ncolors = [:yellow, :orange, :red, :blue, :purple, :green]\n\npie(d,\n    color=colors,\n    radius=4,  # the radius of the pie plot\n    inner_radius=2,  # the inner radius between 0 and radius to create a donut chart\n    strokecolor=:white,\n    strokewidth=5,\n    axis=(autolimitaspect=1,),\n)\n```\n\n**Note:** introduction to two `Axis()` parameters:\n\n* `aspect=nothing`: defined as the axis aspect ratio of the width over height.\n\nThis will change the size of the axis.\n\nIf you set it to `DataAspect()`, the axis aspect ratio width/heigth will matches that of the data limits.\n\nFor example, if the x limits range from 0 to 300 and the y limits from 100 to 250, then `DataAspect()` will result in an aspect ratio of `(300 - 0) / (250 - 100) = 2`. This can be useful when plotting images, because the image will be displayed unsquished.\n\n`AxisAspect(ratio)` reduces the effective axis size within the available layout space so that the axis aspect ratio width/height matches `ratio`.\n\n* `autolimitaspect=nothing`: the ratio of the limits to the axis size equals that number.\n\nFor example, if the axis size is $100\\times 200$, then with `autolimitaspect=1`, the autolimits will also have a ratio of 1 to 2.\n\n```{julia}\nusing CairoMakie\n\npie([π / 2, 2π / 3, π / 4],\n    normalize=false,\n    offset=π / 2,\n    color=[:orange, :purple, :green],\n    axis=(autolimitaspect=1,),\n)\n```\n\n#### Bar plot\n\nUsed to convey relative proportions.\n\n```{julia}\nusing CSV, DataFrames, AlgebraOfGraphics, CairoMakie, CategoricalArrays\n\ndf = CSV.read(\"./data/companyData.csv\", DataFrame)\ndf[!, \"Year\"] = categorical(df[!, \"Year\"])\ndf[!, \"Type\"] = categorical(df[!, \"Type\"]; levels=[\"C\", \"B\", \"A\"])\n\np = data(df) * mapping(:Year, :MarketCap; color=:Type, stack=:Type) * visual(BarPlot)\ndraw(p)\n```\n\n```{julia}\nusing CSV, DataFrames, AlgebraOfGraphics, CairoMakie\n\ndf = CSV.read(\"./data/companyData.csv\", DataFrame)\n\np = data(df) * mapping(:Year, :MarketCap; color=:Type, dodge=:Type) * visual(BarPlot)\ndraw(p)\n```\n\n#### Stack plot\n\nShow how constituent amounts of a metric change over time.\n\n```{julia}\nusing CSV, DataFrames, AlgebraOfGraphics, CairoMakie, CategoricalArrays\n\nfunction areaplot(df::DataFrame, x::AbstractString, y::AbstractString, group::AbstractString)\n    if nrow(df) == 0\n        error(\"the data frame is empty\")\n    end\n\n    tmp_df = groupby(df[:, [x, y, group]], group)\n    final_df = DataFrame([[], [], [], []], [x, y, group, \"row_number\"])\n    for i in 1:length(tmp_df)\n        sort!(tmp_df[i], x; rev=false)\n        transform!(tmp_df[i], eachindex => :row_number)\n        if i == 1\n            sub_tmp_df = copy(tmp_df[i])\n            sub_tmp_df[!, y] = repeat([0], nrow(sub_tmp_df))\n        else\n            sub_tmp_df = copy(tmp_df[i-1])\n            sub_tmp_df[!, group] = tmp_df[i][:, group]\n            tmp_df[i][!, y] = tmp_df[i][!, y] .+ sub_tmp_df[!, y]\n        end\n        final_df = vcat(final_df, sort(sub_tmp_df, x; rev=false), sort(tmp_df[i], x; rev=true))\n    end\n    transform!(final_df, Cols(:row_number, y) => ByRow(((x, y) -> (x, y))) => :Point)\n\n    return final_df\nend\n\ndf = CSV.read(\"./data/companyData.csv\", DataFrame)\ndf[!, \"Year\"] = categorical(df[!, \"Year\"])\ndf[!, \"Type\"] = categorical(df[!, \"Type\"])\nx, y, group = \"Year\", \"MarketCap\", \"Type\"\nfinal_df = areaplot(df, x, y, group)\n\np = data(final_df) * mapping(:Point; color=:Type) * visual(Poly)\ndraw(p)\n```\n\n### Working with files and remote servers\n\n## Statistical inference concepts\n\nThe statistical inference concepts involve **using mathematical techniques to make conclusions about unkown *population* parameters based on collected data**.\n\nThe analyses and methods of statistical inference can be categorized into:\n\n* Frequentist (classical): based on the assumption that **population parameters of some underlying distribution, or probability law, exist and are fixed, but are yet unknown**. The process of statistical inference then deals with **making conclusions about these parameters based on sampled data**.\n\n* Bayesian: only assumes that **there is a prior distribution of the parameters**. The key process deals with **analyzing a posterior distribtution of the parameters**.\n\n* Machine learning.\n\nIn general, a statistical inference process involves **data**, **model**, and **analysis**. The data is assumed to be comprised of random samples from the model. The goal of the analysis is to make informed statements about population parameters of the model based on the data.\n\nSuch statements typically take one of the following forms:\n\n* Point estimation: determination of a single value (or vector of values) representing a best estimate of the parameter/parameters.\n\n* Confidence intervals: determination of a range of values where the parameter lies. Under the model and the statistical process used, it is guaranteed that **the parameter lies within this range with a pre-specified probability**.\n\n* Hypothesis tests: the process of determining **if the parameter lies in a given region, in the complement of that region, or fails to take on a specific value**.\n\n### A random sample\n\nWhen carrying out **frequentist** statistical inference, we assume that there is some underlying distribution $F(x; \\theta)$ from which we are sampling, where $\\theta$ is the scalar or vector-valued unknown parameter we wish to know.\n\nWe assume that each **observation** is **statistically independent** and **identically distributed** as the rest. That is, from a probablistic perspective, the **observations** are taken as **independent and identically distributed** (i.i.d) **random variables**. In mathematical statistics, this is called **a random sample**. We denote the random variables of the observations by $X_1, ..., X_n$, and their respective values by $x_1, ..., x_n$.\n\nTypically, we compute **statistics** from the random sample, such as the **sample mean** and **sample variance**. We can consider each observation as a random variable, so these statistics are random variables too.\n\n$$\n\\overline{X} = \\frac{1}{n} \\sum_{i=1}^n X_i\\ \\ \\ \\ \\text{and}\\ \\ \\ \\ S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X-\\overline{X})^2\n$$\n\nFor $S^2$, we use $n-1$, which makes $S^2$ an **unbiased estimator** of the population variance.\n\nHere, we consider the sample statistics, such as the sample mean and sample variance, as random variables. This means that these statistics also subject to some underlying distributions. To know what distribution each statistics subject to is the first step to do statistical inference.\n\n### Sampling from a normal population\n\nWe often assume that the distribution we sample from is a normal distribution (i.e. $F(x; (\\mu, \\sigma^2))$).\n\nUnder the normality assumption, the distribution of the random variables $\\overline{X}$ and $S^2$ as well as transformations of them are well known:\n\n$$\n\\begin{align}\n\\overline{X} &\\backsim N(\\mu, \\frac{\\sigma^2}{n}) \\\\\n\\frac{(n-1)S^2}{\\sigma^2} &\\backsim \\chi^2_{n-1} \\\\\nT := \\frac{\\overline{X}-\\mu}{S/\\sqrt{n}} &\\backsim t_{n-1}\n\\end{align}\n$$\n\nThe notations $\\chi^2_{n-1}$ and $t_{n-1}$ denote a chi-squared distribution and a student T-distribution, respectively.\n\n### Independence of the sample mean and sample variance\n\nIn many cases, the sample mean and sample variance calculated from the **same** sample group are not independent, but in the special case where the samples $X_1, ..., X_n$ are from a **normal distribution**, independence between $\\overline{X}$ and $S^2$ holds. In fact, this property characetrizes the normal distribution - that is, this property only holds for the normal distribution.\n\n```{julia}\nusing Distributions, CairoMakie, Random, DataFrames\n\nRandom.seed!(1234)\n\nfunction mean_var(dist, n)\n    sample = rand(dist, n)\n    (mean(sample), var(sample))\nend\n\nuni_dist = Uniform(-sqrt(3), sqrt(3))\nn, N = 3, 10^5\n\n# the sample mean and sample variance are calculated from the same sample group\n# so the two are not independent\ndata_uni = DataFrame([mean_var(uni_dist, n) for _ in 1:N], [:mean, :var])\n# the sample mean and sample variance are calculated from two different sample groups\n# so the two are independent\ndata_uni_ind = DataFrame([(mean(rand(uni_dist, n)), var(rand(uni_dist, n))) for _ in 1:N], [:mean, :var])\n\nfig, ax = scatter(data_uni.mean, data_uni.var; color=:blue, label=\"Same group\", markersize=2)\nscatter!(ax, data_uni_ind.mean, data_uni_ind.var; color=:orange, label=\"Separate group\", markersize=2)\nax.xlabel = L\"\\overline{X}\"\nax.ylabel = L\"S^2\"\nax.title = \"Uniform Distribution\"\naxislegend(ax)\nfig\n```\n\n```{julia}\n# in the case where we sample from the normal distribution\n# the sample mean and sample variance are always independent\n# independent of the way we calculate them i.e., from the same sample group or from two different sample groups\ndata_norm = DataFrame([mean_var(Normal(), n) for _ in 1:N], [:mean, :var])\ndata_norm_ind = DataFrame([(mean(rand(Normal(), n)), var(rand(Normal(), n))) for _ in 1:N], [:mean, :var])\n\nfig, ax = scatter(data_norm.mean, data_norm.var; color=:blue, label=\"Same group\", markersize=2)\nscatter!(ax, data_norm_ind.mean, data_norm_ind.var; color=:orange, label=\"Separate group\", markersize=2)\nax.xlabel = L\"\\overline{X}\"\nax.ylabel = L\"S^2\"\nax.title = \"Normal Distribution\"\naxislegend(ax)\nfig\n```\n\n### T-Distribution\n\nThe random variable **T-statistic** is given by\n\n$$\nT = \\frac{\\overline{X}-\\mu}{S/\\sqrt{n}} \\backsim t_{n-1}\n$$\n\nDenoting the mean and variance of the normally distributed observations by $\\mu$ and $\\sigma^2$, respectively, we can represent the T-statistic as\n\n$$\nT = \\frac{\\frac{\\overline{X}-\\mu}{\\sigma/\\sqrt{n}}}{\\sqrt{\\frac{(n-1)S^2}{\\sigma^2}\\frac{1}{n-1}}} = \\frac{Z}{\\sqrt{\\frac{\\chi^2_{n-1}}{n-1}}}\n$$\n\nHere, the numerator $Z$ is a standard normal random variable, and in the denominator the random variable $\\chi^2_{n-1} = (n-1)S^2/\\sigma^2$ is chi-distributed wit $n-1$ degrees of freedom. Furthermore, the numerator and denominator random variables are independent because they are based on the sample mean and sample variance, respectively.\n\nHence, $T \\backsim t(n-1)$, which means a \"T-Distribution with $n-1$ degrees of freedom\".\n\nHere, we check the above fact that T-statistic is derived from two independent random variables (the numerator is a standard normal random variable, while the denominator is a random variable chi-distributed with $n-1$ degrees of freedom):\n\n```{julia}\nusing Random, StatsBase, Distributions, CairoMakie\n\nRandom.seed!(0)\n\nfunction tStat(degree)\n    z = rand(Normal())\n    c = rand(Chisq(degree))\n    z / sqrt(c / degree)\nend\n\nn, N = 10, 10^6\n\nsimulationTStats = [tStat(n - 1) for _ in 1:N]\n\nxGrid = -5:0.01:5\n\nfig, ax = stephist(simulationTStats; bins=400, color=:blue, label=\"Simulated\", normalization=:pdf)\nlines!(ax, xGrid, pdf.(TDist(n - 1), xGrid); color=:red, label=\"Analytical\")\nax.limits = (first(xGrid), last(xGrid), nothing, nothing)\nfig\n```\n\nA T-Distribution with $k$ degrees of freedom can be shown to have a density function,\n\n$$\nf(x) = \\frac{\\Gamma(\\frac{k+1}{2})}{\\sqrt{k\\pi} \\Gamma(\\frac{k}{2})} \\left(1+\\frac{x^2}{k}\\right)^{-\\frac{k+1}{2}}\n$$\n\nNote that $E(\\chi^2_{n-1}) = n-1$ and $Var(\\chi^2_{n-1}) = 2(n-1)$, so $E\\left(\\frac{\\chi^2_{n-1}}{n-1}\\right) = 1$, and $Var(\\frac{\\chi^2_{n-1}}{n-1}) = \\frac{2}{n-1}$.\n\nTherefore, we have $\\frac{\\chi^2_{n-1}}{n-1} \\rightarrow 1$ as $n \\rightarrow \\infty$, with the same holding for $\\sqrt{\\frac{\\chi^2_{n-1}}{n-1}}$.\n\nHence, for large $n$, the distribution of $T$ will converge to the distribution of $Z$.\n\n```{julia}\nusing Distributions, Random, CairoMakie, DataFrames\n\nRandom.seed!(1234)\n\nn, N, alpha = 3, 10^7, 0.1\n\nmyT(n) = rand(Normal()) / sqrt(rand(Chisq(n - 1)) / (n - 1))\nmcQuantile = quantile([myT(n) for _ in 1:N], alpha)\nanalyticQuantile = quantile(TDist(n - 1), alpha)\n\nprintln(\"Quantile from Monte Carlo: \", mcQuantile)\nprintln(\"Analytic quantile: \", analyticQuantile)\n\nxGrid = -5:0.1:5\n\nfig = Figure()\nax = fig[1, 1] = Axis(fig)\n\nlines!(ax, xGrid, pdf.(Normal(), xGrid), label=\"Normal\", color=:red)\nscatter!(ax, xGrid, pdf.(TDist(1), xGrid), label=\"DOF = 1\", color=:blue)\nscatter!(ax, xGrid, pdf.(TDist(5), xGrid), label=\"DOF = 5\", color=:purple)\nscatter!(ax, xGrid, pdf.(TDist(10), xGrid), label=\"DOF = 10\", color=:orange)\nscatter!(ax, xGrid, pdf.(TDist(100), xGrid), label=\"DOF = 100\", color=:green)\n\naxislegend(ax)\n\nfig\n```\n\n### Two samples and the F-Distribution\n\nMany statistical procedures involve **the ratio of sample variances**, or similar quantities, for two or more samples.\n\nFor example, if $X_1, ..., X_{n_1}$ is one sample, and $Y_1, ..., Y_{n_2}$ is another sample, and **both samples are distributed normally with the same parameters**, then the ratio of the two sample variances\n\n$$\nF = \\frac{S_X^2}{S_Y^2}\n$$\n\nIt turns out such a statistic distributed as the **F-Distribution**, with density given by\n\n$$\nf(x) = K(a, b) \\frac{x^{\\frac{a}{2}-1}}{(ax+b)^{\\frac{a+b}{2}}}\\ \\ \\ \\ \\text{with}\\ \\ \\ \\ K(a, b) = \\frac{\\Gamma(\\frac{a+b}{2}) a^{\\frac{a}{2}} b^{\\frac{b}{2}}}{\\Gamma(\\frac{a}{2}) \\Gamma(\\frac{b}{2})}\n$$\n\nHere, the parameters $a$ and $b$ are the numerator degrees of freedom and denominator degrees of freedom, respectively.\n\n```{julia}\nusing Distributions, CairoMakie\n\nn1, n2 = 10, 15\nN = 10^6\nmu, sigma = 10, 4\nnorm_dist = Normal(mu, sigma)\n\nfvs = Array{Float64}(undef, N)\n\nfor i in 1:N\n    d1 = rand(norm_dist, n1)\n    d2 = rand(norm_dist, n2)\n    fvs[i] = var(d1) / var(d2)\nend\n\nf_range = 0:0.1:5\nfig, ax = stephist(fvs, bins=400, color=:blue, label=\"Simulated\", normalization=:pdf)\nlines!(ax, f_range, pdf.(FDist(n1 - 1, n2 - 1), f_range), color=:red, label=\"Analytic\")\nxlims!(ax, low=0, high=5)\naxislegend(ax)\nfig\n```\n\n### The central limit theorem\n\nThe Central Limit Theorem (CLT) indicates that **summations of a large number of independent random quantities, each with finite variance, yield a sum that is approximately normally distributed**.\n\nThis is why the normal distribution is ubiquitous in nature.\n\nConsider an i.i.d sequence $X_1, X_2, ...$, where all $X_i$ are distributed according to some distribution $F(x_i; \\theta)$ with mean $\\mu$ and finite variance $\\sigma^2$.\n\nThen consider the random variable\n\n$$\nY_n := \\sum_{i=1}^n X_i\n$$\n\nIt is clear that $E(Y_n) = n\\mu$ and $Var(Y_n) = n\\sigma^2$.\n\nHence, we may consider a random variable\n\n$$\n\\widetilde{Y}_n := \\frac{Y_n - n\\mu}{\\sqrt{n}\\sigma}\n$$\n\nObserve that $\\widetilde{Y}_n$ is zero mean and unit variance. The CLT states that as $n \\rightarrow \\infty$, the ditribution of $\\widetilde{Y}_n$ converges to a standard normal distribution. That is, for every $x \\in R$,\n\n$$\n\\lim_{n \\rightarrow \\infty} P(\\widetilde{Y}_n \\le x) = \\int_{-\\infty}^x \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{u^2}{2}} du\n$$\n\nAlternatively, this may be viewed as indicating that for non-small $n$\n\n$$\nY_n\\ \\ \\widetilde{\\text{approx}}\\ \\ N(n\\mu, n\\sigma^2)\n$$\n\nIn addition, we have\n\n$$\n\\overline{Y}_n = \\frac{Y_n}{n}\\ \\ \\widetilde{\\text{approx}}\\ \\ N(\\mu, \\frac{\\sigma^2}{n})\n$$\n\nThis means that sample means from i.i.d samples with finite variances are asymptotically distributed according to a normal distribution as the sample size grows.\n\n```{julia}\nusing Distributions, Random, CairoMakie\n\nRandom.seed!(1234)\n\n# note that n = 30 isn't enough to get a perfect fit to a normal distribution in the case of exponential distribution\nn, N = 30, 10^6\n\ndist1 = Uniform(1 - sqrt(3), 1 + sqrt(3))\ndist2 = Exponential(1)\ndist3 = Normal(1, 1)\n\ndata1 = [mean(rand(dist1, n)) for _ in 1:N]\ndata2 = [mean(rand(dist2, n)) for _ in 1:N]\ndata3 = [mean(rand(dist3, n)) for _ in 1:N]\n\nfig, ax = stephist(data1, bins=100, color=:blue, label=\"Average of Uniforms\", normalization=:pdf)\nstephist!(ax, data2, bins=100, color=:orange, label=\"Average of Exponentials\", normalization=:pdf)\nstephist!(ax, data3, bins=100, color=:green, label=\"Average of Normals\", normalization=:pdf)\nlines!(ax, 0:0.01:2, pdf.(Normal(1, 1 / sqrt(n)), 0:0.01:2), color=:red, label=\"Analytic Normal Distribution\")\naxislegend(ax)\nfig\n```\n\n### Point estimation\n\nGiven a random sample, $X_1, ..., X_n$, a common task of statistical inference is to estimate a parameter $\\theta$, or a function of it, say $h(\\theta)$.\n\nThe process of **designing an estimator**, **analyzing its performance**, and **carrying out the estimation** is called *point estimation*.\n\nAlthough we can never know the underlying parameter $\\theta$, or $h(\\theta)$ exactly, we can arrive at an estimate for it via an estimator $\\hat{\\theta} = f(X_1, ..., X_n)$. Here, the design of the estimator is embodied by $f(\\cdot)$, a function that specifies how to construct the estimate from the sample.\n\nWhen performing point estimation, the first question we must answer is how close is $\\hat{\\theta}$ to the actual unknown quantity $\\theta$ or $h(\\theta)$?\n\n#### Describing the performance and behavior of estimators\n\nWhen analyzing the performance of an estimator $\\hat{\\theta}$, it is important to understand that it is a random variable.\n\nOne common measure of its performance is the **Mean Squared Error** (MSE),\n\n$$\n\\begin{align}\nMSE_\\theta(\\hat{\\theta}) &:= E[(\\hat{\\theta}-\\theta)^2] \\\\\n&= E(\\hat{\\theta}^2-2\\hat{\\theta}\\theta+\\theta^2) \\\\\n&= E(\\hat{\\theta}^2) - 2\\theta E(\\hat{\\theta}) + \\theta^2 \\\\\n&= E(\\hat{\\theta}^2) - [E(\\hat{\\theta})]^2 + [E(\\hat{\\theta})]^2 - 2\\theta E(\\hat{\\theta}) + \\theta^2 \\\\\n&= Var(\\hat{\\theta}) + (E(\\hat{\\theta}) - \\theta)^2 \\\\\n&:= variance + bias^2\n\\end{align}\n$$\n\nHere, the MSE can be decomposed into **the variance of the estimator** and **its bias squared**.\n\n* The variance of the estimator represents the dispersion degree of the estimator itself. Low variance is clearly a desirable performance measure. This indicates the stability of the estimator.\n\n* The bias squared represents whether the estimator $\\hat{\\theta}$ is an **unbiased** estimator of the parameter $\\theta$ or $h(\\theta)$. This indicates how close is the $E(\\hat{\\theta})$ to $\\theta$ or $h(\\theta)$.\n\nThis can be illustrated by the folllowing plot:\n\n```{julia}\nusing Distributions, CairoMakie, Random\n\nRandom.seed!(1234)\n\nest_pts = rand(Normal(3, 1), 10)\n\nfig = Figure()\nax = Axis(fig[1, 1])\n\nscatter!(repeat([0], 10), est_pts; color=:black, markersize=10, label=L\"\\hat{\\theta}\")\nscatter!(0, mean(est_pts); label=L\"E(\\hat{\\theta})\", color=:cyan, markersize=20)\nscatter!(0, 10; color=:red, markersize=20, label=L\"\\theta\")\n\nbracket!(0, mean(est_pts), 0, 10; text=L\"(E(\\hat{\\theta})-\\theta)^2\", offset=20, style=:square, orientation=:up)\nbracket!(0, minimum(est_pts), 0, maximum(est_pts); text=L\"Var(\\hat{\\theta})\", offset=20, style=:square, orientation=:down)\n\naxislegend(ax)\nfig\n```\n\nCertainly, we are really interested in **whether an estimator is unbiased**, and then **how low the variance of the estimator is**.\n\nWhether an estimator is unbiased means that $E(\\hat{\\theta}) = \\theta$.\n\nHere, we give some examples:\n\nConsider $X_1, ..., X_n$ distributed according to any distribution with a finite mean $\\mu$.\n\n1. The sample mean $\\overline{X}$ is an unbiased estimator of the population mean $\\mu$:\n\n$$\n\\begin{align}\nE(\\overline{X}) &= E\\left[\\frac{1}{n} \\sum_{i=1}^n X_i\\right] \\\\\n&= \\frac{1}{n} \\sum_{i=1}^n E(X_i) \\\\\n&= \\frac{1}{n} n\\mu \\\\\n&= \\mu\n\\end{align}\n$$\n\n$$\n\\begin{align}\nVar(\\overline{X}) &= Var\\left(\\frac{1}{n} \\sum_{i=1}^n X_i\\right) \\\\\n&= \\frac{1}{n^2} \\sum_{i=1}^n Var(X_i) \\\\\n&= \\frac{1}{n^2} n\\sigma^2 \\\\\n&= \\frac{\\sigma^2}{n}\n\\end{align}\n$$\n\nSo the sample mean $\\overline{X} = \\frac{1}{n} \\sum_{i=1}^n X_i$ is an unbiased estimator of the population mean $\\mu$ with the variance $\\frac{\\sigma^2}{n}$.\n\n2. In the case where the population mean $\\mu$ is known, but the population variance $\\sigma^2$ is unknown, then $\\hat{\\sigma^2} := \\frac{1}{n} \\sum_{i=1}^n (X_i-\\mu)^2$ is an unbiased estimator of the population variance $\\sigma^2$, but $\\hat{\\sigma} := \\sqrt{\\hat{\\sigma^2}}$ is not an unbiased estimator of $\\sigma$ (in fact, $\\hat{\\sigma}$ is asymptotically unbiased. That is, the bias tends to $0$ as the sample size grows):\n\n$$\n\\begin{align}\nE(\\hat{\\sigma^2}) &= E\\left(\\frac{1}{n} \\sum_{i=1}^n (X_i-\\mu)^2\\right) \\\\\n&= \\frac{1}{n} \\sum_{i=1}^n E\\left((X_i-\\mu)^2\\right) \\\\\n&= \\frac{1}{n} n\\sigma^2 \\\\\n&= \\sigma^2\n\\end{align}\n$$\n\nSo in the case where the population mean $\\mu$ is known, $\\hat{\\sigma^2}$ is an unbiased estimator of $\\sigma^2$, but $\\hat{\\sigma}$ is not an unbiased estimator of $\\sigma$.\n\n```{julia}\nusing Random, Statistics\n\nRandom.seed!(1234)\n\n# consider an uniform distribution over [0, 1]\ntrueVar, trueStd = 1 / 12, sqrt(1 / 12)\n\nfunction estVar(n)\n    sample = rand(n)\n    sum((sample .- 0.5) .^ 2) / n\nend\n\nN = 10^7\nfor n in 10:20:90\n    biasVar = mean([estVar(n) for _ in 1:N]) - trueVar\n    biasStd = mean([sqrt(estVar(n)) for _ in 1:N]) - trueStd\n    println(\"n = \", n, \" Var bias: \", round(biasVar; digits=6),\n        \"\\t Std bias: \", round(biasStd; digits=5))\nend\n```\n\n3. In the case where the population mean $\\mu$ is not known, the sample variance $S^2 := \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\overline{X})^2$ is an unbiased estimator of $\\sigma^2$:\n\n$$\n\\begin{align}\n\\sum_{i=1}^{n}(x_i-\\bar{x})^2 &= \\sum_{i=1}^{n}(x_i^2-2x_i\\bar{x}+\\bar{x}^2) \\\\\n&= \\sum_{i=1}^{n}x_i^2 - 2n\\bar{x}\\frac{1}{n}\\sum_{i=1}^{n}x_i + n\\bar{x}^2 \\\\\n&= \\sum_{i=1}^{n}x_i^2 - n\\bar{x}^2\n\\end{align}\n$$\n\n$$\nE(X^2) = Var(X) + [E(X)]^2 = \\sigma^2 + \\mu^2\n$$\n\n$$\n\\begin{align}\nE(S^2) &= E\\left(\\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\overline{X})^2\\right) \\\\\n&= \\frac{1}{n-1}E\\left(\\sum_{i=1}^{n}X_i^2 - n\\overline{X}^2\\right) \\\\\n&= \\frac{1}{n-1}\\left(\\sum_{i=1}^{n}E(X_i^2) - nE(\\overline{X}^2)\\right) \\\\\n&= \\frac{1}{n-1}\\left(n(\\sigma^2+\\mu^2) - n(\\frac{\\sigma^2}{n}+\\mu^2)\\right) \\\\\n&= \\frac{1}{n-1} (n-1)\\sigma^2 \\\\\n&= \\sigma^2\n\\end{align}\n$$\n\nIn summary, we can evaluate the performance and behavior of an estimator from different aspects, such as **unbias** ($E(\\hat{\\theta}) = \\theta$), **effectiveness** ($Var(\\hat{\\theta})$ is as small as possible), **consistency** (an estimator is consistent if it converges to the true value as the number of observations grows to infinity), etc.\n\n#### Designing estimators\n\n##### Method of moments\n\nThe key idea is that the $k$'s moment estimator calculated from the random sample should be equal to the $k$'s moment of the underlying distribution from which we sample (i.e. $\\hat{m}_k = m_k$). Then we can obtain parameter estimates for a distribution.\n\n$$\n\\hat{m}_k = \\frac{1}{n} \\sum_{i=1}^{n}X_i^k\n$$\n\n$$\nm_k = E(X^k) = \\begin{cases}\n\\sum_{i=1}^{\\infty} x_i p(x_i) &\\text{for discrete case} \\\\\n\\int_{-\\infty}^{\\infty} x p(x) dx &\\text{for continuous case}\n\\end{cases}\n$$\n\nIn cases where there are multiple unkown parameters, say $K$, we use the first $K$ moment estimates to formulate a system of $K$ equations and $K$ unkowns. This system of equations can be written as $E[X^k; \\theta_1, ..., \\theta_K] = \\hat{m}_k\\ \\ \\ \\ \\text{for}\\ \\ \\ \\ k=1,...,K$.\n\n```{julia}\nusing Random, Distributions, NLsolve\n\nRandom.seed!(1234)\n\n# Triangular distribution\na, b, c = 3, 5, 4\ndist = TriangularDist(a, b, c)\nn = 2000\nsamples = rand(dist, n)\n\nm_k(k, data) = 1 / n * sum(data .^ k)\nmHats = [m_k(i, samples) for i in 1:3]\n\nfunction equations(F, x)\n    F[1] = 1 / 3 * (x[1] + x[2] + x[3]) - mHats[1]\n    F[2] = 1 / 6 * (x[1]^2 + x[2]^2 + x[3]^2 +\n                    x[1] * x[2] + x[1] * x[3] + x[2] * x[3]) - mHats[2]\n    F[3] = 1 / 10 * (x[1]^3 + x[2]^3 + x[3]^3 +\n                     x[1]^2 * x[2] + x[1]^2 * x[3] + x[2]^2 * x[1] +\n                     x[2]^2 * x[3] + x[3]^2 * x[1] + x[3]^2 * x[2] +\n                     x[1] * x[2] * x[3]) - mHats[3]\nend\n\nnlOutput = nlsolve(equations, [0.1, 0.1, 0.1])\nsol = sort(nlOutput.zero)\naHat, bHat, cHat = sol[1], sol[3], sol[2]\nprintln(\"Found estimates for (a, b, c) = \", (aHat, bHat, cHat), \"\\n\")\nprintln(nlOutput)\n```\n\n##### Maximum likelihood estimation (MLE)\n\nThe key is to consider the **likelihhod** of the parameter $\\theta$ having a specific value given observations $x_1, ..., x_n$. This is done via the likelihood function,\n\n$$\nL(\\theta; x_1, ..., x_n) = f_{X_1, ..., X_n}(x_1, ..., x_n; \\theta)\n$$\n\nIf $X_1, ..., X_n$ are i.i.d., where the joint PDF of $X_1, ..., X_n$ is represented as the product of the individual PDF, then we have\n\n$$\nL(\\theta; x_1, ..., x_n) = f_{X_1, ..., X_n}(x_1, ..., x_n; \\theta) = \\prod_{i=1}^n f(x_i; \\theta)\n$$\n\nNow given the likelihood function, the **maximum likelihood estimator** is a value $\\theta$ that **maximizes** $L(\\theta; x_1, ..., x_n)$. So an MLE is the maximizer of the likelihood.\n\n```{julia}\nusing Random, Distributions, CairoMakie\n\nRandom.seed!(1234)\n\nrealAlpha, realLambda = 2, 3\ngammaDist = Gamma(realAlpha, 1 / realLambda)\n\nn = 10^2\nsamples = rand(gammaDist, n)\n\nalphaGrid = collect(1:0.02:3)\nlambdaGrid = collect(1:0.02:5)\n\nlikelihood = [prod(pdf.(Gamma(a, 1 / l), samples)) for a in alphaGrid, l in lambdaGrid]\n\nsurface(alphaGrid, lambdaGrid, likelihood, axis=(type=Axis3,))\n```\n\nObserve that any maximizer $\\hat{\\theta}$ of $L(\\theta; x_1, ..., x_n)$ will also maximize its logarithm. Practically, both from an analytic and numerical perspective, considering this $log-likelihood function$ is often more attractive:\n\n$$\nl(\\theta; x_1, ..., x_n) := \\log L(\\theta; x_1, ..., x_n) = \\sum_{i=1}^{n}\\log \\left(f(x_i; \\theta)\\right)\n$$\n\n**Note:** the second equality holds only for i.i.d. $X_1, ..., X_n$.\n\nHence, given a sample from a gamma distribution, the log-likelihood function is\n\nFirst, the PDF of the gamma distribution is\n\n$$\nf(x) = \\frac{\\lambda^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha - 1}e^{-\\lambda x}\n$$\n\nwith parameters $\\lambda > 0$ and $\\alpha > 0$.\n\n$$\nl(\\theta; x_1, ..., x_n) = n\\alpha\\log(\\lambda) - n\\log(\\Gamma(\\alpha)) + (\\alpha - 1)\\sum_{i=1}^{n}\\log(x_i) - \\lambda\\sum_{i=1}^{n}x_i\n$$\n\nDivide by $n$ to obtain the following function that needs to be maximized:\n\n$$\n\\tilde{l}(\\theta; \\bar{x}, \\bar{x}_l) = \\alpha\\log(\\lambda) - \\log(\\Gamma(\\alpha)) + (\\alpha - 1)\\bar{x}_l - \\lambda \\bar{x}\n$$\n\nwhere $\\bar{x}$ is the sample mean, and $\\bar{x}_l := \\frac{1}{n}\\sum_{i=1}^{n}\\log(x_i)$.\n\nFurther simplification is possible by removing the stand-alone $-\\bar{x}_l$ term, as it does not affect the optimal value. Heance, our optimization problem is then,\n\n$$\n\\max_{\\lambda > 0, \\alpha > 0} \\alpha (\\log(\\lambda + \\bar{x}_l)) - \\log(\\Gamma(\\alpha)) - \\lambda \\bar{x}\n$$\n\nAs is typical in such cases, the function actually depends on the sample only through the two sufficient statistics $\\bar{x}$ and $\\bar{x}_l$.\n\nThen, taking $\\alpha$ as fixed, we may consider the derivative with respect to $\\lambda$, and equate this to $0$:\n\n$$\n\\frac{\\alpha}{\\lambda} - \\bar{x} = 0\n$$\n\nHence, for any optimal $\\alpha^\\star$, we have $\\lambda = \\frac{\\alpha}{\\bar{x}}$. This allows us to substitute $\\lambda^\\star$ for $\\lambda$ to obtain\n\n$$\n\\max_{\\alpha > 0} \\alpha (\\log(\\alpha) - \\log(\\bar{x}) + \\bar{x}_l) - \\log(\\Gamma(\\alpha)) - \\alpha\n$$\n\nNow by taking the derivative with respective to $\\alpha$, and equating this to $0$, we obtain\n\n$$\n\\log(\\alpha) + 1 - \\log(\\bar{x}) + \\bar{x}_l - \\psi(\\alpha) - 1 = 0\n$$\n\nwhere $\\psi(z) := \\frac{d}{dz}\\log(\\Gamma(z))$ is the well-known **digamma function**.\n\nHence, we find that $\\alpha^\\star$ must satisfy\n\n$$\n\\log(\\alpha) - \\psi(\\alpha) - \\log(\\bar{x}) + \\bar{x}_l = 0\n$$\n\nwith $\\lambda^\\star = \\frac{\\alpha^\\star}{\\bar{x}}$. Our optimal MLE solution is given by $(\\alpha^\\star, \\lambda^\\star)$. In order to find this value, we must solve it numerically.\n\n```{julia}\nusing SpecialFunctions, Distributions, Roots, CairoMakie, Random\n\nRandom.seed!(1234)\n\neq(alpha, xb, xbl) = log(alpha) - digamma(alpha) - log(xb) + xbl\n\nrealAlpha, realLambda = 2, 3\ngammaDist = Gamma(realAlpha, 1 / realLambda)\n\nfunction mle(sample)\n    alpha = find_zero((a) -> eq(a, mean(sample), mean(log.(sample))), 1)\n    lambda = alpha / mean(sample)\n    return [alpha, lambda]\nend\n\nN = 10^4\n\nmles10 = [mle(rand(gammaDist, 10)) for _ in 1:N]\nmles100 = [mle(rand(gammaDist, 100)) for _ in 1:N]\nmles1000 = [mle(rand(gammaDist, 1000)) for _ in 1:N]\n\nfig = Figure()\nax = Axis(fig[1, 1],\n    xlabel=L\"\\alpha\",\n    ylabel=L\"\\lambda\",\n    limits=(0, 6, 0, 8))\n\nscatter!(ax, first.(mles10), last.(mles10),\n    color=:blue, markersize=2, label=\"n = 10\")\nscatter!(ax, first.(mles100), last.(mles100),\n    color=:red, markersize=2, label=\"n = 100\")\nscatter!(ax, first.(mles1000), last.(mles1000),\n    color=:green, markersize=2, label=\"n = 1000\")\n\nmarker_elem1 = MarkerElement(color=:blue, marker=:circle, markersize=20, points=Point2f[(0.5, 0.5)])\nmarker_elem2 = MarkerElement(color=:red, marker=:circle, markersize=20, points=Point2f[(0.5, 0.5)])\nmarker_elem3 = MarkerElement(color=:green, marker=:circle, markersize=20, points=Point2f[(0.5, 0.5)])\nLegend(fig[1, 1],\n    [marker_elem1, marker_elem2, marker_elem3],\n    [\"n = 10\", \"n = 100\", \"n = 1000\"],\n    tellwidth=false, tellheight=false,\n    halign=:left, valign=:top)\nfig\n```\n\n##### Comparing the method of moments and MLE\n\nNow we use the Mean Squared Error (MSE)\n\n$$MSE_\\theta(\\hat{\\theta}) = E[(\\hat{\\theta}-\\theta)^2] = Var(\\hat{\\theta}) + (E[\\hat{\\theta}] - \\theta)^2 = \\text{variance} + \\text{bias}^2$$\n\nto compare the performance and behavior of moments and MLE.\n\nConsider a random sample $x_1, ..., x_n$ from an uniform distribution on the interval $(a, b)$.\n\nThe MLE for the parameter $\\theta = (a, b)$ can be shown to be\n\n$$\n\\begin{align}\n\\hat{a} &= \\min\\{x_1, ..., x_n\\} \\\\\n\\hat{b} &= \\max\\{x_1, ..., x_n\\}\n\\end{align}\n$$\n\nFor the method of moments, since $X \\backsim U(a, b)$, it follows that\n\n$$\n\\begin{align}\nE[X] &= \\frac{a+b}{2} \\\\\nVar(X) &= \\frac{(b-a)^2}{12}\n\\end{align}\n$$\n\nHence, by solving for $a$ and $b$, and replacing $E[X]$ and $Var(X)$ with $\\bar{x}$ and $s^2$ respectively, we obtain\n\n$$\n\\begin{align}\n\\hat{a} &= \\bar{x}-\\sqrt{3}s \\\\\n\\hat{b} &= \\bar{x}+\\sqrt{3}s\n\\end{align}\n$$\n\n```{julia}\nusing Distributions, CairoMakie\n\nN = 10^5\nnMin, nStep, nMax = 10, 10, 200\nsampleSizes = nMin:nStep:nMax\ntrueB = 5\ntrueDist = Uniform(-2, trueB)\n\nMLEest(data) = maximum(data)\nMMest(data) = mean(data) + sqrt(3) * std(data)\n\nres = Dict{Symbol,Array{Float64}}(\n    (sym -> sym => Array{Float64}(undef, length(sampleSizes))).(\n        [:MSEMLE, :MSEMM, :VarMLE, :VarMM, :BiasMLE, :BiasMM]))\n\nfor (i, n) in enumerate(sampleSizes)\n    mleEst, mmEst = Array{Float64}(undef, N), Array{Float64}(undef, N)\n    for j in 1:N\n        samples = rand(trueDist, n)\n        mleEst[j] = MLEest(samples)\n        mmEst[j] = MMest(samples)\n    end\n    meanMLE, meanMM = mean(mleEst), mean(mmEst)\n    varMLE, varMM = var(mleEst), var(mmEst)\n\n    res[:MSEMLE][i] = varMLE + (meanMLE - trueB)^2\n    res[:MSEMM][i] = varMM + (meanMM - trueB)^2\n    res[:VarMLE][i] = varMLE\n    res[:VarMM][i] = varMM\n    res[:BiasMLE][i] = meanMLE - trueB\n    res[:BiasMM][i] = meanMM - trueB\nend\n\nfig = Figure(size=(600, 1200))\nax_mse = Axis(fig[1, 1],\n    xlabel=\"Sample size\",\n    ylabel=\"MSE\")\nscatter!(ax_mse, sampleSizes, res[:MSEMLE]; color=:blue, label=\"MSE (MLE)\")\nscatter!(ax_mse, sampleSizes, res[:MSEMM]; color=:red, label=\"MSE (MM)\")\naxislegend(ax_mse)\n\nax_var = Axis(fig[2, 1],\n    xlabel=\"Sample size\",\n    ylabel=\"Variance\")\nscatter!(ax_var, sampleSizes, res[:VarMLE]; color=:blue, label=\"Variance (MLE)\")\nscatter!(ax_var, sampleSizes, res[:VarMM]; color=:red, label=\"Variance (MM)\")\naxislegend(ax_var)\n\nax_bias = Axis(fig[3, 1],\n    xlabel=\"Sample size\",\n    ylabel=\"Bias\")\nscatter!(ax_bias, sampleSizes, res[:BiasMLE]; color=:blue, label=\"Bias (MLE)\")\nscatter!(ax_bias, sampleSizes, res[:BiasMM]; color=:red, label=\"Bias (MM)\")\naxislegend(ax_bias; position=:rb)\nfig\n```\n\nIn fact, there is more supporting theory for the usefulness of maximum likelihood estimation as $n \\rightarrow \\infty$.\n\n### Confidence interval\n\nGiven a single sample $X_1, ..., X_n$, how does one obtain an indication about the accuracy of the estimate?\n\nConsider the case where we are trying to estimate the parameter $\\theta$. A confidence interval is then an interval $[L, U]$ obtained from our sample data, such that $P(L\\le \\theta \\le U) = 1-\\alpha$, where $1-\\alpha$ is called the confidence level.\n\nConsider a case of a single observation $X$ ($n=1$) taken from a symmetric triangular distribution, with a spread of 2 and an unknown center (mean) $\\mu$.\n\nIn this case, we would set\n\n$$\n\\begin{align}\nL &= X + q_{\\frac{\\alpha}{2}} \\\\\nU &= X + q_{1-\\frac{\\alpha}{2}}\n\\end{align}\n$$\n\nwhere $q_u$ is the $u$'th quantile of a triangular distribution centered at $0$, and having a spread of $2$.\n\nSetting $L$ and $U$ in this manner ensures that $P(L\\le \\theta \\le U) = 1-\\alpha$ holds.\n\nIn this case, we know that $q_{\\frac{\\alpha}{2}} = -(1 - \\sqrt{\\alpha})$ and $q_{1-\\frac{\\alpha}{2}} = 1 - \\sqrt{\\alpha}$.\n\n::: {.callout-important}\n\nTo understand the confidence interval $P(L\\le \\theta \\le U) = 1-\\alpha$, a key point is that there is $1-\\alpha$ chance that the actual parameter $\\theta$ **is covered by** the interval $[L, U]$. This means that if the sampling experiment is repeated say $N$ times, then on average, $N\\times(1-\\alpha)$ of the time the actual parameter $\\theta$ is covered by the interval.\n\n:::\n\n```{julia}\nusing Distributions, CairoMakie, Random\n\nRandom.seed!(1234)\n\nalpha = 0.05\nL(obs) = obs - (1 - sqrt(alpha))\nU(obs) = obs + (1 - sqrt(alpha))\n\nmu = 5.57\ntriDist = TriangularDist(mu - 1, mu + 1, mu)\n\nN = 100\nbounds = zeros(N, 2)\nhits = Vector{Symbol}(undef, N)\nfor i in 1:N\n    obs = rand(triDist)\n    LL, UU = L(obs), U(obs)\n    bounds[i, :] = [LL UU]\n    if LL <= mu && mu <= UU\n        hits[i] = :blue\n    else\n        hits[i] = :red\n    end\nend\n\nfig, ax = rangebars(1:N, bounds[:, 1], bounds[:, 2], color=hits)\nhlines!(ax, mu; color=:green, label=\"Parameter value\")\naxislegend(ax)\nfig\n```\n\n### Hypothesis tests\n\nThe approach involves partitioning the parameter space $\\Theta$ into $\\Theta_0$ and $\\Theta_1$, and then, based on the sample, concluding whether one of the two hypotheses, $H_0 : \\theta \\in \\Theta_0$ or $H_1 : \\theta \\in \\Theta_1$ holds.\n\nThe hypothesis $H_0$ is called the **null hypothesis** and $H_1$ the **alternative hypothesis**. The former is the default hypothesis, and in carrying out hypothesis testing our general aim is to reject this hypothesis.\n\nSince our decision is based on a random sample, there is always a chance of making a mistakenly false conclusion. There are two types of errors that can be made in carrying out a hypothesis testing.\n\n![Type I and type II erorrs with their probabilities $\\alpha$ and $\\beta$ respectively](./figures/two_types_of_errors_in_carrying_out_hypothesis_testing.png){.lightbox fig-alt=\"Click to see a larger version of the image\" fig-align=\"center\"}\n\nNote that $1-\\beta$ is known as the **power** of the hypothesis test.\n\n::: {.callout-important title=\"How to understand: fail to reject $H_0$ and reject $H_0$\"}\n\nNote that in carrying out a hypothesis test, $\\alpha$ is typically specified, while power is not direcly controlled, but rather is influenced by the sample size and other factors.\n\nAn important point in terminology is that we don't use the phrase \"accept\" for the null hypothesis, rather we \"**fail to reject it**\" (if we stick with $H_0$) or \"**reject it**\" (if we choose $H_1$). This is because when we fail to reject $H_0$, we typically don't know the actual value of $\\beta$, hence we aren't able to put a level of certainty on $H_0$ being the case. In other words, when we fail to reject $H_0$, this doesn't mean that $H_0$ is true. We just haven't enough evidence to reject it based on the sample data. This means that we are still likely to make type II error with the probability $\\beta$ (i.e. in reality, $H_0$ is false, but we fail to reject it) though we fail to reject $H_0$. Because we usualy don't know the actual value of $\\beta$, we cannot give such an assertion that $H_0$ is true with a specified confidence level. This is why we just say that we fail to reject $H_0$ based on the sample data, instead of $H_0$ is true.\n\nHowever, if we do reject $H_0$, then by the design of hypothesis tests we can say that our error probability is bounded by $\\alpha$.\n\n:::\n\n#### How to design and perform a hypothesis testing\n\n1. Formulate the scientific question as a hypothesis by partitioning the parameter space $\\Theta$ into $\\Theta_0$ and $\\Theta_1$, and then formimg the two hypotheses $H_0 : \\theta \\in \\Theta_0$ and $H_1 : \\theta \\in \\Theta_1$.\n\n2. Define the **test statistic**, denoted $X^*$, as a function of the sample data.\n\nSince the test statistic follows some distribution under $H_0$, the next step is to consider **how likely** it is to observe the specific value ($X^*$) calculated from the sample data under $H_0$.\n\nTo this end, in setting up a hypothesis test, we typically choose a significance level $\\alpha$ (e.g. $0.05$ or $0.01$), which quantifies our level of tolerance for enduring a type I error. For example, setting $\\alpha = 0.01$ implies we wish to design a test where the probability of type I error is **at most** $0.01$ if $H_0$ holds. Clearly, a low $\\alpha$ is desirable, however, there are tradeoffs involved since seeking a very low $\\alpha$ will imply a high $\\beta$ (low power).\n\n3. Pick a significance level $\\alpha$.\n\nConsider that we have a series of sample observations distributed as continuous uniform between $0$ and some unknown upper bound $m$.\n\nSay we set\n\n$$\nH_0: m=1,\\ \\ \\ \\ H_1: m<1\n$$\n\nwith observations $X_1, ..., X_n$, one possible test statistic is the sample range:\n\n$$\nX^* = max(X_1, ..., X_n) - min(X_1, ..., X_n)\n$$\n\nAs is always the case, the test statistic is a random variable. Under $H_0$, we expect the distribution of $X^*$ to have support $[0, 1]$ with the most likely value being close to $1$. This is because low values of $X^*$ are less plausible under $H_0$. The explicit form of the distribution of $X^*$ can be analytically obtained however for simplicity we use a Monte Carlo simulation to estimate it and present the density.\n\nFor this case, it is sensible to reject $H_0$ if $X^*$ is small enough.\n\n4. Performing hypothesis testing.\n\nAt present, there are two alternatives for performing hypothesis tests:\n\n* Using **rejection region**:\n\nDenoting **quantiles** of this distribution by $q_0(u)$, then we set the rejection region as $R = [0, q_0(\\alpha)]$. Using Monte Carlo, we compute the rejection region where the critical value is the upper boundary $q_0(\\alpha)$ of the rejection region in this case. Note that computing the rejection region does not require any sample data as it is based on model assumptions and not the sample.\n\nThe decision rule for this hypothesis test is simple: compare the observed value of the test statistic, $x^*$, to the critical value $q_0(\\alpha)$ and reject $H_0$ if $x^*\\le q_0(\\alpha)$, otherwise do not reject.\n\n* Using **p-value**:\n\nWe collect the data and compute the observed value of the test statistic $x^*$. The p-value is then the maximal $\\alpha$ under which the test would be rejected with the observed test statistic. In other words, we find $p$ which solves $x^* = q_0(p)$. This is computed via $F_0(x^*)$, where $F(\\cdot)$ is the CDF of $X^*$.\n\nUsing the p-vaue approach, reporting a low p-value implies that we are very confident in rejecting $H_0$, while a high p-value implies we are not. The p-value approach can be used to decide whether $H_0$ should be rejected or not with a specified $\\alpha$. For this case, simply comparing $p$ and $\\alpha$, and reject $H_0$ if $p\\le \\alpha$.\n\n![Using rejection region or p-value to perform hypothesis testing](./figures/using_rejection_region_or_p-value_to_perform_hypothesis_testing.jpg){.lightbox fig-alt=\"Click to see a larger version of the image\" fig-align=\"center\"}\n\n```{julia}\nusing Distributions, CairoMakie, Random, Statistics\n\nRandom.seed!(1)\n\nn, N, alpha = 10, 10^7, 0.05\nmActual = 0.7\ndist0, dist1 = Uniform(0, 1), Uniform(0, mActual)\n\nts(sample) = maximum(sample) - minimum(sample)\n\nempiricalDistUnderH0 = [ts(rand(dist0, n)) for _ in 1:N]\nrejectionValue = quantile(empiricalDistUnderH0, alpha)\n\nsamples = rand(dist1, n)\ntestStat = ts(samples)\npValue = sum(empiricalDistUnderH0 .<= testStat) / N\n\nif testStat > rejectionValue\n    println(\"Do not reject: \", round(testStat; digits=4), \" > \", round(rejectionValue; digits=4))\nelse\n    println(\"Reject: \", round(testStat; digits=4), \" <= \", round(rejectionValue; digits=4))\nend\nprintln(\"p-value = \", round(pValue; digits=4))\n\nfig, ax = stephist(empiricalDistUnderH0; bins=100, color=:blue, normalization=:pdf)\nvlines!(ax, testStat; color=:red, label=\"Observed test statistic\")\nvlines!(ax, rejectionValue; color=:black, label=\"Critical value boundary\")\naxislegend(ax; position=:lt)\nax.title = L\"\\text{The distribution of the test statistic }X^*\\text{ under }H_0\"\nfig\n```\n\n#### Understand type I and type II errors\n\nWhen the alternative parameter spaces $\\Theta_0$ and $\\Theta_1$ are only comprised of a single point each, the hypothesis test is called a **simple hypothesis test**.\n\nConsider a container that conatins two identical types of pipes, except that one type weighs $15$ grams on average and the other $18$ grams on avearge. The standard deviation of the weights of both pipe types is $2$ grams.\n\nImagine that we sample a single pipe, and wish to determine its type. Denote the weight of this pipe by the random variable $X$. For this example, we devise the following statistical hypothesis test: $\\Theta_0 = {15}$ and $\\Theta_1 = {18}$. Now, given a threshold $\\tau$, we reject $H_0$ if $X > \\tau$, otherwise we retain $H_0$.\n\nIn this case, we can explicitly analyze the probabilities of both the type I and type II errors, $\\alpha$ and $\\beta$ respectively.\n\n```{julia}\nusing Distributions, StatsBase, CairoMakie\n\nmu0, mu1, sd, tau = 15, 18, 2, 17.5\ndist0, dist1 = Normal(mu0, sd), Normal(mu1, sd)\ngrid = 5:0.1:25\nh0grid, h1grid = tau:0.1:25, 5:0.1:tau\n\n# CCDF: complementary CDF\nprintln(\"Probability of Type I error: \", ccdf(dist0, tau))\nprintln(\"Probability of Type II error: \", cdf(dist1, tau))\n\nfig, ax = lines(grid, pdf.(dist0, grid); color=:red, label=\"Bolt type 15g\")\nband!(ax, h0grid, repeat([0], length(h0grid)), pdf.(dist0, h0grid); color=(:red, 0.2))\nlines!(ax, grid, pdf.(dist1, grid); color=:green, label=\"Bolt type 18g\")\nband!(ax, h1grid, repeat([0], length(h1grid)), pdf.(dist1, h1grid); color=(:green, 0.2))\nlinesegments!(ax, [tau, 25], [0, 0]; color=:black, label=\"Rejection region\", linewidth=5)\ntext!([15, 18, 15.5, 18.5], [0.21, 0.21, 0.02, 0.02]; text=[L\"H_0\", L\"H_1\", L\"\\beta\", L\"\\alpha\"], align=repeat([(:center, :center)], 4), fontsize=28)\naxislegend(ax)\nfig\n```\n\n#### The Receiver Operating Curve (ROC)\n\nIn the previous example, $\\tau = 17.5$ was arbitrarily chosen.\n\nClearly, if $\\tau$ was increased, the probability of making a type I error, $\\alpha$, would decrease, while the probability of making a type II error, $\\beta$, would increase. Conversely, if we decreased $\\tau$ the reverse would occur.\n\nHere we use the Receiver Operating Curve (ROC) to help to visualize the tradeoff between type I and type II errors. It allows us to visualize the error tradeoffs for all possible $\\tau$ values simutaneously for a particular alternative hypothesis $H_1$.\n\nBelow we plot the analytic coordinates of $(\\alpha(\\tau), 1-\\beta(\\tau))$. This is the ROC. It is a parametric plot of the probability of a type I error and power.\n\n```{julia}\nusing Distributions, StatsBase, CairoMakie\n\nmu0, mu1a, mu1b, mu1c, sd = 15, 16, 18, 20, 2\ntauGrid = 5:0.1:25\n\ndist0 = Normal(mu0, sd)\ndist1a, dist1b, dist1c = Normal(mu1a, sd), Normal(mu1b, sd), Normal(mu1c, sd)\n\nfalsePositive = ccdf.(dist0, tauGrid)\ntruePositiveA, truePositiveB, truePositiveC = ccdf.(dist1a, tauGrid), ccdf.(dist1b, tauGrid), ccdf.(dist1c, tauGrid)\n\nfig, ax = ablines(0, 1; color=:black, linestyle=:dash, label=L\"H_0 = H_1 = 15\")\nlines!(ax, falsePositive, truePositiveA; color=:blue, label=L\"H_{1a}: \\mu_1 = 16\")\nlines!(ax, falsePositive, truePositiveB; color=:red, label=L\"H_{1b}: \\mu_1 = 18\")\nlines!(ax, falsePositive, truePositiveC; color=:green, label=L\"H_{1c}: \\mu_1 = 20\")\nax.xlabel = L\"\\alpha\"\nax.ylabel = L\"\\text{Power }(1-\\beta)\"\naxislegend(ax; position=:rb)\nfig\n```\n\n#### A randomized hypothesis test\n\nWe now investigate the concept of a **randomization test**, which is a type of **non-parametric test**, i.e. a statistical test which does not require that we know what type of distribution the data comes from.\n\nA virtue of non-parametric tests is that they do not impose a specific model.\n\nConsider the following example, where a farmer wants to test whether a new fertilizer is effective at increasing the yield of her tomato plants. As an experiment, she took $20$ plants, kept $10$ as controls, and treated the remaining $10$ with fertilizer, After $2$ months, she harvested the plants and recorded the yield of each plant in kg as shown in the following table:\n\n![Yield in kg for $10$ plants with, and $10$ plants without fertilizer (control)](./figures/an_tomato_example_for_randomization_test.png){.lightbox fig-alt=\"Click to see a larger version of the image\" fig-align=\"center\"}\n\nIt can be observed that the group of plants treated with fertilizer have an average yield $0.494$ kg\ngreater than that of the control group. One could argue that this difference is due to the effects of\nthe fertilizer. We now investigate if this is a reasonable assumption. Let us assume for a moment that the fertilizer had no effect on plant yield ($H_0$) and that the result was simply due to random\nchance. In such a scenario, we actually have $20$ observations from the same group, and regardless of\nhow we arrange our observations, we would expect to observe similar results.\n\nHence, we can investigate the likelihood of this outcome occurring by random chance, by considering all possible combinations of $10$ samples from our group of $20$ observations, and counting how many of these combinations result in a difference in sample means greater than or equal to $0.494$ kg. The proportion of times this occurs is analogous to the likelihood that the difference we observe in our sample means was purely due to random chance. It is in a sense the p-value.\n\nBefore proceeding, we calculate the number of ways one can sample $r = 10$ unique items from $n = 20$ total, which is given by\n\n$$\n\\binom{20}{10} = 184,756\n$$\n\nHence, the number of possible combinations in our example is computationally manageable. Note that in a different situation where $n$ and $r$ would be bigger, e.g. $n = 40$ and $r = 20$, the number of\ncombinations would be too big for an exhaustive search (about $137$ billion). In such a case, a viable alternative is to randomly sample combinations for estimating the p-value.\n\n```{julia}\nusing Combinatorics, Statistics, DataFrames, CSV\n\ndf = CSV.read(\"./data/fertilizer.csv\", DataFrame)\ncontrol = df.Control\nfertilizer = df.FertilizerX\n\nsubGroups = collect(combinations([control; fertilizer], 10))\nmeanFert = mean(fertilizer)\npVal = sum([mean(i) >= meanFert for i in subGroups]) / length(subGroups)\nprintln(\"p-value = \", pVal)\n```\n\n### Bayesian statistics\n\nIn the Bayesian paradigm, the scalar or vector of parameter $\\theta$ is not assumed to exist **as some fixed unknown quantity** but instead is assumed to **follow a distribution**.\n\nThat is, the parameter itself, is a random variable, and the act of Bayesian inference is the process of obtaining more information about the distribution of $\\theta$.\n\nThis allows us to incorparate **prior beliefs** about the parameter before experience from new observations is taken into consideration.\n\nThe key objects at play are **the prior distribution** of the parameter and **the posterior distribution** of the parameter.\n\nThe former is postulated beforehand, or exists as a consequence of previous inference, while the latter captures the distribution of the parameter after observations are taken into consideration.\n\nThe relationship between the prior and the posterior is\n\n$$\n\\text{posterior} = \\frac{\\text{likelihood}\\times\\text{prior}}{evidence}\\ \\ \\ \\ \\text{or}\\ \\ \\ \\ f(\\theta|x) = \\frac{f(x|\\theta)\\times f(\\theta)}{\\int f(x|\\theta)f(\\theta)d\\theta}\n$$\n\nThis is nothing but Bayes's rule $P(B_i|A) = \\frac{P(A|B_i)P(B_i)}{\\sum_{j=1}^{n}P(A|B_j)P(B_j)}, A\\in \\cup_{i=1}^n B_i$ applied to densities.\n\nHere the prior distribution (density) is $f(\\theta)$, and the posterior distribution (density) is $f(\\theta|x)$.\n\nObserve that the denominator, known as **evidence** or **marginal likelihood**, is constant with respect to the parameter $\\theta$. This allows the equation to be written as\n\n$$\nf(\\theta|x) \\propto f(x|\\theta)\\times f(\\theta)\n$$\n\nHence, the posterior distribution can be easily obtained up to the normalizing constant (the evidence) by multiplying the prior with the likelihood.\n\nIn general, carrying out Beyesian inference involves the following steps:\n\n1. Assume some distributional model for the data based on the parameter $\\theta$ which is a random variable.\n\n2. Use previous inference experience, elicit an expert, or make an educated guess to determine a prior distribution $f(\\theta)$. The prior distribution might be parameterized by its own parameters, called **hyper-parameters**.\n\n3. Collect data $x$ and create an expression or a computational mechanism for the likelihood $f(x|\\theta)$ based on the distributional model chosen.\n\n4. Use the Bayes's rule applied to densities to obtain the posterior distribution of the parameters $f(\\theta|x)$.\n\nIn most cases, the evidence (the denominator) is not easily computable. Hence the posterior distribution is only available up to a normalizing constant. In some special cases, the form of the posterior distribution is the same as the prior distribution. In such cases, **conjugacy** holds, the prior is called a **conjugate prior**, and the hyper-parameters are updated from prior to posterior.\n\n5. The posterior distribution can then be used to make conclusions about the model.\n\nFor example, if a single specific parameter value is needed to make the model concrete, a Bayes estimate based on the posterior distribution, for example, the posterior mean, may be computed:\n\n$$\n\\hat{\\theta} = \\int \\theta f(\\theta|x) d\\theta\n$$\n\nFurther analyses such as obtaining credible intervals, similar to condidence intervals, may also be carried out.\n\n6. The model with $\\hat{\\theta}$ can then be used for making conclusions. Alternatively, a whole class of models based on the posterior distribution $f(\\hat{\\theta}|x)$ can be used. This often goes hand in hand with simulation as one is able to generate Monte Carlo samples from the posterior distribtion.\n\n#### A Poisson example\n\nConsider an example where an insurance company models **the number of weekly fires** in a city using a **Poisson distribution** with parameter $\\lambda$. Here, $\\lambda$ is also the expected number of fires per week.\n\nAssume that the following data is collected over a period of $16$ weeks:\n\n$$\nx = (x_1, ..., x_{16}) = (2, 1, 0, 0, 1, 0, 2, 2, 5, 2, 4, 0, 3, 2, 5, 0)\n$$\n\nEach data point indicates the number of fires per week.\n\nIn this case, the MLE is $\\hat{\\lambda} = 1.8125$ simply obtained by the sample mean. Hence, **in a frequentist approach**, after $16$ weeks, the distribution of the number of fires per week is modeled by a Poisson distribution with $\\lambda = 1.8125$. One can then obtain estimates for say, the probability of having more than $5$ fires in a given week as follows:\n\n$$\nP(\\text{fires per week} > 5) = 1 - \\sum_{k=0}^{5} e^{-\\lambda \\frac{\\lambda^k}{k!}} \\approx 0.0107\n$$\n\nHowever, the drawback of such an approach in estimating $\\lambda$ is that it didn't make use of previous information.\n\nSay that for example, further knowledge comes to light that the number of fires per week ranges between $0$ and $10$, and that the typical number is $3$ fires per week. In this case, one can assign a prior distribution to $\\lambda$ that captures this belief.\n\nIn this example, we can assume that we decide to use a triangular distribution which captures prior beliefs about the parameter $\\lambda$ well because it has a defined range and a defined mode.\n\nWith the prior assigned and the data collected, we can use the machinery of Bayesian inference.\n\nIn this case, the prior distibution of the parameter $\\lambda$ is the triangular distribution with the PDF:\n\n$$\nf(\\lambda) = \\begin{cases}\n\\frac{1}{15}\\lambda, & \\lambda \\in [0,3] \\\\\n\\frac{1}{35}(10-\\lambda), & \\lambda \\in (3, 10]\n\\end{cases}\n$$\n\nWith the $16$ observations, $x_1, ..., x_{16}$, the likelihood is\n\n$$\nf(x|\\lambda) = \\prod_{k=1}^{16} e^{-\\lambda \\frac{\\lambda^k}{k!}}\n$$\n\nHence, the posterior distribution $f(\\lambda|x) \\propto f(x|\\lambda)f(\\lambda)$, then dividing by the evidence,\n\n$$\n\\int_0^{10} f(x|\\lambda)f(\\lambda)d\\lambda\n$$\n\n```{julia}\nusing Distributions, CairoMakie\n\nalpha, beta = 8, 2\nprior(lam) = pdf(Gamma(alpha, 1 / beta), lam)\nd = [2, 1, 0, 0, 1, 0, 2, 2, 5, 2, 4, 0, 3, 2, 5, 0]\n\nlike(lam) = *([pdf(Poisson(lam), x) for x in d]...)\nposteriorUpToK(lam) = like(lam) * prior(lam)\n\ndelta = 10^-4\nlamRange = 0:delta:10\nK = sum([posteriorUpToK(lam) * delta for lam in lamRange])\nposterior(lam) = posteriorUpToK(lam) / K\n\nbayesEsitmate = sum([lam * posterior(lam) * delta for lam in lamRange])\n\nprintln(\"Computational Bayes Estimate: \", bayesEsitmate)\n\nfig, ax = lines(lamRange, prior.(lamRange); color=:blue, label=\"Prior distribution\")\nlines!(ax, lamRange, posterior.(lamRange); color=:red, label=\"Posterior distribution\")\nax.limits = (0, 10, 0, 1.2)\nax.xlabel = L\"\\lambda\"\nax.ylabel = \"Density\"\naxislegend(ax)\nfig\n```\n\n#### Congugate prior example\n\nThe gamma distribution is said to be a **conjugate** prior to the Poisson distribution, which means that both the prior distribution and the posterior distribution are gamma distributions when the data is distributional as a Poisson distribution. In this case, the hyper-parameters typically have a simple update law from prior to posterior.\n\nIn the case of a **gamma-Poisson conjugate pair**, assume the hyper-parameters of the prior to have $\\alpha$ (shape parameter) and $\\beta$ (rate parameter). We have\n\n$$\n\\begin{align}\n\\text{posterior} &\\propto \\text{likelihood}\\times \\text{prior} \\\\\n&\\propto \\left(\\prod_{k=1}^n e^{-\\lambda \\frac{\\lambda^k}{k!}}\\right) \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} e^{-\\beta\\lambda} \\\\\n&= \\lambda^{\\alpha+(\\sum_{k=1}^{n} x_k)-1}e^{-\\lambda(\\beta+n)} \\\\\n&\\propto \\text{gamma density with shape parameter } \\alpha + \\sum x_i \\text{ and rate parameter } \\beta+n\n\\end{align}\n$$\n\nThe hyper-parameter $\\alpha$ is updated to $\\alpha + \\sum x_i$ and the hyper-parameter $\\beta$ is updated to $\\beta + n$.\n\n```{julia}\nusing Distributions, CairoMakie\n\nalpha, beta = 8, 2\nprior(lam) = pdf(Gamma(alpha, 1 / beta), lam)\nd = [2, 1, 0, 0, 1, 0, 2, 2, 5, 2, 4, 0, 3, 2, 5, 0]\n\nlike(lam) = *([pdf(Poisson(lam), x) for x in d]...)\nposteriorUpToK(lam) = like(lam) * prior(lam)\n\ndelta = 10^-4\nlamRange = 0:delta:10\nK = sum([posteriorUpToK(lam) * delta for lam in lamRange])\nposterior(lam) = posteriorUpToK(lam) / K\n\nbayesEsitmate = sum([lam * posterior(lam) * delta for lam in lamRange])\n\nnewAlpha, newBeta = alpha + sum(d), beta + length(d)\nclosedFormBayesEstimate = mean(Gamma(newAlpha, 1 / newBeta))\n\nprintln(\"Computational Bayes Estimate: \", bayesEsitmate)\nprintln(\"Closed form Bayes Estimate: \", closedFormBayesEstimate)\n\nfig, ax = lines(lamRange, prior.(lamRange); color=:blue, label=\"Prior distribution\")\nlines!(ax, lamRange, posterior.(lamRange); color=:red, label=\"Posterior distribution\")\nax.limits = (0, 10, 0, 1.2)\nax.xlabel = L\"\\lambda\"\nax.ylabel = \"Density\"\naxislegend(ax)\nfig\n```\n\n#### Markov Chain Monte Carlo example\n\nIn cases where the dimension of the parameter space is high, carrying out straighforward integration is not possible.\n\nHowever, there are other ways of carrying out Bayesian inference.\n\nOne such popular way is by using algorithms that fall under the category known as **Markov Chain Monte Carlo** (MCMC).\n\nThe **Metropolis-Hastings** algorithm is one such popular MCMC algorithm.\n\nIt produces a series of samples $\\theta(1), \\theta(2), \\theta(3), ...$, where it is guaranteed that for large $t$, $\\theta(t)$ is distributed according to the posterior distribution.\n\nTechnically, the random sequence $\\{\\theta(t)\\}_{t=1}^\\infty$ is a Markov chain and it is guaranteed that the stationary distribution of this Markov chain is the specified posterior distribution. That is, the posterior distribution is an input parameter to the algorithm.\n\nThe major bebefits of Metropolis-Hastings and similar MCMC algorithms is that **they only use the ratios of the posterior on different parameter values**. For example, for parameter $\\theta_1$ and $\\theta_2$, the algorithm only uses the posterior distribution via the ratio\n\n$$\nL(\\theta_1, \\theta_2) = \\frac{f(\\theta_1|x)}{f(\\theta_2|x)}\n$$\n\nThis means that the normalizing constant (evidence) is not needed as it is implicitly canceled out. Thus, using $\\text{psoterior} \\propto \\text{likelihood}\\times \\text{prior}$ suffices.\n\nFurther to the posterior distribution, an additional input parameter to Metropolis-Hastings is the so-called **proposal density**, denoted by $q(\\cdot|\\cdot)$. This is a family of distributions where given a certain value of $\\theta_1$ taken as a parameter, the new value, say $\\theta_2$ is distributed with PDF\n\n$$\nq(\\theta_2|\\theta_1)\n$$\n\nThe idea of Metropolis-Hastings is to walk around the parameter space by randomly generating new values using $q(\\cdot|\\cdot)$.\n\nAt each step, some new values are accepted while others are not, all in a manner which ensures the desired limiting behavior. The algorithm specification is to accept with probability\n\n$$\nH = \\min\\left\\{1,\\ L(\\theta^*, \\theta(t)) \\frac{q(\\theta(t)|\\theta^*)}{q(\\theta^*|\\theta(t))}\\right\\}\n$$\n\nwhere $\\theta^*$ is the new proposed value, generated via $q(\\cdot|\\theta(t))$, and $\\theta(t)$ is the current value.\n\nWith each such iteration, the new value is accepted with probability $H$ and rejected otherwise. With certian technical requirements on the posterior and proposal densities, the theory of Markov chains then guarantees that the stationary distribution of the sequence $\\{\\theta(t)\\}$ is the posterior distribution.\n\nDifferent variants of the Metropolis-Hastings algorithm employ different types of proposal densities. There are also generalizations and extensions that we don't discuss here, such as *Gibbs Sampling* and *Hamiltonian Monte Carlo*.\n\nAs an example, we use the *folded normal distribution* as a proposal density. This distribution is achieved by taking a normal random variable $X$ with mean $\\mu$ and variance $\\sigma^2$ and considering $Y=|X|$. In this case, the PDF of $Y$ is\n\n$$\nf(y) = \\frac{1}{\\sigma 2\\pi} \\left(e^{-\\frac{(y-\\mu)^2}{2\\sigma^2}} + e^{-\\frac{(y+\\mu)^2}{2\\sigma^2}}\\right)\n$$\n\nIt supports to generate the non-negative parameters in question.\n\n```{julia}\nusing Distributions, CairoMakie\n\nalpha, beta = 8, 2\n# prior with Gamma distribution which has two hyper-parameters α and β\nprior(lam) = pdf(Gamma(alpha, 1 / beta), lam)\nd = [2, 1, 0, 0, 1, 0, 2, 2, 5, 2, 4, 0, 3, 2, 5, 0]\n\n# data with Poisson distribution which has a parameter λ\nlike(lam) = *([pdf(Poisson(lam), x) for x in d]...)\n# posterior ∝ likelihood * prior\nposteriorUpToK(lam) = like(lam) * prior(lam)\n\nsig = 0.5\n# use the folded normal distribution as the proposal density\n# with each parameter θ1, it produces a new parameter θ2\nfoldedNormalPDF(x, mu) = (1 / sqrt(2 * pi * sig^2)) * (exp(-(x - mu)^2 / 2sig^2) + exp(-(x + mu)^2 / 2sig^2))\nfoldedNormalRV(mu) = abs(rand(Normal(mu, sig)))\n\nfunction sampler(piProb, qProp, rvProp)\n    lam = 1\n    warmN, N = 10^5, 10^6\n    samples = zeros(N - warmN)\n\n    for t in 1:N\n        while true\n            lamTry = rvProp(lam)\n            Lo = piProb(lamTry) / piProb(lam)\n            H = min(1, Lo * qProp(lam, lamTry) / qProp(lamTry, lam))\n            if rand() < H\n                lam = lamTry\n                if t > warmN\n                    samples[t-warmN] = lam\n                end\n                break\n            end\n        end\n    end\n    return samples\nend\n\nmcmcSamples = sampler(posteriorUpToK, foldedNormalPDF, foldedNormalRV)\nprintln(\"MCMC Bayes Estimate: \", mean(mcmcSamples))\n\nfig, ax = stephist(mcmcSamples; bins=100, color=:black, normalization=:pdf, label=\"Histogram of MCMC samples\")\n\nlamRange = 0:0.01:10\nlines!(ax, prior.(lamRange); color=:blue, label=\"Prior distribution\")\n\nclosedFormPosterior(lam) = pdf(Gamma(alpha + sum(d), 1 / (beta + length(d))), lam)\nlines!(ax, lamRange, closedFormPosterior.(lamRange); color=:red, label=\"Posterior distribution\")\n\nax.limits = (0, 10, 0, 1.2)\nax.xlabel = L\"\\lambda\"\nax.ylabel = \"Density\"\naxislegend(ax)\nfig\n```\n\n## Confidence intervals\n\nIn the setting of **symmetric** sampling distributions, a typical formula for the confidence interval $[L, U]$ is of the form\n\n$$\n\\hat{\\theta} \\pm K_\\alpha s_\\text{err}\n$$\n\nHere, $\\hat{\\theta}$ is typically the point estimate for the parameter in question, $s_\\text{err}$ is some measure or estimate of the variability (e.g. standard error) of the parameter, and $K_\\alpha$ is a constant which depends on the model at hand and on $\\alpha$. Typically by decreasing $\\alpha \\rightarrow 0$, we have that $K_\\alpha$ increases, implying a wider confidence interval. In addition, the specific form of $K_\\alpha$ often depends on conditions such as\n\n- **Sample size**: is the sample size small or large.\n\n- **Variance**: is the variance $\\sigma^2$ known or unknown.\n\n- **Distribution**: is the population assumed normally distributed or not.\n\n### Single sample confidence intervals for the mean\n\nAssume that we want to estimate the population mean $\\mu$ using a random sample, $X_1, ..., X_n$.\n\nAs mentioned before, an unbiased point estimate for the mean is the sample mean $\\overline{X}$, so a typical formula for the confidence interval of the mean is\n\n$$\n\\overline{X} \\pm K_\\alpha \\frac{S}{\\sqrt{n}}\n$$\n\n#### Population variance known\n\nIf we assume that the population variance $\\sigma^2$ is known and the data is normally distributed, then the sample mean $\\overline{X}$ is normally distributed with mean $\\mu$ and variance $\\frac{\\sigma^2}{n}$. This yields\n\n$$\nP\\left( \\mu - z_{1-\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}} \\le \\overline{X} \\le \\mu + z_{1-\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}} \\right) = 1-\\alpha\n$$\n\nwhere $z_{1-\\frac{\\alpha}{2}}$ is the $1-\\frac{\\alpha}{2}$ quantile of the standard normal distribution.\n\nBy rearranging the inequalities inside the probability statement above, we obtain the following confidence interval formula\n\n$$\n\\bar{x} \\pm z_{1-\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}}\n$$\n\nIn practice, $\\sigma^2$ is rarely known; hence, it is tempting to replace $\\sigma$ by $s$ (sample standard deviation) in the formula above. **Such a replacement is generally fine for large samples**.\n\nIn addition, the validity of the normality assumption should also be considered. In cases where the data is not normally distributed, the probability statement above only approximately holds. However, as $n \\rightarrow \\infty$, it quickly becomes precise due to the central limit theorem.\n\n```{julia}\nusing Distributions, HypothesisTests\n\nd = [\n    53.35674558144255,\n    53.45887516134873,\n    52.282838627926125,\n    52.98746570643515,\n    51.993167774733486,\n    53.373333606198,\n    55.75410538860477,\n    50.279496381439365,\n    53.6359586914001,\n    53.517705831707495,\n    53.70044994508253,\n    54.15592592604583,\n    53.55054914606446,\n    52.37319589109419,\n    53.4900750059897,\n    52.939458524079605,\n    52.16761562743534,\n    50.87140009591033,\n    53.144919157924924,\n    52.09084035473537,\n]\nxBar, n = mean(d), length(d)\nsig = 1.2\nalpha = 0.1\nz = quantile(Normal(), 1 - alpha / 2)\n\nprintln(\"Calculating formula: \", (xBar - z * sig / sqrt(n), xBar + z * sig / sqrt(n)))\nprintln(\"Using confint() function: \", confint(OneSampleZTest(xBar, sig, n); level=1 - alpha))\n```\n\n::: {.callout-note}\n\nAssume $U \\sim N(0, 1)$, and $X \\sim N(\\mu, \\sigma)$, we have\n\n$$\nU = \\frac{X-\\mu}{\\sigma}\n$$\n\n$$\nX = \\mu + U\\sigma\n$$\n\n$$\nP(-z_{1-\\frac{\\alpha}{2}} \\le U \\le z_{1-\\frac{\\alpha}{2}}) = 1-\\alpha\n$$\n\n$$\nP(\\mu - z_{1-\\frac{\\alpha}{2}}\\sigma \\le X \\le \\mu + z_{1-\\frac{\\alpha}{2}}\\sigma) = 1-\\alpha\n$$\n\n:::\n\n#### Population variance unknown\n\nIn cases where the population variance is unknown, if we replace $\\sigma$ by the sample standard deviation $s$, we can use the T-distribution instead of the normal distribution to obtain the confidence interval for the mean\n\n$$\n\\bar{x} \\pm t_{1-\\frac{\\alpha}{2}, n-1} \\frac{s}{\\sqrt{n}}\n$$\n\nwhere $t_{1-\\frac{\\alpha}{2}, n-1}$ is the $1-\\frac{\\alpha}{2}$ quantile of a T-distribution with $n-1$ degrees of freedom.\n\nFor small samples, the replacement of $z_{1-\\frac{\\alpha}{2}}$ by $t_{1-\\frac{\\alpha}{2}, n-1}$ significantly affects the width of the confidence interval, as for the same value of $\\alpha$, the T case is wider.\n\nHowever, as $n \\rightarrow \\infty$, we have, $t_{1-\\frac{\\alpha}{2}, n-1} \\rightarrow z_{1-\\frac{\\alpha}{2}}$.\n\n```{julia}\nusing Distributions, HypothesisTests\n\nd = [\n    53.35674558144255,\n    53.45887516134873,\n    52.282838627926125,\n    52.98746570643515,\n    51.993167774733486,\n    53.373333606198,\n    55.75410538860477,\n    50.279496381439365,\n    53.6359586914001,\n    53.517705831707495,\n    53.70044994508253,\n    54.15592592604583,\n    53.55054914606446,\n    52.37319589109419,\n    53.4900750059897,\n    52.939458524079605,\n    52.16761562743534,\n    50.87140009591033,\n    53.144919157924924,\n    52.09084035473537,\n]\nxBar, n = mean(d), length(d)\ns = std(d)\nalpha = 0.1\nt = quantile(TDist(n - 1), 1 - alpha / 2)\n\nprintln(\"Calculating formula: \", (xBar - t * s / sqrt(n), xBar + t * s / sqrt(n)))\nprintln(\"Using confint() function: \", confint(OneSampleTTest(xBar, s, n); level=1 - alpha))\n```\n\n### Two sample confidence intervals for the difference in means\n\nIt is often of interest to estimate the difference between the population means, $\\mu_1 - \\mu_2$.\n\nFirst we collect two random samples, $x_{i,1}, ..., x_{i,n}$ for $i = 1,2$, each with the sample mean $\\bar{x}_i$ and sample standard deviation $s_i$. In addition, the difference of sample means, $\\bar{x}_1 - \\bar{x}_2$ serves as a point estimate for the difference in population means, $\\mu_1 - \\mu_2$.\n\nA confidence interval for $\\mu_1 - \\mu_2$ around the point estimate $\\bar{x}_1 - \\bar{x}_2$ is then constructed via the same process seen previously\n\n$$\n\\bar{x}_1 - \\bar{x}_2 \\pm K_\\alpha s_\\text{err}\n$$\n\n#### Population variances known\n\nIn cases where the population variances are known, we may explicitly compute\n\n$$\nVar(\\overline{X}_1 - \\overline{X}_2) = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\n$$\n\nHence, the standard error is given by\n\n$$\ns_\\text{err} = \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\n$$\n\nWhen combined with the assumption that the data is normally distributed, we can derive the following confidence interval\n\n$$\n\\bar{x}_1 - \\bar{x}_2 \\pm z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\n$$\n\n#### Population variances unknown and assumed equal\n\nIn cases where the population variances are unknown but assumed equal, denoted by $\\sigma^2$. Based on this assumption, it is sensible to use both sample variances to estimate $\\sigma^2$. This estimated variance using both samples is known as the pooled sample variance, and is given by\n\n$$\nS_p^2 = \\frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1+n_2-2}\n$$\n\nIn fact, it is a weighted average of the sample variances of the individual samples.\n\nIn this case, it can be shown that\n\n$$\nT = \\frac{\\overline{X}_1 - \\overline{X}_2 - (\\mu_1 - \\mu_2)}{S_\\text{err}}\n$$\n\nis distributed as a T-distribution with $n_1+n_2-2$ degreees of freedom, where the standard error is\n\n$$\nS_\\text{err} = S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n$$\n\nHence, we arrive at the following confidence interval\n\n$$\n\\bar{x}_1 - \\bar{x}_2 \\pm t_{1-\\frac{\\alpha}{2}, n_1+n_2-2} S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n$$\n\n```{julia}\nusing CSV, Distributions, HypothesisTests\n\nd1 = [\n    53.35674558144255,\n    53.45887516134873,\n    52.282838627926125,\n    52.98746570643515,\n    51.993167774733486,\n    53.373333606198,\n    55.75410538860477,\n    50.279496381439365,\n    53.6359586914001,\n    53.517705831707495,\n    53.70044994508253,\n    54.15592592604583,\n    53.55054914606446,\n    52.37319589109419,\n    53.4900750059897,\n    52.939458524079605,\n    52.16761562743534,\n    50.87140009591033,\n    53.144919157924924,\n    52.09084035473537,\n]\n\nd2 = [\n    48.23239011,\n    52.2735178,\n    52.07209917,\n    51.8813638,\n    50.89860065,\n    53.13910845,\n    50.88296219,\n    49.80725709,\n    49.04791179,\n    50.91491626,\n    50.73578183,\n    47.6154076,\n    50.89317122,\n    52.95593896,\n    51.90831274,\n    52.22159829,\n    51.60575821,\n    49.96704471,\n]\n\nxBar1, xBar2 = mean(d1), mean(d2)\nn1, n2 = length(d1), length(d2)\nalpha = 0.05\nt = quantile(TDist(n1 + n2 - 2), 1 - alpha / 2)\n\ns1, s2 = std(d1), std(d2)\nsP = sqrt(((n1 - 1) * s1^2 + (n2 - 1) * s2^2) / (n1 + n2 - 2))\n\nprintln(\"Calculating formula: \", (xBar1 - xBar2 - t * sP * sqrt(1 / n1 + 1 / n2), xBar1 - xBar2 + t * sP * sqrt(1 / n1 + 1 / n2)))\nprintln(\"Using confint() function: \", confint(EqualVarianceTTest(d1, d2); level=1 - alpha))\n```\n\n#### Population variances unkown and not assumed equal\n\nIn cases where the population variances are unknown and not assumed equal, the estimate for $S_\\text{err}$ is given by\n\n$$\nS_\\text{err} = \\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}\n$$\n\nHence, in this case the statistic $T = \\frac{(\\overline{X}_1 - \\overline{X}_2) - (\\mu_1 - \\mu_2)}{S_\\text{err}}$ is adapted to\n\n$$\nT = \\frac{\\overline{X}_1 - \\overline{X}_2 - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}}\n$$\n\nIt turns out that the above formula is only T-distributed with $n_1+n_2-2$ degrees of freedom **if the variances are equal**, otherwise, it isn't.\n\nHowever, the *Satterthwaite approximation* suggests that $T\\ \\ \\widetilde{approx}\\ \\ t(v)$, where the degrees of freedom $v$ is\n\n$$\nv = \\frac{\\left(\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}\\right)^2}{\\frac{\\left(s_1^2/n_1\\right)^2}{n_1-1} + \\frac{\\left(s_2^2/n_2\\right)^2}{n_2-1}}\n$$\n\nHence, we arrive at the confidence interval\n\n$$\n\\bar{x}_1 - \\bar{x}_2 \\pm t_{1-\\frac{\\alpha}{2}, v} \\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}\n$$\n\n```{julia}\nusing CSV, Distributions, HypothesisTests\n\nd1 = [\n    53.35674558144255,\n    53.45887516134873,\n    52.282838627926125,\n    52.98746570643515,\n    51.993167774733486,\n    53.373333606198,\n    55.75410538860477,\n    50.279496381439365,\n    53.6359586914001,\n    53.517705831707495,\n    53.70044994508253,\n    54.15592592604583,\n    53.55054914606446,\n    52.37319589109419,\n    53.4900750059897,\n    52.939458524079605,\n    52.16761562743534,\n    50.87140009591033,\n    53.144919157924924,\n    52.09084035473537,\n]\n\nd2 = [\n    48.23239011,\n    52.2735178,\n    52.07209917,\n    51.8813638,\n    50.89860065,\n    53.13910845,\n    50.88296219,\n    49.80725709,\n    49.04791179,\n    50.91491626,\n    50.73578183,\n    47.6154076,\n    50.89317122,\n    52.95593896,\n    51.90831274,\n    52.22159829,\n    51.60575821,\n    49.96704471,\n]\n\nxBar1, xBar2 = mean(d1), mean(d2)\ns1, s2 = std(d1), std(d2)\nn1, n2 = length(d1), length(d2)\nalpha = 0.05\n\nv = (s1^2 / n1 + s2^2 / n2)^2 / ((s1^2 / n1)^2 / (n1 - 1) + (s2^2 / n2)^2 / (n2 - 1))\nt = quantile(TDist(v), 1 - alpha / 2)\n\nprintln(\"Calculating formula: \", (xBar1 - xBar2 - t * sqrt((s1^2 / n1 + s2^2 / n2)), xBar1 - xBar2 + t * sqrt((s1^2 / n1 + s2^2 / n2))))\nprintln(\"Using confint() function: \", confint(UnequalVarianceTTest(d1, d2); level=1 - alpha))\n```\n\n#### More on the Satterthwaite approximation\n\nObserve that both sides of the \"distributed as\" ($\\sim$) symbol are random variables which depdend on the same random experiment.\n\nHence, the statement can be presented generally, as a case of the following format:\n\n$$\nX(\\omega) \\sim F_{h(\\omega)}\n$$\n\nwhere $\\omega$ is a point in the sample space.\n\nHere, $X(\\omega)$ is a random variabe, and $F$ is a distribution that depends on a parameter $h$, which depends on $\\omega$.\n\nIn our case of the Satterthwaite approximation, $h$ is $v$, defined above.\n\n::: {.callout-note title=\"Why both sides of the $\\sim$ symbol are random variables which depend on the same random experiment\"}\n\nWhen we had finished a random experiment, we obtained a sample $\\omega$ from the sample space.\n\nThen, we can derive some statistic ($X(\\omega)$) from the sample, which means that $X$ is a random variable. In addition, under the same experiment, $X$ should be distributed as some distribution $F$, which may be defined by some parameter $h$, saying the degrees of freedom. Obviously, $h$ itself depends on the same sample obtained from the sample space before.\n\n:::\n\nBy using the inverse probability transformation, the above formula is equivalent to\n\n$$\nF_{h(\\omega)}\\left(X(\\omega)\\right) \\sim \\text{Uniform}(0, 1)\n$$\n\nThis means that the Satterthwaite approximation is a better approximative distribution close to the true distribution which $T$ calculated in the case of population variances unknown and not assumed equal is distributed as in comparison with the alternative of treating $h$ as simply dependent on the number of observations made ($n_1+n_2-2$).\n\nWe can make a simple validation using Q-Q plot: $t(v)$ is more similar with the true distribution of $T$ than $t(n_1+n_2-2)$\n\n```{julia}\nusing Distributions, Statistics, Random, CairoMakie\n\nRandom.seed!(0)\n\nmu1, sig1, n1 = 0, 2, 8\nmu2, sig2, n2 = 0, 30, 15\n\ndist1 = Normal(mu1, sig1)\ndist2 = Normal(mu2, sig2)\n\nN = 10^6\ntdArray = Array{Tuple{Float64,Float64}}(undef, N)\n\nfunc(s1, s2, n1, n2) = (s1^2 / n1 + s2^2 / n2)^2 / ((s1^2 / n1)^2 / (n1 - 1) + (s2^2 / n2)^2 / (n2 - 1))\n\nfor i in 1:N\n    x1Data = rand(dist1, n1)\n    x2Data = rand(dist2, n2)\n    x1Bar, x2Bar = mean(x1Data), mean(x2Data)\n    s1, s2 = std(x1Data), std(x2Data)\n    tStatistics = (x1Bar - x2Bar) / sqrt(s1^2 / n1 + s2^2 / n2)\n    tdArray[i] = (tStatistics, func(s1, s2, n1, n2))\nend\nsort!(tdArray, by=first)\n\ninvVal(v, i) = quantile(TDist(v), i / (N + 1))\n\nxCoords = Array{Float64}(undef, N)\nyCoords1 = Array{Float64}(undef, N)\nyCoords2 = Array{Float64}(undef, N)\n\nfor i in 1:N\n    xCoords[i] = first(tdArray[i])\n    yCoords1[i] = invVal(last(tdArray[i]), i)\n    yCoords2[i] = invVal(n1 + n2 - 2, i)\nend\n\nfig, ax = qqplot(xCoords, yCoords1; color=:blue, label=\"Calculated v\", qqline=:identity)\nqqplot!(ax, xCoords, yCoords2; color=:red, label=\"Fixed v\")\naxislegend(ax; position=:lt)\nfig\n```\n\n### Confidence intervals for proportions\n\nIn certain inference settings the parameter of interest is a population proportion.\n\nWhen carrying out inference for a proportion we assume that there exists some unknown population proportion $p \\in (0, 1)$.\n\nWe then sample an i.i.d. sample of observations $I_1, ..., I_n$, where for the $i$'th observation, $I_i = 0$ if the event in question does not happen, and $I_i = 1$ if the event occurs.\n\nA natural estimator for the proportion is then the sample mean of $I_1, ..., I_n$, which we denote\n\n$$\n\\hat{p} = \\frac{\\sum_{i=1}^{n}I_i}{n}\n$$\n\nPlease note that $\\overline{X} \\sim N(\\mu, \\frac{\\sigma^2}{n})$ as $n \\rightarrow \\infty$, so is $\\hat{p}$.\n\nNow observe that each $I_i$ is a Bernoulli random variable with success probability $p$. Under the i.i.d assumption this means that the numerator of the above formula (i.e. $\\sum_{i=1}^{n}I_i$) is binomially distributed with parameters $n$ and $p$. Hence\n\n$$\nE\\left[\\sum_{i=1}^{n}I_i\\right] = np\n$$\n\nand\n\n$$\n\\text{Var}(\\sum_{i=1}^{n}I_i) = np(1-p)\n$$\n\nSo we have\n\n$$\nE(\\hat{p}) = p\n$$\n\nand\n\n$$\n\\text{Var}(\\hat{p}) = \\frac{p(1-p)}{n}\n$$\n\nHence, $\\hat{p}$ is an **unbiased** and **consistent** estimator of $p$. That is, **on average** $\\hat{p}$ estimates $p$ perfectly (unbiased), and if **more** observations are collected the variance of the estimator vanishes and then $\\hat{p} \\rightarrow p$ (consistent).\n\nFurthermore, we can use the central limit theorem to create a normal approximation for the distribution of $\\hat{p}$ and yield an approximate condidence interval.\n\nDue to the fact $\\hat{p} \\sim N(p, \\frac{p(1-p)}{n})$ as $n \\rightarrow \\infty$, we have\n\n$$\n\\tilde{Z}_n = \\frac{\\hat{p}-p}{\\sqrt{\\frac{p(1-p)}{n}}}\n$$\n\nwhich is a random variable that approximately follows a standard normal distribution. The approximation becomes exact as $n$ grows. Due to the fact that $\\hat{p}$ is an unbiased and consistent estimator of $p$, it's reasonable that we replace $p$ with $\\hat{p}$, and then we have\n\n$$\n\\hat{Z}_n = \\frac{\\hat{p}-p}{\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}}\n$$\n\nSo we have\n\n$$\nP(z_\\frac{\\alpha}{2} \\le \\hat{Z}_n \\le z_{1-\\frac{\\alpha}{2}}) \\approx 1-\\alpha\n$$\n\nand then along with the fact that $z_\\frac{\\alpha}{2} = -z_{1-\\frac{\\alpha}{2}}$ as follows\n\n$$\n\\begin{align}\n1-\\alpha &\\approx P(-z_{1-\\frac{\\alpha}{2}} \\le \\hat{Z}_n \\le z_{1-\\frac{\\alpha}{2}}) \\\\\n&= P(-z_{1-\\frac{\\alpha}{2}} \\le \\frac{\\hat{p}-p}{\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}} \\le z_{1-\\frac{\\alpha}{2}}) \\\\\n&= P(\\hat{p} - z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\le p \\le \\hat{p} + z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}})\n\\end{align}\n$$\n\nWe thus arrive the following approximate confidence interval for proportions\n\n$$\n\\hat{p} \\pm z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n$$\n\nSimilarly, we have the confidence interval for the case of two populations\n\n$$\n\\hat{p}_1 - \\hat{p}_2 \\pm z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n}}\n$$\n\n```{julia}\nusing CSV, DataFrames, CategoricalArrays, Distributions\n\ndat = CSV.read(\"./data/purchaseData.csv\", DataFrame)\nprintln(\"Levels of Grade: \", levels(dat.Grade))\nprintln(\"Data points: \", nrow(dat))\n\nn = sum(.!(ismissing.(dat.Grade)))\nprintln(\"Non-missing data points: \", n)\ndat2 = dropmissing(dat[:, [:Grade]], :Grade)\n\ngradeInQuestion = \"E\"\nindicatorVector = dat2.Grade .== gradeInQuestion\nnumSuccess = sum(indicatorVector)\nphat = numSuccess / n\nserr = sqrt(phat * (1 - phat) / n)\n\nalpha = 0.05\nconfidencePercent = 100 * (1 - alpha)\nzVal = quantile(Normal(), 1 - alpha / 2)\nconfInt = (phat - zVal * serr, phat + zVal * serr)\n\nprintln(\"\\nOut of $n non-missing observations, $numSuccess are at level $gradeInQuestion.\")\nprintln(\"Hence a point estimate for the proportion of grades at level $gradeInQuestion is $phat.\")\nprintln(\"A $confidencePercent% confidence interval for the proportion of level $gradeInQuestion is:\\n$confInt\")\n```\n\n#### Sample size planing\n\nDenote the confidence interval of the proportion as $\\hat{p} \\pm E$ where $E$ is the margin of error or half the width of the confidence interval, denoted by\n\n$$\nE = z_{1-\\frac{\\alpha}{2}}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n$$\n\nWe may often want to plan an experiment or a sampling scheme such that $E$ is not too wide.\n\nSay we want $E \\le \\epsilon$ with the confidence probability $1-\\alpha$, we have\n\n$$\nE = z_{1-\\frac{\\alpha}{2}}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\le \\epsilon\n$$\n\nGiven that $x(1-x)$ is maximized at $x=1/2$ with a maximal value of $1/4$. Hence,\n\n$$\nE \\le \\frac{z_{1-\\frac{\\alpha}{2}}}{2\\sqrt{n}} \\le \\epsilon\n$$\n\nFinally, we have\n\n$$\nn \\ge \\frac{z_{1-\\frac{\\alpha}{2}}^2}{4\\epsilon^2}\n$$\n\n#### Validity of the approximation\n\nIn many cases, this confidence interval approximation for the proportion works well, however for small sample sizes $n$ or values of $p$ near $0$ or $1$, this is often too crude of an approximation. A consequence is that one may obtain a confidence interval that isn't actually a $1-\\alpha$ confidence interval, but rather has a different coverage probability.\n\nOne common rule of thumb used to decide if the confidence interval for the proportion is valid is to require that both the product $np$ and the product $n(1-p)$ be at least $10$.\n\nHere, we expore some combinations of $n$ and $p$. For each combination, we use $N$ Monte Carlo expriments and calculate the following\n\n$$\n(1-\\alpha) - \\frac{1}{N} \\sum_{k=1}^{N}\\mathbf{1}\\left\\{p \\in \\left[\\hat{p} - z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}, \\hat{p} + z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\right]\\right\\}\n$$\n\nThis estimated difference of the actual converage probability of the confidence interval and the desired confidence level $1-\\alpha$ is a measure of the accuracy of the confidence level. We expect this difference to be almost $0$ if the approximation is good. Otherwise, a higher absolute difference is oberved.\n\n```{julia}\nusing Random, Distributions, CairoMakie\n\nN = 5e3\nalpha = 0.05\nconfLevel = 1 - alpha\nz = quantile(Normal(), 1 - alpha / 2)\n\nfunction randCI(n, p)\n    sample = rand(n) .< p\n    pHat = sum(sample) / n\n    serr = sqrt(pHat * (1 - pHat) / n)\n    (pHat - z * serr, pHat + z * serr)\nend\ncover(p, ci) = ci[1] <= p && p <= ci[2]\n\npGrid = 0.1:0.01:0.9\nnGrid = 5:1:50\nerrs = zeros(length(nGrid), length(pGrid))\n\nfor i in 1:length(nGrid)\n    for j in 1:length(pGrid)\n        Random.seed!(1234)\n        n, p = nGrid[i], pGrid[j]\n        coverageRatio = sum([cover(p, randCI(n, p)) for _ in 1:N]) / N\n        errs[i, j] = confLevel - coverageRatio\n    end\nend\n\nfig, ax1, hm1 = heatmap(pGrid, nGrid, errs'; colormap=to_colormap([\"white\", \"black\"]))\nax2, hm2 = heatmap(fig[2, 1], pGrid, nGrid, errs' .<= 0.04; colormap=to_colormap([\"black\", \"white\"]))\nColorbar(fig[1, 2], hm1)\nfig\n```\n\n### Confidence interval for the variance of a normal population\n\nFirst, a point estimator of the population variance is the sample variance\n\n$$\ns^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(X_i-\\overline{X})^2\n$$\n\nWe have known that $\\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi_{n-1}^2$. So denoting the $\\gamma$-quantile of this distribution via $\\chi_{\\gamma,n-1}^2$, we have\n\n$$\nP(\\chi_{\\frac{\\alpha}{2},n-1}^2 \\le \\frac{(n-1)s^2}{\\sigma^2} \\le \\chi_{1-\\frac{\\alpha}{2},n-1}^2) = 1-\\alpha\n$$\n\nThen we have\n\n$$\nP(\\frac{(n-1)s^2}{\\chi_{1-\\frac{\\alpha}{2},n-1}^2} \\le \\sigma^2 \\le \\frac{(n-1)s^2}{\\chi_{\\frac{\\alpha}{2},n-1}^2}) = 1-\\alpha\n$$\n\nNote: this equality holds only if the data is normally distributed.\n\n```{julia}\nusing Distributions\n\ndat = rand(Normal(0, 1), 1000)\nn, s, alpha = length(dat), std(dat), 0.05\n\nci = ((n - 1) * s^2 / quantile(Chisq(n - 1), 1 - alpha / 2),\n    (n - 1) * s^2 / quantile(Chisq(n - 1), alpha / 2))\n\nprintln(\"Point estimate for the variance: \", s^2)\nprintln(\"Confidence interval for the variance: \", ci)\n```\n\n#### Sensitivity of the normal assumption\n\nAs mentioned before, this confidence interval for the variance of a normal population only holds for normally distributed population and is more sensitive about the normal assumption than the other confidence intervals constructed before.\n\nWe'll find that the sample variance distributions of a normal distribution and a logistic distribution are quite different, though they have the same pupulation mean $\\mu$ and population variance $\\sigma^2$, and the PDF's curve shape of the logistic distribution is somewhat similar with the normal.\n\nThe logistic distribution is defined by the location and scale parameters $\\mu$ and $\\eta$, and the PDF is\n\n$$\nf(x) = \\frac{e^{-\\frac{x-\\mu}{\\eta}}}{\\eta(1+e^{-\\frac{x-\\mu}{\\eta}})^2}\n$$\n\nwith mean $\\mu$ and variance $\\sigma^2 = \\eta^2\\pi^2/3$.\n\n```{julia}\nusing Distributions, CairoMakie\n\nmu, sig = 2, 3\neta = sqrt(3) * sig / pi\n\nn, N = 15, 10^7\ndNormal = Normal(mu, sig)\ndLogistic = Logistic(mu, eta)\nxGrid = -8:0.1:12\n\nsNormal = [var(rand(dNormal, n)) for _ in 1:N]\nsLogistic = [var(rand(dLogistic, n)) for _ in 1:N]\n\nfig = Figure()\nax1 = Axis(fig[1, 1];\n    xlabel=\"x\",\n    ylabel=\"Density\")\nlines!(ax1, xGrid, pdf.(dNormal, xGrid); color=:blue, label=\"Normal\")\nlines!(ax1, xGrid, pdf.(dLogistic, xGrid); color=:red, label=\"Logistic\")\naxislegend(ax1)\nax2 = Axis(fig[2, 1],\n    xlabel=\"Sample Variance\",\n    ylabel=\"Density\",\n    limits=(0, 30, 0, 0.14))\nstephist!(ax2, sNormal; bins=200, color=:blue, normalization=:pdf, label=\"Normal\")\nstephist!(ax2, sLogistic; bins=200, color=:red, normalization=:pdf, label=\"Logistic\")\naxislegend(ax2)\nfig\n```\n\nIn addition, we can check the actual confidence interval coverage under different model assumptions:\n\n```{julia}\nusing Distributions, CairoMakie\n\nmu, sig = 2, 3\neta = sqrt(3) * sig / pi\nn, N = 15, 10^4\ndNormal = Normal(mu, sig)\ndLogistic = Logistic(mu, eta)\nalphaUsed = 0.001:0.001:0.1\n\nfunction alphaSimulator(dist, n, alpha)\n    popVar = var(dist)\n    coverageCount = 0\n    for _ in 1:N\n        sVar = var(rand(dist, n))\n        Lo = (n - 1) * sVar / quantile(Chisq(n - 1), 1 - alpha / 2)\n        Up = (n - 1) * sVar / quantile(Chisq(n - 1), alpha / 2)\n        coverageCount += Lo < popVar && popVar < Up\n    end\n    1 - coverageCount / N\nend\n\nfig, ax = ablines(0, 1; color=:green, label=\"y = x\")\nscatter!(ax, alphaUsed, alphaSimulator.(dNormal, n, alphaUsed); color=:blue, label=\"Normal\")\nscatter!(ax, alphaUsed, alphaSimulator.(dLogistic, n, alphaUsed); color=:red, label=\"Logistic\")\naxislegend(ax; position=:lt)\nfig\n```\n\n### Bootstrap confidence intervals\n\nBootstrap, also called empirical bootstrap, is a useful technique which relies on resampling from the observed data $x_1, ..., x_n$ **with replacement** in order to empirically construct **the distribution of the point estimator** for some unknown population parameters.\n\nOne way in which this resampling can be conducted is to apply the inverse probability transform on the empirical cumulative distribution function.\n\n::: {.callout-note}\n\n1. Obtain a sample: $\\mathbf{X} = (x_1, ..., x_n)$.\n\n2. Obtain the empirical cumulative distribution function $F_X(\\mathbf{X})$ based on $\\mathbf{X}$.\n\n3. Use the inverse probability transformation: $\\mathbf{U} = F_X(\\mathbf{X}) \\Longrightarrow \\mathbf{X} = F_X^{-1}(\\mathbf{U})$.\n\n4. Resample $\\mathbf{X}$ via resampling $\\mathbf{U} \\sim U(0, 1)$ with replacement.\n\nIt seems that we sample a large number of \"new\" samples from the population, and then we can get a large number of point estimators, which then are used to construct the empirical distribution of the point estimator. Finally, to get the $1-\\alpha$ confidence interval, we can just get the quantiles of $\\frac{\\alpha}{2}$ and $1-\\frac{\\alpha}{2}$.\n\nBootstrap requires that the number of observations is not too small.\n\n:::\n\nHowever, from an implementation perspective, a simpler alternative is to consider the data points $x_1, ..., x_n$, and then randomly sample $n$ discrete uniform indexes, $j_1, ..., j_n$ each in the range $\\{1, ..., n\\}$. The resampled data denoted by $x^* = (x_1^*, ..., x_n^*)$ is then\n\n$$\nx^* = (x_{j_1}, ..., x_{j_n})\n$$\n\nThat is, each point in the resampeld data is a random observation from the original data, where we allow to **sample with replacement**.\n\nIn Julia, $x^* = \\text{rand}(\\mathbf{X}, n)$, where we use the `rand()` method to uniformally sample $n$ random copies of elements in $\\mathbf{X}$ with replacement.\n\nThe idea of empirical bootstrap is now to repeat the resampling a large number of times, say $N$, and for each resampled data vector, $x^*(1), ..., x^*(N)$ to compute the parameter estimate. If the parameter estimate is denoted by the function $h: R^n \\mapsto R$, then we end up with values\n\n$$\n\\begin{align}\nh^*(1) &= h(x_1^*(1), ..., x_n^*(1)) \\\\\nh^*(2) &= h(x_1^*(2), ..., x_n^*(2)) \\\\\n&\\vdots \\\\\nh^*(N) &= h(x_1^*(N), ..., x_n^*(N))\n\\end{align}\n$$\n\nFinally, a bootstrap confidence interval is determined by computing the respective lower and upper $(\\frac{\\alpha}{2}, 1-\\frac{\\alpha}{2})$ quantiles of the sequence $h^*(1), ..., h^*(N)$.\n\n```{julia}\nusing Random, Distributions, CairoMakie\n\nRandom.seed!(0)\n\nsampleData = [\n    53.35674558144255,\n    53.45887516134873,\n    52.282838627926125,\n    52.98746570643515,\n    51.993167774733486,\n    53.373333606198,\n    55.75410538860477,\n    50.279496381439365,\n    53.6359586914001,\n    53.517705831707495,\n    53.70044994508253,\n    54.15592592604583,\n    53.55054914606446,\n    52.37319589109419,\n    53.4900750059897,\n    52.939458524079605,\n    52.16761562743534,\n    50.87140009591033,\n    53.144919157924924,\n    52.09084035473537,\n]\nn, N = length(sampleData), 10^6\nalpha = 0.1\n\n# sampling with replacement\nbootstrapSampleMeans = [mean(rand(sampleData, n)) for _ in 1:N]\nLmean = quantile(bootstrapSampleMeans, alpha / 2)\nUmean = quantile(bootstrapSampleMeans, 1 - alpha / 2)\n\nprintln(\"Bootstrap confidence interval for the mean: \", (Lmean, Umean))\n\nfig, ax = stephist(bootstrapSampleMeans; bins=1000, color=:blue, normalization=:pdf, label=\"Sample means\")\nvlines!(ax, [Lmean, Umean]; color=:red, label=\"90% CI\")\naxislegend(ax)\nfig\n```\n\nSimply, we can carry out a computational experiment to show that if the number of sample observations is not very large, then the coverage probability of bootstrapped confidence interval is only approximately $1-\\alpha$, but not exactly. However, as the sample size $n$ grows, the coverage probability converges to the desired $1-\\alpha$.\n\n```{julia}\nusing Random, Distributions\n\nRandom.seed!(0)\n\nlambda = 0.1\ndist = Exponential(1 / lambda)\nactualMedian = median(dist)\n\nM = 10^3\nN = 10^4\nnRange = 2:2:20\nalpha = 0.05\n\nfor n in nRange\n    coverageCount = 0\n    for _ in 1:M\n        sampleData = rand(dist, n)\n        bootstrapSampleMeans = [median(rand(sampleData, n)) for _ in 1:N]\n        Lo = quantile(bootstrapSampleMeans, alpha / 2)\n        Up = quantile(bootstrapSampleMeans, 1 - alpha / 2)\n        coverageCount += Lo < actualMedian && actualMedian < Up\n    end\n    println(\"n = \", n, \"\\t coverage = \", coverageCount / M)\nend\n```\n\n### Prediction intervals\n\nA prediction interval tells us a predicted range that **a single next observation** of data is expected to fall within with some level of confidence. This differs from a confidence interval which indicates how confident we are of a particular **parameter** that we are trying to estimate.\n\nIn brief, a prediction interval is constructed based on **the distribution of a population** itself from which we sample observations, while a confidence interval is constructed based on **the distribution of a particular parameter** which we are trying to estimate (e.g. $\\overline{X} \\sim N(\\mu, \\frac{\\sigma^2}{n})$).\n\nSuppose we have a sequence of data points (observations) $x_1, x_2, x_3, ...$, which come from a normal distribution and are assumed i.i.d. Further assume that we observed the first $n$ data points $x_1, ..., x_n$, but have not yet observed $X_{n+1}$. Note that we use lower case $x$ for values observed and upper case $X$ for yet unobserved random variables.\n\nIn this case, a $1-\\alpha$ prediction interval for the single future observation $X_{n+1}$ is given by\n\n$$\n\\bar{x} - t_{1-\\frac{\\alpha}{2},n-1}s\\sqrt{1+\\frac{1}{n}} \\le X_{n+1} \\le \\bar{x} + t_{1-\\frac{\\alpha}{2},n-1}s\\sqrt{1+\\frac{1}{n}}\n$$\n\nwhere $\\bar{x}$ and $s$ are respectively the sample mean and sample standard deviation computed from $x_1, ..., x_n$.\n\nNote that as the number of observations $n$ grows, the bounds of the prediction interval decreases towards\n\n$$\n\\bar{x} - z_{1-\\frac{\\alpha}{2}}s \\le X_{n+1} \\le \\bar{x} - z_{1-\\frac{\\alpha}{2}}s\n$$\n\nand finally has an expected width close to $2z_{1-\\frac{\\alpha}{2}}\\sigma$.\n\n::: {.callout-note}\n\nSuppose $X$ is a normally distributed variable (i.e. $X \\sim N(\\mu, \\sigma^2)$), and then we have $U = \\frac{X-\\mu}{\\sigma} \\sim N(0, 1)$.\n\nFor $U$, we have a $1-\\alpha$ prediction interval $-z_{1-\\frac{\\alpha}{2}} \\le U \\le z_{1-\\frac{\\alpha}{2}}$, and then we have $\\mu - z_{1-\\frac{\\alpha}{2}}\\sigma \\le X \\le \\mu + z_{1-\\frac{\\alpha}{2}}\\sigma$, which is a $1-\\alpha$ prediction interval of $X$ in the case of population mean and population variance known.\n\nSimply, a prediction interval is such an interval constructed based on the distribution of observations and needs to cover a $1-\\alpha$ proportion of this distribution.\n\n:::\n\n```{julia}\nusing Random, Statistics, Distributions, CairoMakie\n\nRandom.seed!(0)\n\nmu, sig = 50, 5\ndist = Normal(mu, sig)\nalpha = 0.01\nnMax = 40\n\nobservations = rand(dist, 1)\npiLarray, piUarray = [], []\n\nfor _ in 2:nMax\n    xNew = rand(dist)\n    push!(observations, xNew)\n\n    xBar, sd = mean(observations), std(observations)\n    n = length(observations)\n    tVal = quantile(TDist(n - 1), 1 - alpha / 2)\n    delta = tVal * sd * sqrt(1 + 1 / n)\n    piL, piU = xBar - delta, xBar + delta\n\n    push!(piLarray, piL)\n    push!(piUarray, piU)\nend\n\nfig, ax = scatter(1:nMax, observations; color=:blue, label=\"Observations\")\nscatterlines!(2:nMax, piUarray; marker=:cross, color=:red, markercolor=:black, label=\"Prediction Interval\")\n\nscatterlines!(2:nMax, piLarray; marker=:cross, color=:red, markercolor=:black)\naxislegend(ax)\nax.xlabel = \"Number of observations\"\nax.ylabel = \"Value\"\nfig\n```\n\n### Credible intervals\n\nThe concept of credible intervals comes from the field of Bayesian statistics.\n\nIn general, we often need to find an interval $[l, u]$ such that given some probability density $f(x)$, the interval satisfies\n\n$$\n\\int_l^u f(x)dx = 1-\\alpha\n$$\n\nThis is needed for confidence intervals, prediction intervals, and credible intervals.\n\nHowever, as long as $\\alpha < 1$, there is more than one interval $[l, u]$ that satisfies the above formula.\n\nIn certain cases, there is a \"natural\" interval. For example, for a symmetric distribution, using equal quantiles is natural (i.e. $l = z_{\\frac{\\alpha}{2}}$ and $u = z_{1-\\frac{\\alpha}{2}}$). In such cases, the mean is also the median and further if the density is unimodal (has a single maximum) the mean and median are also the mode.\n\nHowever, consider asymmetric distribution, there isn't an immediate \"natrual\" choice for $l$ and $u$. There are three types of intervals we often use:\n\n1. **Classic interval:** this type of interval has the mode of the density (assuming the density is unimodal) at its center between $l$ and $u$. An alternative is to use mean or median at the center. That is, assuming the centrality measure (mode, mean, or median) is $m$, we have $[l, u] = [m-E, m+E]$. One way to define $E$ is via\n\n$$\nE = \\max\\{\\epsilon \\ge 0: \\int_{m-\\epsilon}^{m+\\epsilon} f(x)dx \\le 1-\\alpha\\}\n$$\n\n2. **Equal tail interval:** this type of interval simply sets $l$ and $u$ as the $\\frac{\\alpha}{2}$ and $1-\\frac{\\alpha}{2}$ quantiles, respectively. Namely,\n\n$$\n\\frac{\\alpha}{2} = \\int_{-\\infty}^l f(x)dx\n$$\n\nand\n\n$$\n1 - \\frac{\\alpha}{2} = \\int_{u}^{\\infty} f(x)dx\n$$\n\n3. **Highest density interval:** this type of interval seeks to cover the part of the support that is most probable. A consequence is that if the density is unimodal then this highest density interval is also the narrowest possible confidence interval. We can crudely does so by starting with a high-density value and decreasing it gradually while seeking for the associated interval $[l, u]$. An alternative would be to gradually increment $l$ each time finding a corresponding $u$ that satisfies the above formula and within this search to choose the interval that minimizes the width $u-l$.\n\nFor a symmetric and unimodal distribution, all three of these confidence intervals agree.\n\nWe now explain the credible intervals. In the Bayesian setting, we treat the unknown parameter $\\theta$ as a random variable, this is totally different from the classical case where we treat the unknown parameter $\\theta$ as a fixed value. The process of inference is based on observing data, $x = (x_1, ..., x_n)$ which has a distribution depending on the unknown parameter $\\theta$, and fusing it with the prior distribution $f(\\theta)$ that the unknown parameter $\\theta$ is distributed as before observing data $x = (x_1, ..., x_n)$ to obtain the posterior distribution $f(\\theta|x)$ which the unknown parameter $\\theta$ is distributed as after observing data $x = (x_1, ..., x_n$.\n\nHere too, as in the frequentist case, we may wish to describe an interval where it is likely that our parameter lies. Then for a fixed confidence interval $1-\\alpha$, seek $[l, u]$, such that\n\n$$\n\\int_l^u f(\\theta|x)d\\theta = 1-\\alpha\n$$\n\nThere is a basic difference between confidence intervals in the frequentist setting and credible intervals in the Bayesian setting:\n\nFor a given $1-\\alpha$ interval $[L, U]$,\n\n1. In the frequentist setting, the unknown parameter $\\theta$ is a fixed value while $L$ and $U$ are random variables depending on observed data $X$ which is a random variable. So this is why we often say the confidence interval $[L, U]$ will cover the unknown but fixed parameter $\\theta$ under the confidence level $1-\\alpha$.\n\n2. In the Bayesian setting, the unknown parameter $\\theta$ is a random variable while $L$ and $U$ are deterministic values. So we will see the unknown parameter $\\theta$ will fall within the credible interval $[L, U]$ with the probability $1-\\alpha$.\n\n```{julia}\nusing Distributions, CairoMakie\n\nalpha, beta = 8, 2\ndat = [2, 1, 0, 0, 1, 0, 2, 2, 5, 2, 4, 0, 3, 2, 5, 0]\n\nnewAlpha, newBeta = alpha + sum(dat), beta + length(dat)\npost = Gamma(newAlpha, newBeta)\n\nxGrid = quantile(post, 0.01):0.001:quantile(post, 0.99)\nsignificance = 0.9\nhalfAlpha = (1 - significance) / 2\n\ncoverage(l, u) = cdf(post, u) - cdf(post, l)\n\nfunction classicCI(dist)\n    l, u = mode(dist), mode(dist)\n    while coverage(l, u) < significance\n        l -= 0.00001\n        u += 0.00001\n    end\n    (l, u)\nend\nequalTailCI(dist) = (quantile(dist, halfAlpha), quantile(dist, 1 - halfAlpha))\nfunction highestDensityCI(dist)\n    height = 0.999 * maximum(pdf.(dist, xGrid))\n    l, u = mode(dist), mode(dist)\n    while coverage(l, u) < significance\n        range = filter(theta -> pdf(dist, theta) > height, xGrid)\n        l, u = minimum(range), maximum(range)\n        height -= 0.00001\n    end\n    (l, u)\nend\n\nl1, u1 = classicCI(post)\nl2, u2 = equalTailCI(post)\nl3, u3 = highestDensityCI(post)\n\nprintln(\"Classical: \", (l1, u1), \"\\tWidth: \", u1 - l1, \"\\tCoverage: \", coverage(l1, u1))\nprintln(\"Equal tails: \", (l2, u2), \"\\tWidth: \", u2 - l2, \"\\tCoverage: \", coverage(l2, u2))\nprintln(\"Highest density: \", (l3, u3), \"\\tWidth: \", u3 - l3, \"\\tCoverage: \", coverage(l3, u3))\n\nfig, ax = lines(xGrid, pdf.(post, xGrid); color=:black, label=\"Gamma Posterior Distribution\")\nrangebars!(ax, [-0.00025], [l1], [u1]; direction=:x, whiskerwidth=10, color=:blue, label=\"Classic CI\")\nrangebars!(ax, [-0.0005], [l2], [u2]; direction=:x, whiskerwidth=10, color=:red, label=\"Equal Tail CI\")\nrangebars!(ax, [-0.00075], [l3], [u3]; direction=:x, whiskerwidth=10, color=:green, label=\"Highest Density CI\")\naxislegend(ax)\nfig\n```\n\n## Hypothesis testing\n\nTo perform a hypothesis testing, first, we need to partition the parameter space $\\Theta$ as two hypotheses:\n\n* Null hypothesis: $H_0: \\theta \\in \\Theta_0$\n\n* Alternative hypothesis: $H_1: \\theta \\in \\Theta_1$\n\nAnd then, we need to determine the test statistic used to perform the hypothesis testing. Once this is done, we can calculate the test statistic and then get the rejection region (e.g. $(-\\infty, ICDF(\\frac{\\alpha}{2}))$ and $(ICDF(1-\\frac{\\alpha}{2}), +\\infty)$ for a two-sided hypothesis test) or the p-value ($CDF(\\text{the value of test statistic})$) under certain confidence level $1-\\alpha$ under $H_0$ (the distribution of the test statistic is often known under $H_0$ but it's usually unknown under $H_1$).\n\nFinally, make a statement (rejecting or not rejecting $H_0$) under the assumption of null hypothesis based on some confidence level.\n\nSo there are several key concepts concerning a hypothesis testing: two alternative hypotheses, confidence level, test statistic, the distribution of the test statistic under the null hypothesis, rejection region or $p$-value.\n\n### Single sample hypothesis tests for the mean\n\nAssume that the observations $X_1, ..., X_n$ are **normally** distributed and we want to know whether this sample is from a population with $\\mu = \\mu_0$ or not ($\\mu \\ne \\mu_0$).\n\nIn this case, we set up the hypothesis as two-sided (which means that $\\mu < \\mu_0$ and $\\mu > \\mu_0$ are both plausible) and the confidence level $1-\\alpha$.\n\n#### Population variance known (Z-Test)\n\nAssume that $\\sigma$ is known. Under $H_0$, $\\overline{X}$ follows a normal distribution with mean $\\mu_0$ and variance $\\frac{\\sigma^2}{n}$. Hence, it holds that under $H_0$ the Z-statistic\n\n$$\nZ = \\frac{\\overline{X}-\\mu_0}{\\sigma/n}\n$$\n\nfollows a standard normal distribution ($Z \\sim N(0, 1)$).\n\nIn this case, under the null hypothesis, $Z$ is a standard normal random variable, and hence to carry out a hypothesis test we observe its position relative to a standard normal distribution. Specifically, we check if it lies within the rejection region or not, and if it does, we reject the null hypothesis, otherwise we don't. In addition, we can also calculate the $p$-value ($p = 2P(Z > |z|)$). If $p$-value is less than some pre-designated significance level $\\alpha$, then we can reject $H_0$ or we don't.\n\n```{julia}\nusing Random, Distributions, HypothesisTests\n\nRandom.seed!(0)\n\nmu0 = 25\nmu1, sigma = 27, 2\nn = 100\nd = rand(Normal(mu1, sigma), n)\nxBar = mean(d)\n\ntestStatistic = (xBar - mu0) / (sigma / sqrt(n))\npVal = 2 * ccdf(Normal(), testStatistic)\nprintln(\"Manual hypothesis testing: \\nz-statistic: \", testStatistic, \"\\np-value: \", pVal, \"\\n\")\n\nOneSampleZTest(xBar, sigma, n, mu0)\n```\n\n#### Population variance unknown (T-Test)\n\nWhen the population variance is unknown, then $\\overline{X}$ does not subject to $N(\\mu, \\frac{s}{\\sqrt{n}})$. But we know that\n\n$$\nT = \\frac{\\overline{X} - \\mu_0}{S/\\sqrt{n}} \\sim T(n-1)\n$$\n\nunder the null hypothesis.\n\nThe observed test statistic from the data is then\n\n$$\nt = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}\n$$\n\nand the corresponding $p$-value for a two-sided test is\n\n$$\np = 2P(T_{n-1} > |t|)\n$$\n\nwhere $T_{n-1}$ is a random variable distributed according to a T-distribution with $n-1$ degrees of freedom.\n\n```{julia}\nusing Random, Distributions, HypothesisTests\n\nRandom.seed!(0)\n\nmu0, mu1, sigma = 51, 53, 2\ndist = Normal(mu1, sigma)\nd = rand(dist, 20)\n\nxBar, n, s = mean(d), length(d), std(d)\ntStatistic = (xBar - mu0) / (s / sqrt(n))\npVal = 2 * ccdf(TDist(n - 1), abs(tStatistic))\n\nprintln(\"Manually calculated t-statistic: \", tStatistic)\nprintln(\"Manually calculated p-value: \", pVal)\n\nOneSampleTTest(d, mu0)\n```\n\n#### Non-parametric sign test\n\nThe validity of Z-Test or T-Test relies heavily on the assumption that the sample $X_1, ..., X_n$ is comprised of independent normal random variables with variance known or unknown. This is because only under this assunmption does $\\overline{X} \\sim N(\\mu, \\frac{\\sigma^2}{n})$ or $T \\sim T(n-1)$.\n\nFor the non-parametric sign test, ***non-parametric* implies that the distribution of the test statistic does not depend on any particular distributional assumption for the population**.\n\nFor the sign test, we begin by denoting the random variables\n\n$$\nX^+ = \\sum_{i=1}^{n}\\mathbf{1}\\{X_i > \\mu_0\\} \\quad\\text{and}\\quad X^- = \\sum_{i=1}^{n}\\mathbf{1}\\{X_i < \\mu_0\\} = n - X^+\n$$\n\nwhere $\\mathbf{1}$ is the indicator function. The variable $X^+$ is a count of the number of observations that exceed $\\mu_0$, and similarly $X^-$ is a count of the number of observations that are below $\\mu_0$.\n\nObserve that under $H_0: \\mu = \\mu_0$, it holds that $P(X_i > \\mu_0) = P(X_i < \\mu_0) = 1/2$. Note that here we are actually taking $\\mu_0$ as the median of the distribution and assuming that $P(X_i = \\mu_0) = 0$ as is the case for a continuous distribution.\n\nHence, under $H_0$, the random variables $X^+$ and $X^-$ both follow a binomial $(n, 1/2)$ distribution. Given the symmetry of this binomial distribution we define the test statistic to be\n\n$$\nU = max\\{X^+, X^-\\}\n$$\n\nHence, with observed data, and an observed test statistic $u$, the $p$-value can be calculated via\n\n$$\np = 2P(B > u)\n$$\n\nwhere $B \\sim B(n, 1/2)$.\n\n```{julia}\nusing Random, Distributions\n\nRandom.seed!(0)\n\nmu0, mu1, sigma = 51, 53, 2\ndist = Normal(mu1, sigma)\nd = rand(dist, 20)\nn = length(d)\n\nxPos = sum(d .> mu0)\ntestStat = max(xPos, n - xPos)\n\nbinom = Binomial(n, 0.5)\npVal = 2 * ccdf(binom, testStat)\n\nprintln(\"mu0: \", mu0, \"\\nmu1: \", mu1)\nprintln(\"Binomial mean: \", mean(binom), \"\\nTest statistic: \", testStat)\nprintln(\"p-value: \", pVal)\n```\n\n#### Sign test vs. T-test\n\nWhen the normality assumption holds, the T-test is often more powerful than the sign test. That is, for a fixed $\\alpha$, the probability of detecting $H_1$ is higher for the T-test than for the sign test if $H_1 \\ne H_0$. This makes it a more effective test to use, if the data can be assumed normally distributed.\n\n```{julia}\nusing Random, Distributions, CairoMakie\n\nRandom.seed!(0)\n\nActualMuRange = 51:0.02:55\nsigma = 1.2\nmu0 = 53\nn, N = 50, 10^4\npowerT, powerU = [], []\n\nfor ActualMu in ActualMuRange\n    dist = Normal(ActualMu, sigma)\n    rejectT, rejectU = 0, 0\n\n    for _ in 1:N\n        d = rand(dist, n)\n        xBar, stdDev = mean(d), std(d)\n\n        tStatistics = (xBar - mu0) / (stdDev / sqrt(n))\n        pValT = 2 * ccdf(TDist(n - 1), abs(tStatistics))\n\n        xPos = sum(d .> mu0)\n        uStat = max(xPos, n - xPos)\n        pValSign = 2 * ccdf(Binomial(n, 0.5), uStat)\n\n        rejectT += pValT < 0.05\n        rejectU += pValSign < 0.05\n    end\n\n    push!(powerT, rejectT / N)\n    push!(powerU, rejectU / N)\nend\n\nfig, ax = lines(ActualMuRange, powerT; color=:blue, label=\"T test\")\nlines!(ax, ActualMuRange, powerU; color=:red, label=\"Sign test\")\nhlines!(ax, [0.05]; color=:green, label=\"Alpha\")\nvlines!(ax, [mu0 - sigma, mu0 + sigma]; color=:black, label=\"mu0 ± sigma\")\naxislegend(ax; position=:rb)\nax.xlabel = \"Different values of muActual\"\nax.ylabel = \"Proportion of times H0 rejected (power)\"\nfig\n```\n\nFrom the result, **under the normality assumption**, we obsered that\n\n* When $H_1 = H_0$, then the power is $\\alpha$ (i.e. the probability of rejecting $H_0$).\n\n* T-test is more powerful than sign test when $H_1 \\ne H_0$.\n\n* When $H_1 \\to H_0$, sign test will make higher $\\alpha$ error than T-test.\n\n* When $H_1$ is away from $H_0$ by one $\\sigma$ or more, then the powers of T-test and sign test are really similar.\n\n### Two sample hypothesis tests for comparing means\n\nWe now present some common hypothesis tests for the inference on **the difference in means of two populations**.\n\nCommonly, we wish to investigate if the population difference, $\\Delta_0$, takes on a specific value.\n\nHence, we may wish to set up a **two-sided** hypothesis test as\n\n$$\nH_0: \\mu_1 - \\mu_2 = \\Delta_0 \\quad\\text{and}\\quad H_1: \\mu_1 - \\mu_2 \\ne \\Delta_0\n$$\n\nor one could formulate a **one-sided** hypothesis test, such as\n\n$$\nH_0: \\mu_1 - \\mu_2 \\le \\Delta_0 \\quad\\text{and}\\quad H_1: \\mu_1 - \\mu_2 > \\Delta_0\n$$\n\nor the reverse if desired.\n\nIt is common to consider $\\Delta_0 = 0$, in which case the hypothesis test can be stated as $H_0: \\mu_1 = \\mu_2$, and $H_1: \\mu_1 \\ne \\mu_2$.\n\nFor the tests introduced below, we assume that the observations $X_1^{(1)}, ..., X_n^{(1)}$ from population $1$ and $X_1^{(2)}, ..., X_n^{(2)}$ from population $2$ are all **normally distributed**, where $X_i^{(j)}$ has mean $\\mu_j$ and variance $\\sigma_j^2$.\n\nThe testing methodology then differs based on the following three cases:\n\n1. The population variances $\\sigma_1$ and $\\sigma_2$ are **known**.\n\n2. The population variances $\\sigma_1$ and $\\sigma_2$ are **unknown but assumed equal**.\n\n3. The population variances $\\sigma_1$ and $\\sigma_2$ are **unknown and not assumed equal**.\n\nIn each of these cases, the test statistic is given by\n\n$$\n\\frac{(\\overline{X}_1 - \\overline{X}_2) - \\Delta_0}{S_{err}}\n$$\n\nwhere $\\overline{X}_j$ is the sample mean of $X_1^{(j)}, ..., x_n^{(j)}$, and the standard error $S_{err}$ varies according to the case ($1-3$).\n\n#### Population variances known\n\nWhen the population variances $\\sigma_1^2$ and $\\sigma_2^2$ are known, we have $\\overline{X}_1 \\sim N(\\mu_1, \\frac{\\sigma_1^2}{n_1})$ and $\\overline{X}_2 \\sim N(\\mu_2, \\frac{\\sigma_2^2}{n_2})$, and then $\\overline{X}_1 - \\overline{X}_2 \\sim N(\\mu_1 - \\mu_2, \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2})$. Hence we set\n\n$$\nS_{err} = \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\n$$\n\nIn this case, the test statistic follows a standard normal distribution under $H_0$, so we have\n\n$$\nz = \\frac{(\\bar{x}_1 - \\bar{x}_2) - \\Delta_0}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\n$$\n\n```{julia}\nusing Random, Distributions\n\nRandom.seed!(0)\n\nmu1, sigma1 = 10, 2\nmu2, sigma2 = 12, 1.5\n\nd1 = rand(Normal(mu1, sigma1), 20)\nd2 = rand(Normal(mu2, sigma2), 30)\n\nxBar1, n1 = mean(d1), length(d1)\nxBar2, n2 = mean(d2), length(d2)\n\n# to test whether μ1 = μ2 (two-sided)\ndelta0 = 0\n\ntestStat = (xBar1 - xBar2 - delta0) / sqrt(sigma1^2 / n1 + sigma2^2 / n2)\npVal = 2 * ccdf(Normal(), abs(testStat))\n\nprintln(\"μ1 = \", mu1, \"\\nμ2 = \", mu2)\nprintln(\"Manually calculated test statistic: \", testStat)\nprintln(\"Manually calculated p-value: \", pVal)\n```\n\n#### Population variances unknown and assumed equal\n\nIn case the population variances are **unknown and assumed equal** ($\\sigma^2 := \\sigma_1^2 = \\sigma_2^2$), the pooled sample variance (weighted arithmetic mean), $S_p^2$ is used to estimate $\\sigma^2$ based on both samples. It is given by\n\n$$\nS_p^2 = \\frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1+n_2-2}\n$$\n\nwhere $S_j^2$ is the sample variance of sample $j$. It can be shown that under $H_0$, if we set\n\n$$\nS_{err} = \\sqrt{\\frac{S_p^2}{n_1} + \\frac{S_p^2}{n_2}} = S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n$$\n\nthe test statistic is distributed as a T-distribution with $n_1+n_2-2$ degrees of freedom.\n\nFor this case, the observed test statistic is\n\n$$\nt = \\frac{(\\bar{x}_1 - \\bar{x}_2) - \\Delta_0}{S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n$$\n\n```{julia}\nusing Random, Distributions, HypothesisTests\n\nRandom.seed!(0)\n\nmu1, sigma1 = 10, 2\nmu2, sigma2 = 12, 2\n\nd1 = rand(Normal(mu1, sigma1), 20)\nd2 = rand(Normal(mu2, sigma2), 30)\n\nxBar1, s1, n1 = mean(d1), std(d1), length(d1)\nxBar2, s2, n2 = mean(d2), std(d2), length(d2)\n\n# to test whether μ1 = μ2 (two-sided)\ndelta0 = 0\n\nsP = sqrt(((n1 - 1) * s1^2 + (n2 - 1) * s2^2) / (n1 + n2 - 2))\ntestStat = (xBar1 - xBar2 - delta0) / (sP * sqrt(1 / n1 + 1 / n2))\npVal = 2 * ccdf(TDist(n1 + n2 - 2), abs(testStat))\n\nprintln(\"Manually calculated test statistic: \", testStat)\nprintln(\"Manually calculated p-value: \", pVal)\n\nEqualVarianceTTest(d1, d2, delta0)\n```\n\n#### Population variances unknown and not assumed equal\n\nWhere the population variances are unknown and not assumed equal ($\\sigma_1^2 \\ne \\sigma_2^2$), we set\n\n$$\nS_{err} = \\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}\n$$\n\nThen the observed test statistic is given by\n\n$$\nt = \\frac{(\\bar{x}_1 - \\bar{x}_2) - \\Delta_0}{\\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}}\n$$\n\nThe distribution of the test statistic does not follow an exact T-distribution with $n_1+n_2-2$ degrees of freedom. Instead, we use the *Satterthwaite approxmation*, and determine the degrees of freedom via\n\n$$\nv = \\frac{(s_1^2n_1 + s_2^2n_2)^2}{\\frac{(s_1^2/n_1)^2}{n_1-1} + \\frac{(s_2^2/n_2)^2}{n2-1}}\n$$\n\n```{julia}\nusing Random, Distributions, HypothesisTests\n\nRandom.seed!(0)\n\nmu1, sigma1 = 10, 2\nmu2, sigma2 = 12, 1.5\n\nd1 = rand(Normal(mu1, sigma1), 20)\nd2 = rand(Normal(mu2, sigma2), 30)\n\nxBar1, s1, n1 = mean(d1), std(d1), length(d1)\nxBar2, s2, n2 = mean(d2), std(d2), length(d2)\n\n# to test whether μ1 = μ2 (two-sided)\ndelta0 = 0\n\nv = (s1^2 / n1 + s2^2 / n2)^2 / ((s1^2 / n1)^2 / (n1 - 1) + (s2^2 / n2)^2 / (n2 - 1))\ntestStat = (xBar1 - xBar2 - delta0) / sqrt(s1^2 / n1 + s2^2 / n2)\npVal = 2 * ccdf(TDist(v), abs(testStat))\n\nprintln(\"Manually calculated degrees of freedom, v: \", v)\nprintln(\"Manually calculated test statistic: \", testStat)\nprintln(\"Manually calculated p-value: \", pVal)\nUnequalVarianceTTest(d1, d2, delta0)\n```\n\n### Analysis of variance (ANOVA or F-test)\n\nAs mentioned above, Z-test or T-test can handle the problems of comparing means of two populations. However, what if there are more than two populations that need to be compared? You may say that we can compare each pair of them, but in fact, this will reduce the hypothesis test power.\n\nWhen we have more than two populatons to be compared, we call each population a treatment or a group. It is of interest to see if vairous \"treatments\" have an effect on some mean value or not.\n\nMore formally, assume that there is some *overall mean* $\\mu$ and there are $L \\ge 2$ treatments, where each treatment may potentially alter the mean by $\\tau_i$. In this case, the mean of the population under treatment $i$ can be represented by $\\mu_i = \\mu + \\tau_i$, with $\\mu$ an overall mean, and\n\n$$\n\\sum_{i=1}^{L} \\tau_i = 0\n$$\n\nThis condition on the parameters $\\tau_1, ..., \\tau_L$ ensures that given $\\mu_1, ..., \\mu_L$, the overall mean $\\mu$ and $\\tau_1, ..., \\tau_L$ are well defined.\n\nGiven $\\mu_1, ..., \\mu_L$, we have\n\n$$\n\\mu = \\frac{1}{L} \\sum_{i=1}^{L} \\mu_i \\quad\\text{and}\\quad \\tau_i = \\mu_i - \\mu\n$$\n\n**The question is then: *Do the treatments have any effect or not?***\n\nSuch a question is presented via the hypothesis formulation:\n\n$$\nH_0: \\tau_1 = \\tau_2 = \\cdots = \\tau_L = 0 \\quad\\text{vs.}\\quad H_1: \\exists i\\ |\\ \\tau_i \\ne 0\n$$\n\nNote that $H_0$ is equivalent to the statement that $\\mu_1 = \\mu_2 = \\cdots = \\mu_L$, indicating that the treatments do not have an effect. Furthermore, $H_1$ states that there exists an $i$ with $\\tau_i \\ne 0$ is equivalent to the case where there exist at least two treatments, $i$ and $j$ such that $\\mu_i \\ne \\mu_j$.\n\nAssume that we collect observations as follows:\n\n$$\n\\begin{align}\n\\text{Treatment 1: } &x_{11}, x_{12}, ..., x_{1n_1} \\\\\n\\text{Treatment 2: } &x_{21}, x_{22}, ..., x_{2n_2} \\\\\n&\\vdots \\\\\n\\text{Treatment L: } &x_{L1}, x_{L2}, ..., x_{Ln_L}\n\\end{align}\n$$\n\nwhere $n_1, n_2, ..., n_L$ are the sample sizes for each treatment.\n\nThen denote the total number of observations via\n\n$$\nm = \\sum_{i=1}^{L} n_i\n$$\n\nWe also consider the sample means for each treatment\n\n$$\n\\bar{x}_i = \\frac{1}{n_i} \\sum_{i=1}^{n_i} x_{ij}\n$$\n\nand the overall sample mean\n\n$$\n\\bar{x} = \\frac{1}{m} \\sum_{i=1}^{L} \\sum_{i=1}^{n_i} x_{ij} = \\sum_{i=1}^{L} \\frac{n_i}{m} x_{ij}\n$$\n\nIn ANOVA, the statistical model assumes that the observations of each treatment come from an underlying model of the following form:\n\n$$\nX_i = \\mu_i + \\epsilon = \\mu + \\tau_i + \\epsilon \\quad\\text{where}\\quad \\epsilon \\sim N(0, \\sigma^2)\n$$\n\nwhere $X_i$ is the model for the $i$th treatment, and $\\epsilon$ is some noise term with common unknown variance across all treatment groups, independent across measurements.\n\n#### Decomposing sum of squares\n\nA key idea of ANOVA is the decomposition of the total variability into two components: the variablity between the treatments, and the variability within the treatments.\n\nThe total sum of squares ($SS_\\text{Total}$) is a measure of the total variability of all observations, and is calculated as follows:\n\n$$\nSS_\\text{Total} = \\sum_{i=1}^{L} \\sum_{j=1}^{n_i} (x_{ij} - \\bar{x})^2\n$$\n\nwhere $\\bar{x}$ is the overall mean.\n\nNow we decompose $SS_\\text{Total}$ into two parts:\n\n$$\n\\begin{align}\n\\sum_{i=1}^{L} \\sum_{j=1}^{n_i} (x_{ij} - \\bar{x})^2 &= \\sum_{i=1}^{L} \\sum_{j=1}^{n_i} (x_{ij} - \\bar{x}_i + \\bar{x}_i - \\bar{x})^2 \\\\\n&= \\sum_{i=1}^{L} \\sum_{j=1}^{n_i} \\left((x_{ij} - \\bar{x}_i)^2 + 2(x_{ij} - \\bar{x}_i)(\\bar{x}_i - \\bar{x}) + (\\bar{x}_i - \\bar{x})^2\\right) \\\\\n&= \\underbrace{\\sum_{i=1}^{L} \\sum_{j=1}^{n_i} (x_{ij} - \\bar{x}_i)^2}_{SS_\\text{Error}} + \\underbrace{\\sum_{i=1}^{L} n_i (\\bar{x}_i - \\bar{x})^2}_{SS_\\text{Treatment}}\n\\end{align}\n$$\n\nNote $\\sum_{i=1}^{n_i} (x_{ij} - \\bar{x}_i) = 0$. So we have\n\n$$\nSS_\\text{Total} = SS_\\text{Error} + SS_\\text{Treatment}\n$$\n\nNote that $SS_\\text{Error}$ is also known as the sum of variability within the groups, and that $SS_\\text{Treatment}$ is also known as the variability between the groups.\n\nThe decomposition holds under both $H_0$ and $H_1$, and hence allows us to construct a test statistic. Intuitively, under $H_0$, both $SS_\\text{Error}$ and $SS_\\text{Treatment}$ should contribute to $SS_\\text{Total}$ **in the same manner (once properly normalized)**. Alternatively, under $H_1$, it is expected that $SS_\\text{Treatment}$ would contribute **more heavily** to the total variability.\n\nThe test statistic we constructed is called F-statistic, defined as the ratio of $SS_\\text{Treatment}$ and $SS_\\text{Error}$ normalized by their respective degrees of freedom $L-1$ and $m-L$:\n\n$$\nF = \\frac{SS_\\text{Treatment}/(L-1)}{SS_\\text{Error}/(m-L)}\n$$\n\nThese normalized quantities are, respectively, denoted by $MS_\\text{Treatment}$ and $MS_\\text{Error}$ standing for \"Mean Squared\". Hence, $F = \\frac{MS_\\text{Treatment}}{MS_\\text{Error}}$\n\nUnder $H_0$, the F-statistic follows an F-distribution with $L-1$ degrees of freedom for the numerator and $m-L$ degrees of freedom for the denominator. Intuitively, under $H_0$, we expect the numerator and denominator to have similar values, hence expect $F$ to be around $1$ (indeed most of the mass of $F$ distributions is concentrated around $1$). However, if $MS_\\text{Treatment}$ is significantly larger, then it indicates that $H_0$ may not hold. Hence, the approach of the F-test is to reject $H_0$ if the F-statistic is geater than the $1-\\alpha$ quantile of the respective F-distribution. Similarly, the $p$-value for an observed F-statistic $f_o$ is given by\n\n$$\np = P(F_{L-1, m-L} > f_o)\n$$\n\nwhere $F_{L-1, m-L}$ is an F-distributed random variable with $L-1$ numerator degrees of freedom and $m-L$ denominator degrees of freedom.\n\n```{julia}\nusing Random, Distributions, DataFrames, GLM, CategoricalArrays\n\nRandom.seed!(0)\n\nallData = [rand(Normal(10, 1), 20), rand(Normal(11, 2), 30), rand(Normal(11, 2), 25)]\n\n# manual ANOVA\n# the decomposition of the sum of squares\n# F-test\nnArray = length.(allData)\nd = length(nArray)\n\nxBarTotal = mean(vcat(allData...))\nxBarArray = mean.(allData)\n\nssBetween = sum([nArray[i] * (xBarArray[i] - xBarTotal)^2 for i in 1:d])\nssWithin = sum([sum([(ob - xBarArray[i])^2 for ob in allData[i]]) for i in 1:d])\n\ndfBetween = d - 1\ndfError = sum(nArray) - d\n\nmsBetween = ssBetween / dfBetween\nmsError = ssWithin / dfError\n\nfStat = msBetween / msError\npVal = ccdf(FDist(dfBetween, dfError), fStat)\n\nprintln(\"Maunal ANOVA: \\nF-Statistic: \", fStat, \"\\np-value: \", pVal)\n\n# ANOVA using GLM package which requires the DataFrames package\nnArray = length.(allData)\nd = length(nArray)\n\ntreatment = vcat([fill(k, nArray[k]) for k in 1:d]...)\nresponse = vcat(allData...)\ndf = DataFrame(Response=response, Treatment=categorical(treatment))\nmodelH0 = lm(@formula(Response ~ 1), df)\nmodelH1 = lm(@formula(Response ~ 1 + Treatment), df)\nres = ftest(modelH1.model, modelH0.model)\n\nprintln(\"GLM ANOVA: \\nF-Statistic: \", res.fstat[2], \"\\np-value: \", res.pval[2])\n```\n\n#### More on the distribution of the F-Statistic\n\nHere we use the Monte Carlo simulation to generate the distribution of the F-Statistic for two different cases ($H_0$ and $H_1$).\n\nUnder $H_0$, the distribution of F-Statistic obtained via Monte Carlo should be exactly the same as the analytical F-distribution with the same numerator and denominator degrees of freedom, but it's not for the distribution of F-Statistic under $H_1$.\n\n```{julia}\nusing Random, Distributions, CairoMakie\n\nRandom.seed!(0)\n\nfunction anovaFStat(allData)\n    xBarArray = mean.(allData)\n    nArray = length.(allData)\n    xBarTotal = mean(vcat(allData...))\n    Le = length(nArray)\n\n    ssBetween = sum([nArray[i] * (xBarArray[i] - xBarTotal)^2 for i in 1:Le])\n    ssWithin = sum([sum([(ob - xBarArray[i])^2 for ob in allData[i]]) for i in 1:Le])\n\n    return ((ssBetween / (Le - 1)) / (ssWithin / (sum(nArray) - Le)))\nend\n\ncase1 = [13.4, 13.4, 13.4, 13.4, 13.4]\ncase2 = [12.7, 11.8, 13.4, 12.7, 12.9]\nstdDevs = [2, 2, 2, 2, 2]\nnumObs = [24, 15, 13, 23, 9]\nLe = length(case1)\n\nN = 10^5\n\nmcFstatsH0 = Array{Float64}(undef, N)\nfor i in 1:N\n    mcFstatsH0[i] = anovaFStat([rand(Normal(case1[j], stdDevs[j]), numObs[j]) for j in 1:Le])\nend\n\nmcFstatsH1 = Array{Float64}(undef, N)\nfor i in 1:N\n    mcFstatsH1[i] = anovaFStat([rand(Normal(case2[j], stdDevs[j]), numObs[j]) for j in 1:Le])\nend\n\nfig, ax = stephist(mcFstatsH0; bins=100, color=:blue, normalization=:pdf, label=\"H0\")\nstephist!(ax, mcFstatsH1; bins=100, color=:red, normalization=:pdf, label=\"H1\")\n\ndfBetween = Le - 1\ndfError = sum(numObs) - Le\nxGrid = 0:0.01:10\n\nlines!(ax, xGrid, pdf.(FDist(dfBetween, dfError), xGrid); color=:green, label=\"F-Statistic analytic\")\n\ncritVal = quantile(FDist(dfBetween, dfError), 0.95)\n\nvlines!(ax, [critVal]; color=:black, linestyle=:dash, label=\"Critical value boundary\")\n\naxislegend(ax)\nax.xlabel = \"F-value\"\nax.ylabel = \"Density\"\nfig\n```\n\nAnalysis presented here is just the *one-way ANOVA*, which means that we have only one treatment category having different treatment levels. Often we may have two treatment categories, each of which has different treatment levels (*two-way ANOVA*). This can be extended to higher dimensional extensions, which are often considered in *block factorial design*. In addition, once we reject $H_0$, we often want to known which specific treatments have an effect and in which way. This involves *multiple comparisons*.\n\n### Independence and goodness of fit\n\n#### Testing for an indenpendent sequence\n\nHere, we'll use the *Wald-Wolfowitz runs test* to perform hypothesis testing for checking if a sequence of random variables is i.i.d. (indenpendent and identically distributed).\n\nConsider a sequence of data points $x_1, ..., x_n$ with sample mean $\\bar{x}$. For simplicity, assume that no point equals the sample mean. Now transform the sequence to $y_1, ..., y_n$ via $y_i = x_i - \\bar{x}$.\n\nWe now consider the signs of $y_i$. For example, in a dataset with $20$ observations, once considering the signs we may have a sequence such as\n\n$$\n+-+-----++--+---++++\n$$\n\nindicating that the first is positive (greater than the mean), the second is negative (less than the mean), and so on.\n\nNote that we assume no exact $0$ for $y_i$ and if such exist we can arbitrarily assign them to be either positive or negative. We then create a random variable $R$ by counting the number of *runs* in this sequence, **where a run is a consecutive sequence of points having the same sign**. In our example, the runs (visually separated by white space) are\n\n$$\n+\\ -\\ +\\ -----\\ ++\\ --\\ +\\ ---\\ ++++\n$$\n\nHance $R = 9$.\n\nThe essence of the Wald-Wolfowitz runs test is an approximation of the distribution of $R$ under $H_0$. The null hypothesis is that the data is i.i.d. Under $H_0$, $R$ can be shown to be approximately follow a **normal** distribution with mean $\\mu$ and variance $\\sigma^2$, where\n\n$$\n\\mu = 2 \\frac{n_+n_-}{n} + 1,\\qquad \\sigma^2 = \\frac{(\\mu - 1)(\\mu - 2)}{n - 1}\n$$\n\nHere $n_+$ is the number of positive signs and $n_-$ is the number of negative signs. Note that $n_+$ and $n_-$ are also random variables. Clearly, $n_+ + n_- = n$, the total number of observations. With such values at hand the test creates the $p$-value via\n\n$$\n2P(Z > \\left|\\frac{R - \\mu}{\\sigma}\\right|)\n$$\n\nwhere $Z$ is a standard normal variable.\n\nIn the following code, we'll validate that the random variable $R$ (i.e. runs) is approximately distributed as a normal distribution with mean $\\mu$ and variance $\\sigma$ defined above under $H_0$ (i.e. the data points are i.i.d.). In other words, the distribution of the $p$-value is $U(0, 1)$.\n\n```{julia}\nusing Random, StatsBase, Distributions, CairoMakie\n\nRandom.seed!(0)\n\n# we'll use a Monte Carlo simulation to obtain the empirical distribution of the p-value\n# sample size: n\n# experimental numbers: N\nn, N = 10^3, 10^6\n\n# calculate the p-value for each sample\nfunction waldWolfowitz(data)\n    n = length(data)\n    sgns = data .> mean(data)\n    nPlus, nMinus = sum(sgns), n - sum(sgns)\n    wwMu = 2 * nPlus * nMinus / n + 1\n    wwVar = (wwMu - 1) * (wwMu - 2) / (n - 1)\n\n    R = 1\n    for i in 1:(n-1)\n        R += sgns[i] != sgns[i+1]\n    end\n\n    zStat = abs((R - wwMu) / sqrt(wwVar))\n    2 * ccdf(Normal(), zStat)\nend\n\n# repeat the process for N times\npVals = [waldWolfowitz(rand(Normal(), n)) for _ in 1:N]\n# here, we calculate the hypothesis testing power\n# under H0, the power is equal to α\nfor alpha in [0.001, 0.005, 0.01, 0.05, 0.1]\n    pva = sum(pVals .< alpha) / N\n    println(\"For alpha = $(alpha), p-value area = $(pva)\")\nend\n\nfig = Figure(size=(500, 800))\n# we can see that the distributio of the p-value is U(0, 1)\n# the reason why spikes of high density appear is that\n# we are approximating a discrete random variable R (can only have positive integers) with a normal random variable\nax1 = Axis(fig[1, 1]; xlabel=\"p-value\", ylabel=\"Frequency\")\nhist!(ax1, pVals; bins=5 * n)\n\npGrid = 0:0.001:1\n# get the ECDF using the ecdf function from StatsBase package\nFhat = ecdf(pVals)\n\n# the ECDF indicates that the distribution is almost uniform\nax2 = Axis(fig[2, 1]; xlabel=\"p-value\", ylabel=\"ECDF\")\nlines!(ax2, pGrid, Fhat.(pGrid))\nfig\n```\n\n### More on power\n\n#### Parameters affecting power\n\nEstimate the hypothesis testing powers under different scenarios:\n\n```{julia}\nusing Random, Distributions, KernelDensity, CairoMakie\n\nRandom.seed!(1)\n\n# calculate T-statistic by sampling n observations from a given normal distribution\nfunction tSat(mu0, mu, sig, n)\n    sample = rand(Normal(mu, sig), n)\n    xBar = mean(sample)\n    s = std(sample)\n    (xBar - mu0) / (s / sqrt(n))\nend\n\nmu0, mu1A, mu1B = 20, 22, 24\nsig, n = 7, 5\nN = 10^6\nalpha = 0.05\n\n# the underlying mean equals the mean under the null hypothesis\n# the power is α\ndataH0 = [tSat(mu0, mu0, sig, n) for _ in 1:N]\n# the underlying mean is increased\ndataH1A = [tSat(mu0, mu1A, sig, n) for _ in 1:N]\n# the underlying mean is increased further\ndataH1B = [tSat(mu0, mu1B, sig, n) for _ in 1:N]\n# increase the sample size\ndataH1C = [tSat(mu0, mu1B, sig, 2 * n) for _ in 1:N]\n# the underlying variance is decreased\ndataH1D = [tSat(mu0, mu1B, sig / 2, 2 * n) for _ in 1:N]\n\n# calculate the quantile of alpha\ntCrit = quantile(TDist(n - 1), 1 - alpha)\n# estimate the power\nestPwr(sample) = sum(sample .> tCrit) / N\n\nprintln(\"Rejection boundary: \", tCrit)\nprintln(\"Power under H0 (equal α): \", estPwr(dataH0))\nprintln(\"Power under H1A (increase μ): \", estPwr(dataH1A))\nprintln(\"Power under H1B (increase μ further): \", estPwr(dataH1B))\nprintln(\"Power under H1C (increase sample size n to 2n): \", estPwr(dataH1C))\nprintln(\"Power under H1D (decrease σ to σ/2): \", estPwr(dataH1D))\n\nkH0 = kde(dataH0)\nkH1A = kde(dataH1A)\nkH1B = kde(dataH1B)\nkH1C = kde(dataH1C)\nkH1D = kde(dataH1D)\n\nxGrid = -10:0.1:15\n\nfig, ax = lines(xGrid, pdf(kH0, xGrid); color=:blue, label=\"Distribution under H0\")\nlines!(ax, xGrid, pdf(kH1A, xGrid); color=:red, label=\"Distribution under H1A\")\nlines!(ax, xGrid, pdf(kH1B, xGrid); color=:green, label=\"Distribution under H1B\")\nlines!(ax, xGrid, pdf(kH1C, xGrid); color=:orange, label=\"Distribution under H1C\")\nlines!(ax, xGrid, pdf(kH1D, xGrid); color=:purple, label=\"Distribution under H1D\")\nvlines!(ax, [tCrit]; color=:black, label=\"Critical value boundary\")\nax.xlabel = L\"\\Delta = \\mu - \\mu_0\"\nax.ylabel = \"Density\"\naxislegend(ax)\nfig\n```\n\nUnder a normal population, $\\mu$, $\\sigma$, $\\alpha$, and sample size all have an effect on the power. But in practice, if keeping $\\alpha$ constant, it is only the sample size can be controlled.\n\nAs a consequence, we need to know what is the sample size for our expected power.\n\n#### Power curves\n\nAs mentioned before, if keeeping $\\alpha$ constant, it is only the sample size can be controlled. Therefore, underlying a given $\\alpha$, we must know\n\n* To obtain a given power, how many observations (i.e. sample size $n$) do we need?\n\n* For a few of available sample sizes, what's the largest power we can obtain?\n\n**A *power curve* is a plot of the power as a function of certain parameters we are interested in.**\n\nFor example, under the hypothesis test setup:\n\n$$\nH_0: \\mu = \\mu_0\\ \\ \\ \\ \\text{and}\\ \\ \\ \\ H_1: \\mu > \\mu_0\n$$\n\nwe want to estimate the power of a one-sided T-test for different scenarios combining the mean $\\mu$ and the sample size $n$ under normality assumption.\n\n```{julia}\nusing Random, Distributions, CairoMakie\n\n# calculate T-statistic by sampling n observations from a given normal distribution\nfunction tSat(mu0, mu, sig, n)\n    sample = rand(Normal(mu, sig), n)\n    xBar = mean(sample)\n    s = std(sample)\n    (xBar - mu0) / (s / sqrt(n))\nend\n\n# estimate the statistical power under a given scenario (μ, n)\nfunction powerEstimate(mu0, mu1, sig, n, alpha, N)\n    Random.seed!(0)\n    sampleH1 = [tSat(mu0, mu1, sig, n) for _ in 1:N]\n    critVal = quantile(TDist(n - 1), 1 - alpha)\n    sum(sampleH1 .> critVal) / N\nend\n\nmu0 = 20\nsig = 5\nalpha = 0.05\nN = 10^4\nrangeMu1 = 16:0.1:30\nnList = [5, 10, 20, 30]\n\npowerCurves = [powerEstimate.(mu0, rangeMu1, sig, n, alpha, N) for n in nList]\n\nfig, ax = lines(rangeMu1, powerCurves[1]; color=:blue, label=\"n = $(nList[1])\")\nlines!(ax, rangeMu1, powerCurves[2]; color=:red, label=\"n = $(nList[2])\")\nlines!(ax, rangeMu1, powerCurves[3]; color=:green, label=\"n = $(nList[3])\")\nlines!(ax, rangeMu1, powerCurves[4]; color=:purple, label=\"n = $(nList[4])\")\nax.xlabel = L\"\\mu\"\nax.ylabel = \"Power\"\naxislegend(ax; position=:lt)\nfig\n```\n\nAs seen in the above figure, under a given power, if $\\mu$ is away from $\\mu_0$ further, then we can take less observations. On the other hand, for a given $\\mu$, if we hope to increase the power, we need to increase the sample size.\n\nAnother point to note is that the x-axis could be adjusted to represent the difference $\\Delta = \\mu - \\mu_0$. Furthermore, one could make the axis scale invariant by dividing $\\Delta$ by the standard deviation.\n\n#### Distribution of the $p$-value\n\nFor a uniform $U(0, 1)$ random variable, we have its CDF $F(x) = x, x \\in [0, 1]$, and CCDF $F_c(x) = 1 - x, x \\in [0, 1]$.\n\n$p$-value is defined as $p = P(S > u)$, where $S$ is a random variable representing the test statistic, $u$ is the observed test statistic, and $p$ is the $p$-value of the observed test statistic.\n\nTo discuss the distribution of the $p$-value, denote the random variable of the $p$-value by $P$. Hence $P = 1 - F(S)$, where $F(\\cdot)$ is the CDF of the test statistic under $H_0$. Note that $P$ is just a transformation of the test statistic random variable $S$. Assume that $S$ is continuous and assume that $H_0$ holds, hence $P(S < u) = F(u)$. we now have\n\n$$\n\\begin{align}\nP(P > x) &= P(1 - F(S) > x) \\\\\n&= P(F(S) < 1 - x) \\\\\n&= P(S < F^{-1}(1-x)) \\\\\n&= F(F^{-1}(1-x)) \\\\\n&= 1 - x\n\\end{align}\n$$\n\nObviously, the CDF of the random variable $P$ is $F(x) = P(P < x) = x, x \\in [0, 1]$, so the distribution of the $p$-value is $U(0, 1)$ under $H_0$.\n\nIf $H_0$ does not hold, then $P(S < u) \\ne F(u)$ and the derivation above fails. In such a case, the distribution of the $p$-value is no longer uniform. In fact, in such a case, if the setting is such that the power of the test statistic increases, then we expect the distribution of the $p$-value to be more concentrated around $0$ than a uniform distribution.\n\n```{julia}\nusing Random, Distributions, KernelDensity, CairoMakie\n\nRandom.seed!(1)\n\nfunction pval(mu0, mu, sig, n)\n    sample = rand(Normal(mu, sig), n)\n    xBar = mean(sample)\n    s = std(sample)\n    tStatistics = (xBar - mu0) / (s / sqrt(n))\n    # under H0, t ∼ T(n - 1)\n    ccdf(TDist(n - 1), tStatistics)\nend\n\nmu0, mu1A, mu1B = 20, 23, 26\nsig, n, N = 7, 5, 10^6\n\npValsH0 = [pval(mu0, mu0, sig, n) for _ in 1:N]\npValsH1A = [pval(mu0, mu1A, sig, n) for _ in 1:N]\npValsH1B = [pval(mu0, mu1B, sig, n) for _ in 1:N]\n\nalpha = 0.05\nestPwr(pVals) = sum(pVals .< alpha) / N\n\nprintln(\"Power under H0: \", estPwr(pValsH0))\nprintln(\"Power under H1A: \", estPwr(pValsH1A))\nprintln(\"Power under H1B: \", estPwr(pValsH1B))\n\nfig, ax = stephist(pValsH0; bins=100, normalization=:pdf, color=:blue, label=\"Under H0\")\nstephist!(ax, pValsH1A; bins=100, normalization=:pdf, color=:red, label=\"Under H1A\")\nstephist!(ax, pValsH1B; bins=100, normalization=:pdf, color=:green, label=\"Under H1B\")\nvlines!(ax, [alpha]; color=:black, label=\"α\", linestyle=:dash)\naxislegend(ax)\nax.xlabel = \"p-value\"\nax.ylabel = \"Density\"\nfig\n```\n\n## Appendices\n\n### Base conversions\n\n1. Base $10$ to base $k$:\n\n- 整数部分：除 $k$ 取余，逆序写出，直到**商**为 $0$。\n\n- 小数部分：乘 $k$ 取整，顺序写出，直到**小数部分**为 $0$ 或达到指定的精度为止。\n\nNote: $\\frac{1}{2} \\frac{1}{1} \\frac{0}{0} \\frac{.}{} \\frac{1}{-1} \\frac{0}{-2} \\frac{1}{-3}$.\n\n### Misc\n\n1. The largest integer which can be represented by `Int32` is $2^{31} - 1$.\n\n对于 `Int32` 来说，最高位为**符号位**，因此最大的二进制数为 $\\underbrace{1...1}_{\\text{31 1's}}$，即所能表示的最大整数为 $1\\cdot 2^0 + 1\\cdot 2^1 + ... + 1\\cdot 2^{30}$，为了表示方便，我们将其加 $1$，变为 $1\\underbrace{0...0}_{\\text{31 0's}}$，再减 $1$，即 $2^{31} - 1$。\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"center","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../../../../styles.css"],"toc":true,"toc-depth":6,"number-sections":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.56","theme":{"light":"cerulean","dark":"slate"},"image":"../../neuron_icon.png","title-block-banner":true,"title":"Probability and statistics with Julia","author":"Rui Yang","date":"2024-09-22","date-modified":"last-modified","categories":["probability","statistics","julia"],"jupyter":"julia-1.10","toc-location":"left","number-depth":6,"fig-cap-location":"bottom","lightbox":true,"tbl-cap-location":"top","page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}