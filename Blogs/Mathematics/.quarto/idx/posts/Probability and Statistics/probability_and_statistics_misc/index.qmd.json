{"title":"Probability and statistics (misc)","markdown":{"yaml":{"title":"Probability and statistics (misc)","author":"Rui Yang","date":"2024-09-22","date-modified":"last-modified","categories":["probability","statistics","misc"],"format":{"html":{"toc":true,"toc-depth":6,"toc-location":"left","fig-align":"center","number-depth":6,"number-sections":true,"fig-cap-location":"bottom","fig-format":"png","lightbox":true,"tbl-cap-location":"top","page-layout":"full"}},"execute":{"warning":false}},"headingText":"From the book *Calculus with applications* by *Peter D. Lax*","containsRefs":false,"markdown":"\n\n\n### Probability\n\nProbability is the branch of mathematics that deals with **events** whose **individual outcomes** are **unpredictable**, but whose outcomes **on average** are **predictable**.\n\nExperiments can be divided into two types:\n\n-   **Deterministic:** whose individual outcomes are predictable;\n\n-   **Nondeterministic (random):** whose individual outcomes are unpredictable.\n\n#### Discrete probability\n\nNext, all experiments we'll deal with are *repeatable* (it can be performed repeatedly any number of times) and *random* (any single performance of the experiment is unpredictable).\n\nIn this section, we'll deal with experiments having *a finite number of possible outcomes*. We denote the number of possible outcomes by $n$, and number them from $1$ to $n$.\n\n##### The probability of any single outcome\n\nIn a random and repeatable experiment, if we denote $S_j$ by *the number of instances* among the first $N$ experiments where the $j$th outcome was observed to occur, then the frequency $\\frac{S_j}{N}$ with which the $j$th outcome has been observed to occur tends to a limit as $N$ tends to infinity. We call this limit the probability of the $j$th outcome and denote it by $p_j$:\n\n$$\np_j = \\lim\\limits_{N \\to \\infty} \\frac{S_j}{N}\n$$\n\nThese probabilities have the following properties:\n\n-   $0 \\leq p_j \\leq 1$;\n\n::: callout-note\nFor $\\frac{S_j}{N}$ lies between $0$ and $1$, and therefore so does its limit $p_j$.\n:::\n\n-   $\\sum_{j=1}^{n} p_j = 1$.\n\n::: callout-note\nWe have\n\n$$\nS_1 + S_2 + \\cdots + S_n = N\n$$\n\nDividing by $N$, we get\n\n$$\n\\frac{S_1}{N} + \\frac{S_2}{N} + \\cdots + \\frac{S_n}{N} = 1\n$$\n\nAs $N$ tends to infinity, we have\n\n$$\n\\lim\\limits_{N \\to \\infty} \\sum_{j=1}^{n} \\frac{S_j}{N} = \\lim\\limits_{N \\to \\infty} \\sum_{j=1}^{n} p_j = 1\n$$\n:::\n\n##### The probability of an event\n\nIn fact very often, we are not interested in all the details of the outcome of an experiment, but merely in a particular aspect of it (e.g. throwing a die, we may be interested only in whether the outcome is even or odd).\n\nAn occurrence such as throwing an even number is called an *event*, which is defined as the following:\n\n> An event $E$ is defined as **any collection of possible outcomes**.\n\n**Note:** we say that an event $E$ occurred whenever any outcome belonging to $E$ occurred.\n\nThe probability $p(E)$ of an event $E$ is\n\n$$\np(E) = \\lim\\limits_{N \\to \\infty} \\frac{S(E)}{N}\n$$\n\nwhere $S(E)$ is the number of instances among the first $N$ experiments when the event $E$ took place.\n\n::: callout-note\nWe have\n\n$$\nS(E) = \\sum_{j\\ \\text{in}\\ E} S_j\n$$\n\nDividing by $N$\n\n$$\n\\frac{S(E)}{N} = \\sum_{j\\ \\text{in}\\ E} \\frac{S_j}{N}\n$$\n\nAs $N$ tends to infinity, we have\n\n$$\np(E) = \\sum_{j\\ \\text{in}\\ E} p_j\n$$\n:::\n\n###### The arithmetic rules of probability of some special events\n\n1.  Addition rule for disjoint events\n\nTwo events $E_1$ and $E_2$ are called *disjoint* if both cannot take place simultaneously (i.e. $E_1 \\cap E_2 = \\emptyset$).\n\nThen we have\n\n$$\np(E_1 \\cup E_2) = p(E_1) + p(E_2)\n$$\n\n::: callout-note\n$$\np(E_1 \\cup E_2) = \\sum_{j\\ \\text{in}\\ E_1\\ \\text{or}\\ E_2} p_j\n$$\n\nDisjointness means that an outcome $j$ may belong either to $E_1$ or to $E_2$ **but not to both**; therefore,\n\n$$\np(E_1 \\cup E_2) = \\sum_{j\\ \\text{in}\\ E_1\\ \\text{or}\\ E_2} p_j = \\sum_{j\\ \\text{in}\\ E_1} p_j + \\sum_{j\\ \\text{in}\\ E_2} p_j = p(E_1) + p(E_2)\n$$\n:::\n\n2.  Product rule for independent events\n\nTwo events $E$ and $F$ are called *independent* if the outcome of one cannot influence the other, nor is the outcome of both under the influence of a common cause.\n\nThen we have\n\n$$\np(E \\cap F) = p(E)p(F)\n$$\n\n::: callout-note\nAmong the first $N$ experiments, count the number of times $E$ has occurred ($S(E)$), F has occurred ($S(F)$), and $E \\cap F$ has occurred ($S(E \\cap F)$). Then we have\n\n$$\np(E) = \\lim\\limits_{N \\to \\infty} \\frac{S(E)}{N}\n$$\n\n$$\np(F) = \\lim\\limits_{N \\to \\infty} \\frac{S(F)}{N}\n$$\n\n$$\np(E \\cap F) = \\lim\\limits_{N \\to \\infty} \\frac{S(E \\cap F)}{N}\n$$\n\nSuppose that we single out from the sequence of combined experiments the subsequence of those where $E$ occurred. The frequency of occurrence of $F$ in this subsequence is $\\frac{S(E \\cap F)}{S(E)}$. If the two events $E$ and $F$ are truly independent, the frequency with which $F$ occurs in this subsequence should be the same as the frequency with which $F$ occurs in the original sequence, i.e.\n\n$$\n\\lim\\limits_{N \\to \\infty} \\frac{S(E \\cap F)}{S(E)} = \\lim\\limits_{N \\to \\infty} \\frac{S(F)}{N} = p(F)\n$$\n\nWe write the frequency of $\\frac{S(E \\cap F)}{N}$ as the product\n\n$$\n\\frac{S(E \\cap F)}{N} = \\frac{S(E \\cap F)}{S(E)}\\frac{S(E)}{N}\n$$\n\nThen we have\n\n$$\np(E \\cap F) = \\lim\\limits_{N \\to \\infty} \\frac{S(E \\cap F)}{N} = \\lim\\limits_{N \\to \\infty} \\frac{S(E \\cap F)}{S(E)} \\cdot \\lim\\limits_{N \\to \\infty} \\frac{S(E)}{N} = p(F)p(E)\n$$\n:::\n\n##### Characteristics of random variables\n\n1.  Numerical outcome\n\nThe numerical outcome of an experiment means the assignment of a real number $x_j$ to each of the possible outcomes.\n\nNote that different outcomes may be assigned the same number so we have to re-calculate the probability $p(x_j)$ for each $x_j$ with which $x_j$ occurred.\n\n2.  Expectation\n\nIn a random experiment with $n$ possible outcomes of probability $p_j$ and numerical outcome $x_j$, the average numerical outcome, called the mean of $x$ or expectation of $x$, denoted by $\\bar{x}$ or $E(x)$, is given by the formula\n\n$$\n\\bar{x} = E(x) = p_1x_1 + p_2x_2 + \\cdots + p_nx_n\n$$\n\n::: callout-note\nAmong the first $N$ experiments, denote by $S_j$ the number of instances with which the $j$th outcome was observed. Then, the average numerical outcome is\n\n$$\n\\frac{S_1x_1 + S_2x_2 + \\cdots + S_nx_n}{N}\n$$\n\nAs $N \\to \\infty$, we have\n\n$$\n\\bar{x} = E(x) = \\lim\\limits_{N \\to \\infty} \\frac{S_1x_1 + S_2x_2 + \\cdots + S_nx_n}{N} = p_1x_1 + p_2x_2 + \\cdots + p_nx_n\n$$\n:::\n\n3.  Variance\n\nNext we are tempted to know such a fact: by how much do the numerical outcomes differ on average from the mean?\n\nThis is characterized by the variance, the expectation of the square of the difference of the numerical outcome and its mean:\n\n$$\nV = \\overline{(x - \\bar{x})^2} = E((x - \\bar{x})^2)\n$$\n\n::: callout-note\n$$\n\\begin{aligned}\nV & = \\overline{(x - \\bar{x})^2} \\\\\n  & = E((x - \\bar{x})^2) \\\\\n  & = \\sum_{j=1}^{n} p_j (x_j - \\bar{x})^2 \\\\\n  & = p_1x_1^2 + \\cdots + p_nx_n^2 - 2(p_1x_1 + \\cdots + p_nx_n)\\bar{x} + (\\bar{x})^2 \\\\\n  & = \\bar{x^2} - (\\bar{x})^2 \\\\\n  & = E(x^2) - (E(x))^2\n\\end{aligned}\n$$\n:::\n\n**Note:** the square root of the variance $\\sqrt{V}$ is called the standard deviation.\n\n##### Some special distributions\n\n1.  The binomial distribution\n\nSuppose a random experiment has two possible outcomes $A$ and $B$, with probabilities $p$ and $q$ respectively, where $p + q = 1$.\n\nSuppose we repeat the experiment $N$ times, and the repeated experiments are independent of each other.\n\nIf we let $k$ ($k = 0, 1, ..., N$) denote the number of times with which A occurs, then the probability that A occurs exactly $k$ times is given by the formula\n\n$$\nb_k(N) = \\dbinom{N}{k} p^k q^{N-k}\n$$\n\n::: callout-note\nSince the outcomes of the experiments are independent of each other, the probability of a particular sequence of $k$ $A$'s and $N - k$ $B$'s is $p^k q^{N-k}$.\n\nIn addition, there are exactly $\\dbinom{N}{k}$ arrangements of $k$ $A$'s and $N - k$ $B$'s, which are disjoint of each other.\n:::\n\nIn addition, we have\n\n$$\nE(x) = \\sum_{k=0}^{N} kp(x = k) = Np\n$$\n\n**Note:** the binomial theorem is $(a + b)^N = \\sum_{k=0}^{N} \\dbinom{N}{k} a^k b^{N-k}$.\n\n1.  The Poisson distribution\n\nSuppose **each week** there are **a large number of vehicles** through a busy intersection and there are **on average** $\\mu$ accidents.\n\nSuppose the probability of a vehicle having an accident is independent of the occurrence of previous accidents.\n\nThen we use a binomial distribution to determine the probability of $k$ accidents in a week:\n\nSetting $p = \\frac{\\mu}{N}$, then we have\n\n$$\n\\begin{aligned}\nb_k(N) & = \\dbinom{N}{k} p^k q^{N-k} \\\\\n       & = \\frac{N(N-1) \\cdots (N-k+1)}{k!} p^k q^{N-k} \\\\\n       & = (1-\\frac{1}{N}) \\cdots (1-\\frac{k-1}{N}) \\frac{N^kp^k(1-p)^{N-k}}{k!} \\\\\n       & = \\frac{(1-\\frac{1}{N}) \\cdots (1-\\frac{k-1}{N})}{(1-p)^k} \\frac{\\mu^k}{k!} (1-\\frac{\\mu}{N})^N\n\\end{aligned}\n$$\n\nSince $e^{-x} = \\lim\\limits_{n \\to \\infty} (1-\\frac{x}{n})^n$ ($e^x = \\lim\\limits_{n \\to \\infty} (1+\\frac{x}{n})^n$), then we have\n\n$$\n\\lim\\limits_{N \\to \\infty,\\ \\mu = Np} b_k(N) = \\frac{\\mu^k}{k!} e^{-\\mu}\n$$\n\nThis gives us an estimate for $b_k(N)$ when $N$ is **large**, $p$ is **small**, and $Np = \\mu$.\n\nThe Poisson distribution is defined as\n\n$$\np_k(\\mu) = \\frac{\\mu^k}{k!} e^{-\\mu}\n$$\n\nwhere $\\mu$ is a parameter. $p_k$ is the probability of $k$ favorable outcomes, $k = 0, 1, ...$.\n\nIn addition, the combination of two Poisson processes is again a Poisson process.\n\nDenote by $p_k(\\mu)$ and $p_k(\\nu)$ the probability of $k$ favorable outcomes in these two processes. We claim that the probability of $k$ favorable outcomes when both experiments are performed is $p_k(\\mu + \\nu)$.\n\n::: callout-note\nThere will be $k$ favorable outcomes for the combined experiment if the first experiment has $j$ ($j = 0, 1, ..., k$) favorable outcomes and the second experiment has $k - j$.\n\nIf the experiments are independent, the probability of such a combined outcome is the product of the probabilities $p_j(\\mu) p_{k-j}(\\nu)$.\n\nSo the probability of the combined experiment to have $k$ favorable outcomes is the sum\n\n$$\n\\begin{aligned}\n\\sum_j p_j(\\mu) p_{k-j}(\\nu) & = \\sum_j \\frac{\\mu^j}{j!} e^{-\\mu} \\frac{\\nu^{k-j}}{(k-j)!} e^{-\\nu} \\\\\n                             & = \\frac{1}{k!} e^{-(\\mu + \\nu)} \\sum_j \\frac{k!}{j!(k-j)!} \\mu^j \\nu^{(k-j)} \\\\\n                             & = \\frac{(\\mu + \\nu)^k}{k!} e^{-(\\mu + \\nu)} \n\\end{aligned}\n$$\n\nwhich is the Poisson distribution $p_k(\\mu + \\nu)$.\n:::\n\n#### Continuous probability\n\nSuppose we have such an experiment making a physical measurement with an apparatus subject to random disturbances that can be reduced but not totally eliminated. Then every real number is a possible numerical outcome of such an experiment.\n\nRepeat the experiment as many times as we wish and denote by $S(x)$ the number of instances among the first $N$ performances for which the numerical outcome was **less than** $x$.Then the frequency $\\frac{S(x)}{N}$ with which this event occurs tends to a limit as $N$ tends to infinity. This limit is **the probability** that **the outcome is less than** $\\mathbfcal{x}$, and is denoted by $P(x)$:\n\n$$\nP(x) = \\lim\\limits_{N \\to \\infty} \\frac{S(x)}{N}\n$$\n\nThe probability $P(x)$ has the following properties:\n\n1.  $0 \\leq P(x) \\leq 1$: $0 \\leq S(x) \\leq N \\implies 0 \\leq \\frac{S(x)}{N} \\leq 1 \\implies 0 \\leq P(x) \\leq 1$.\n\n2.  $P(x)$ is a **nondecreasing** function of $x$: $S(x)$ is a **nondecreasing** function of $x$, so that $\\frac{S(x)}{N}$ is a **nondecreasing** function of $x$; then so is the limit $P(x)$.\n\n3.  $P(x) \\to 0\\ (x \\to -\\infty)$.\n\n4.  $P(x) \\to 1\\ (x \\to \\infty)$.\n\n5.  $P(x)$ is a continuously differentiable function (denote the derivative of $P$ by $p$): $\\frac{\\mathrm{d}P(x)}{dx} = p(x)$.\n\nThe function $p(x)$ is called the **probability density function**.\n\n::: {.callout-note title=\"Addition rule for disjoint events\"}\nSuppose $E$ and $F$ are two events with probabilities $P(E)$ and $P(F)$ respectively.\n\nSuppose they are **disjoint** (i.e. $E \\cap F = \\emptyset$).\n\nThen we have\n\n$$\nP(E \\cup F) = P(E) + P(F)\n$$\n\nLet $E: x < a$, $F: a \\leq x < b$, then we have $E \\cup F: x < b$.\n\nThen we have\n\n$$\nP(E) = P(a),\\ P(E \\cup F) = P(b)\n$$\n\nWe conclude that\n\n$$\nP(F) = P(b) - P(a)\n$$\n\nis the probability of $a \\leq x < b$.\n:::\n\nAccording to the **mean value theorem**, for every $a$ and $b$, there is a number $c$ lying between $a$ and $b$ such that\n\n$$\nP(b) - P(a) = p(c)(b - a)\n$$\n\nAccording to the **fundamental theorem of calculus**\n\n$$\nP(b) - P(a) = \\int_a^b p(x)\\mathrm{d}x\n$$\n\nAccording to $P(a) \\to 0\\ (a \\to -\\infty)$\n\n$$\nP(b) = \\int_{-\\infty}^b p(x)\\mathrm{d}x\n$$\n\nAccording to $P(b) \\to 1\\ (b \\to \\infty)$\n\n$$\n1 = \\int_{-\\infty}^{\\infty} p(x)\\mathrm{d}x\n$$\n\nThis is the continuous analogue of the basic fact that $p_1 + p_2 + \\cdots + p_n = 1$ in discrete probability.\n\n6.  $p(x) \\geq 0$: $P(x)$ is a nondecreasing function of $x$.\n\n7.  The expectation is:\n\n$$\n\\bar{x} = \\int_{-\\infty}^{\\infty} xp(x)\\mathrm{d}x\n$$\n\n::: callout-note\nImagine the experiment performed as many times as we wish, and denote the sequence of outcomes by\n\n$$\na_1, a_2, ..., a_N, ...\n$$\n\nDivide the interval $I$ **in which all outcomes lie** into $n$ subintervals $I_1, ..., I_n$. Denote the endpoints by\n\n$$\ne_0 < e_1 < \\cdots < e_n\n$$\n\nThe probability of $e_{j-1} \\leq x < e_j$ (i.e. $x$ lies in the interval $I_j$) is\n\n$$\nP_j = P(e_j) - P(e_{j-1}) = p(x_j)(e_j - e_{j-1})\n$$\n\nwhere $x_j$ is a point in $I_j$ guaranteed by the mean value theorem, and $e_j - e_{j-1}$ denotes the length of $I_j$.\n\nWe now simplify the original experiment by recording merely the intervals $I_j$ in which the outcome falls, and calling the numerical outcome in this case $x_j$, the point in $I_j$ appears in the above formula; therefore, the actual outcome falling into $I_j$ differs from $x_j$ by at most $e_j - e_{j-1}$.\n\nNow consider the sequence of outcomes $a_1, a_2, ...$ of the **original experiment**. Denote the corresponding outcomes of the simplified experiment by $b_1, b_2, ...$. The simplified experiment has a finite number of outcomes. For such discrete experiments, we have the expectation\n\n$$\n\\bar{x}_n = \\lim\\limits_{N \\to \\infty} \\frac{b_1 + \\cdots + b_N}{N}\n$$\n\nwhere $n$ is the number of subintervals of $I$.\n\nIn fact, the expectation $\\bar{x}_n$ of the simplified experiment can also be calculated by formula\n\n$$\n\\bar{x}_n = P_1x_1 + \\cdots + P_nx_n\n$$\n\nThen we have\n\n$$\n\\bar{x}_n = p(x_1)x_1(e_1 - e_0) + \\cdots + p(x_n)x_n(e_n - e_{n-1})\n$$\n\nAs $n \\to \\infty$, we have\n\n$$\n\\bar{x} = \\lim\\limits_{n \\to \\infty} \\sum_{i=1}^n x_ip(x_i)\\Delta x_i = \\int_{e_0}^{e_n} xp(x)\\mathrm{d}x\n$$\n\nSo we conclude\n\n$$\n\\bar{x} = \\int_{-\\infty}^{\\infty} xp(x)\\mathrm{d}x\n$$\n:::\n\n8.  The probability density of a combined experiment of two experiments independent of each other:\n\n-   Case 1: the outcome of the first experiment may be any real number, but the second experiment can have only a finite number of outcomes.\n\nDenote by $P(a)$ the probability of $x < a$. The second experiment has $n$ possible outcomes $a_1, ..., a_n$ with probabilities $Q_1, ..., Q_n$.\n\n**We define *the numerical outcome of the combined experiment* to be *the sum of the separate numerical outcomes of the two experiments* that constitute it.**\n\nWe denote by $E(x)$ the event that the numerical outcome of the combined experiment is less than $x$, and denote its probability by $U(x)$.\n\nThen we have\n\n$$\nU(x) = Q_1P(x-a_1) + \\cdots + Q_nP(x-a_n)\n$$\n\n::: callout-note\nWe denote by $E_j(x)$ the event that the numerical outcome of the second experiment is $a_j$. The numerical outcome of the combined experiment is then less than $x$ if and only if the numerical outcome of the first experiment is less than $x - a_j$.\n\nThen we have\n\n$$\nE(x) = E_1(x) \\cup \\cdots \\cup E_n(x)\n$$\n\nwhere the events $E_j(x)$ are disjoint.\n\nIt follows from **the addition rule for disjoint events** that\n\n$$\nU(x) = P(E_1(x)) + \\cdots + P(E_n(x))\n$$\n\nSince the two experiments are independent, we have $P(E_j(x)) = Q_j P(x-a_j)$.\n\nSo we have\n\n$$\nU(x) = Q_1P(x-a_1) + \\cdots + Q_nP(x-a_n)\n$$\n:::\n\n-   Case 2: both experiments can have any real number as outcome.\n\nDenote by $P(a)$ and $Q(a)$ the probabilities that the outcome is less than $a$ in each of the two experiments, respectively.\n\nAssume the outcome of the second experiment always lies in some finite interval $I$. Then we subdivide $I$ into a finite number $n$ of subintervals $I_j = [e_{j-1}, e_j)$. Let $Q_j$ denote the probability of the outcome of the experiment lying in $I_j$.\n\nSuppose $Q(x)$ is continuously differentiable and denote its derivative by $q(x)$.\n\nAccording to the mean value theorem, we have\n\n$$\nQ_j = Q(e_j) - Q(e_{j-1}) = q(a_j)(e_j - e_{j-1})\n$$\n\nwhere $a_j$ is some point in $I_j$.\n\nWe discretize the second experiment by lumping together all outcomes that lie in $I_j$ and redefine the numerical outcome in that case to be $a_j$.\n\nThen we have\n\n$$\n\\begin{aligned}\nU_n(x) & = q(a_1)P(x-a_1)(e_1-e_0) + \\cdots + q(a_n)P(x-a_n)(e_n-e_{n-1}) \\\\\n       & = \\sum_{i=1}^{n} q(a_i)p(x-a_i)\\Delta a_i\n\\end{aligned}\n$$\n\nAs $n \\to \\infty$, we have\n\n$$\n\\begin{aligned}\nU(x) & = \\lim\\limits_{n \\to \\infty} U_n(x) \\\\\n     & = \\lim\\limits_{n \\to \\infty} \\sum_{i=1}^{n} q(a_i)p(x-a_i)\\Delta a_i \\\\\n     & = \\int\\limits_{I} q(a)P(x-a)\\mathrm{d}a\n\\end{aligned}\n$$\n\nThen we have\n\n$$\nU(x) = \\int_{-\\infty}^{\\infty} q(a)P(x-a)\\mathrm{d}a\n$$\n\nFurther, let us suppose $P(x)$ is continuously differentiable, and denote its derivative by $p(x)$.\n\nThen we have\n\n$$\nu(x) = \\int_{-\\infty}^{\\infty} q(a)p(x-a)\\mathrm{d}a\n$$\n\nwhere $u(x)$ is the derivative of $U(x)$.\n\nIn a word, we have proved the following fact:\n\n::: callout-note\nConsider two independent experiments whose outcomes lie in some finite interval and have probability $p$ and $q$ respectively.\n\nIn the combined experiment of the two experiments, define the outcome of the combined experiment to be the sum of the outcomes of the individual experiments.\n\nThen the combined experiment has the probability density:\n\n$$\nu(x) = \\int_{-\\infty}^{\\infty} q(a)p(x-a)\\mathrm{d}a\n$$\n:::\n\n9. The convolution of the functions $q$ and $p$:\n\nThe function $u$ defined by $u(x) = \\int_{-\\infty}^{\\infty} q(a)p(x-a)\\mathrm{d}a$ is called the convolution of the functions $q$ and $p$. This relation is denoted by $u = q*p$.\n\nThe convolution has the following properties:\n\n::: callout-note\nLet $q_1$, $q_2$, and $p$ be continuous functions defined for all real numbers $x$, and assume the functions are $0$ outside a finite interval. Then we have\n\n-   Convolution is distributive: $(q_1+q_2)*p = q_1*p + q_2*p$.\n\n-   Let $k$ be any constant. Then $(kq)*p = k(q*p)$.\n\n-   Convolution is commutative: $q*p = p*q$.\n\n-   The integral of the convolution is the product of the integrals of the factors:\n\n$$\n\\int_{-\\infty}^{\\infty} u(x) \\mathrm{d}x = \\int_{-\\infty}^{\\infty} p(x) \\mathrm{d}x \\int_{-\\infty}^{\\infty} q(a) \\mathrm{d}a\n$$\n:::\n","srcMarkdownNoYaml":"\n\n## From the book *Calculus with applications* by *Peter D. Lax*\n\n### Probability\n\nProbability is the branch of mathematics that deals with **events** whose **individual outcomes** are **unpredictable**, but whose outcomes **on average** are **predictable**.\n\nExperiments can be divided into two types:\n\n-   **Deterministic:** whose individual outcomes are predictable;\n\n-   **Nondeterministic (random):** whose individual outcomes are unpredictable.\n\n#### Discrete probability\n\nNext, all experiments we'll deal with are *repeatable* (it can be performed repeatedly any number of times) and *random* (any single performance of the experiment is unpredictable).\n\nIn this section, we'll deal with experiments having *a finite number of possible outcomes*. We denote the number of possible outcomes by $n$, and number them from $1$ to $n$.\n\n##### The probability of any single outcome\n\nIn a random and repeatable experiment, if we denote $S_j$ by *the number of instances* among the first $N$ experiments where the $j$th outcome was observed to occur, then the frequency $\\frac{S_j}{N}$ with which the $j$th outcome has been observed to occur tends to a limit as $N$ tends to infinity. We call this limit the probability of the $j$th outcome and denote it by $p_j$:\n\n$$\np_j = \\lim\\limits_{N \\to \\infty} \\frac{S_j}{N}\n$$\n\nThese probabilities have the following properties:\n\n-   $0 \\leq p_j \\leq 1$;\n\n::: callout-note\nFor $\\frac{S_j}{N}$ lies between $0$ and $1$, and therefore so does its limit $p_j$.\n:::\n\n-   $\\sum_{j=1}^{n} p_j = 1$.\n\n::: callout-note\nWe have\n\n$$\nS_1 + S_2 + \\cdots + S_n = N\n$$\n\nDividing by $N$, we get\n\n$$\n\\frac{S_1}{N} + \\frac{S_2}{N} + \\cdots + \\frac{S_n}{N} = 1\n$$\n\nAs $N$ tends to infinity, we have\n\n$$\n\\lim\\limits_{N \\to \\infty} \\sum_{j=1}^{n} \\frac{S_j}{N} = \\lim\\limits_{N \\to \\infty} \\sum_{j=1}^{n} p_j = 1\n$$\n:::\n\n##### The probability of an event\n\nIn fact very often, we are not interested in all the details of the outcome of an experiment, but merely in a particular aspect of it (e.g. throwing a die, we may be interested only in whether the outcome is even or odd).\n\nAn occurrence such as throwing an even number is called an *event*, which is defined as the following:\n\n> An event $E$ is defined as **any collection of possible outcomes**.\n\n**Note:** we say that an event $E$ occurred whenever any outcome belonging to $E$ occurred.\n\nThe probability $p(E)$ of an event $E$ is\n\n$$\np(E) = \\lim\\limits_{N \\to \\infty} \\frac{S(E)}{N}\n$$\n\nwhere $S(E)$ is the number of instances among the first $N$ experiments when the event $E$ took place.\n\n::: callout-note\nWe have\n\n$$\nS(E) = \\sum_{j\\ \\text{in}\\ E} S_j\n$$\n\nDividing by $N$\n\n$$\n\\frac{S(E)}{N} = \\sum_{j\\ \\text{in}\\ E} \\frac{S_j}{N}\n$$\n\nAs $N$ tends to infinity, we have\n\n$$\np(E) = \\sum_{j\\ \\text{in}\\ E} p_j\n$$\n:::\n\n###### The arithmetic rules of probability of some special events\n\n1.  Addition rule for disjoint events\n\nTwo events $E_1$ and $E_2$ are called *disjoint* if both cannot take place simultaneously (i.e. $E_1 \\cap E_2 = \\emptyset$).\n\nThen we have\n\n$$\np(E_1 \\cup E_2) = p(E_1) + p(E_2)\n$$\n\n::: callout-note\n$$\np(E_1 \\cup E_2) = \\sum_{j\\ \\text{in}\\ E_1\\ \\text{or}\\ E_2} p_j\n$$\n\nDisjointness means that an outcome $j$ may belong either to $E_1$ or to $E_2$ **but not to both**; therefore,\n\n$$\np(E_1 \\cup E_2) = \\sum_{j\\ \\text{in}\\ E_1\\ \\text{or}\\ E_2} p_j = \\sum_{j\\ \\text{in}\\ E_1} p_j + \\sum_{j\\ \\text{in}\\ E_2} p_j = p(E_1) + p(E_2)\n$$\n:::\n\n2.  Product rule for independent events\n\nTwo events $E$ and $F$ are called *independent* if the outcome of one cannot influence the other, nor is the outcome of both under the influence of a common cause.\n\nThen we have\n\n$$\np(E \\cap F) = p(E)p(F)\n$$\n\n::: callout-note\nAmong the first $N$ experiments, count the number of times $E$ has occurred ($S(E)$), F has occurred ($S(F)$), and $E \\cap F$ has occurred ($S(E \\cap F)$). Then we have\n\n$$\np(E) = \\lim\\limits_{N \\to \\infty} \\frac{S(E)}{N}\n$$\n\n$$\np(F) = \\lim\\limits_{N \\to \\infty} \\frac{S(F)}{N}\n$$\n\n$$\np(E \\cap F) = \\lim\\limits_{N \\to \\infty} \\frac{S(E \\cap F)}{N}\n$$\n\nSuppose that we single out from the sequence of combined experiments the subsequence of those where $E$ occurred. The frequency of occurrence of $F$ in this subsequence is $\\frac{S(E \\cap F)}{S(E)}$. If the two events $E$ and $F$ are truly independent, the frequency with which $F$ occurs in this subsequence should be the same as the frequency with which $F$ occurs in the original sequence, i.e.\n\n$$\n\\lim\\limits_{N \\to \\infty} \\frac{S(E \\cap F)}{S(E)} = \\lim\\limits_{N \\to \\infty} \\frac{S(F)}{N} = p(F)\n$$\n\nWe write the frequency of $\\frac{S(E \\cap F)}{N}$ as the product\n\n$$\n\\frac{S(E \\cap F)}{N} = \\frac{S(E \\cap F)}{S(E)}\\frac{S(E)}{N}\n$$\n\nThen we have\n\n$$\np(E \\cap F) = \\lim\\limits_{N \\to \\infty} \\frac{S(E \\cap F)}{N} = \\lim\\limits_{N \\to \\infty} \\frac{S(E \\cap F)}{S(E)} \\cdot \\lim\\limits_{N \\to \\infty} \\frac{S(E)}{N} = p(F)p(E)\n$$\n:::\n\n##### Characteristics of random variables\n\n1.  Numerical outcome\n\nThe numerical outcome of an experiment means the assignment of a real number $x_j$ to each of the possible outcomes.\n\nNote that different outcomes may be assigned the same number so we have to re-calculate the probability $p(x_j)$ for each $x_j$ with which $x_j$ occurred.\n\n2.  Expectation\n\nIn a random experiment with $n$ possible outcomes of probability $p_j$ and numerical outcome $x_j$, the average numerical outcome, called the mean of $x$ or expectation of $x$, denoted by $\\bar{x}$ or $E(x)$, is given by the formula\n\n$$\n\\bar{x} = E(x) = p_1x_1 + p_2x_2 + \\cdots + p_nx_n\n$$\n\n::: callout-note\nAmong the first $N$ experiments, denote by $S_j$ the number of instances with which the $j$th outcome was observed. Then, the average numerical outcome is\n\n$$\n\\frac{S_1x_1 + S_2x_2 + \\cdots + S_nx_n}{N}\n$$\n\nAs $N \\to \\infty$, we have\n\n$$\n\\bar{x} = E(x) = \\lim\\limits_{N \\to \\infty} \\frac{S_1x_1 + S_2x_2 + \\cdots + S_nx_n}{N} = p_1x_1 + p_2x_2 + \\cdots + p_nx_n\n$$\n:::\n\n3.  Variance\n\nNext we are tempted to know such a fact: by how much do the numerical outcomes differ on average from the mean?\n\nThis is characterized by the variance, the expectation of the square of the difference of the numerical outcome and its mean:\n\n$$\nV = \\overline{(x - \\bar{x})^2} = E((x - \\bar{x})^2)\n$$\n\n::: callout-note\n$$\n\\begin{aligned}\nV & = \\overline{(x - \\bar{x})^2} \\\\\n  & = E((x - \\bar{x})^2) \\\\\n  & = \\sum_{j=1}^{n} p_j (x_j - \\bar{x})^2 \\\\\n  & = p_1x_1^2 + \\cdots + p_nx_n^2 - 2(p_1x_1 + \\cdots + p_nx_n)\\bar{x} + (\\bar{x})^2 \\\\\n  & = \\bar{x^2} - (\\bar{x})^2 \\\\\n  & = E(x^2) - (E(x))^2\n\\end{aligned}\n$$\n:::\n\n**Note:** the square root of the variance $\\sqrt{V}$ is called the standard deviation.\n\n##### Some special distributions\n\n1.  The binomial distribution\n\nSuppose a random experiment has two possible outcomes $A$ and $B$, with probabilities $p$ and $q$ respectively, where $p + q = 1$.\n\nSuppose we repeat the experiment $N$ times, and the repeated experiments are independent of each other.\n\nIf we let $k$ ($k = 0, 1, ..., N$) denote the number of times with which A occurs, then the probability that A occurs exactly $k$ times is given by the formula\n\n$$\nb_k(N) = \\dbinom{N}{k} p^k q^{N-k}\n$$\n\n::: callout-note\nSince the outcomes of the experiments are independent of each other, the probability of a particular sequence of $k$ $A$'s and $N - k$ $B$'s is $p^k q^{N-k}$.\n\nIn addition, there are exactly $\\dbinom{N}{k}$ arrangements of $k$ $A$'s and $N - k$ $B$'s, which are disjoint of each other.\n:::\n\nIn addition, we have\n\n$$\nE(x) = \\sum_{k=0}^{N} kp(x = k) = Np\n$$\n\n**Note:** the binomial theorem is $(a + b)^N = \\sum_{k=0}^{N} \\dbinom{N}{k} a^k b^{N-k}$.\n\n1.  The Poisson distribution\n\nSuppose **each week** there are **a large number of vehicles** through a busy intersection and there are **on average** $\\mu$ accidents.\n\nSuppose the probability of a vehicle having an accident is independent of the occurrence of previous accidents.\n\nThen we use a binomial distribution to determine the probability of $k$ accidents in a week:\n\nSetting $p = \\frac{\\mu}{N}$, then we have\n\n$$\n\\begin{aligned}\nb_k(N) & = \\dbinom{N}{k} p^k q^{N-k} \\\\\n       & = \\frac{N(N-1) \\cdots (N-k+1)}{k!} p^k q^{N-k} \\\\\n       & = (1-\\frac{1}{N}) \\cdots (1-\\frac{k-1}{N}) \\frac{N^kp^k(1-p)^{N-k}}{k!} \\\\\n       & = \\frac{(1-\\frac{1}{N}) \\cdots (1-\\frac{k-1}{N})}{(1-p)^k} \\frac{\\mu^k}{k!} (1-\\frac{\\mu}{N})^N\n\\end{aligned}\n$$\n\nSince $e^{-x} = \\lim\\limits_{n \\to \\infty} (1-\\frac{x}{n})^n$ ($e^x = \\lim\\limits_{n \\to \\infty} (1+\\frac{x}{n})^n$), then we have\n\n$$\n\\lim\\limits_{N \\to \\infty,\\ \\mu = Np} b_k(N) = \\frac{\\mu^k}{k!} e^{-\\mu}\n$$\n\nThis gives us an estimate for $b_k(N)$ when $N$ is **large**, $p$ is **small**, and $Np = \\mu$.\n\nThe Poisson distribution is defined as\n\n$$\np_k(\\mu) = \\frac{\\mu^k}{k!} e^{-\\mu}\n$$\n\nwhere $\\mu$ is a parameter. $p_k$ is the probability of $k$ favorable outcomes, $k = 0, 1, ...$.\n\nIn addition, the combination of two Poisson processes is again a Poisson process.\n\nDenote by $p_k(\\mu)$ and $p_k(\\nu)$ the probability of $k$ favorable outcomes in these two processes. We claim that the probability of $k$ favorable outcomes when both experiments are performed is $p_k(\\mu + \\nu)$.\n\n::: callout-note\nThere will be $k$ favorable outcomes for the combined experiment if the first experiment has $j$ ($j = 0, 1, ..., k$) favorable outcomes and the second experiment has $k - j$.\n\nIf the experiments are independent, the probability of such a combined outcome is the product of the probabilities $p_j(\\mu) p_{k-j}(\\nu)$.\n\nSo the probability of the combined experiment to have $k$ favorable outcomes is the sum\n\n$$\n\\begin{aligned}\n\\sum_j p_j(\\mu) p_{k-j}(\\nu) & = \\sum_j \\frac{\\mu^j}{j!} e^{-\\mu} \\frac{\\nu^{k-j}}{(k-j)!} e^{-\\nu} \\\\\n                             & = \\frac{1}{k!} e^{-(\\mu + \\nu)} \\sum_j \\frac{k!}{j!(k-j)!} \\mu^j \\nu^{(k-j)} \\\\\n                             & = \\frac{(\\mu + \\nu)^k}{k!} e^{-(\\mu + \\nu)} \n\\end{aligned}\n$$\n\nwhich is the Poisson distribution $p_k(\\mu + \\nu)$.\n:::\n\n#### Continuous probability\n\nSuppose we have such an experiment making a physical measurement with an apparatus subject to random disturbances that can be reduced but not totally eliminated. Then every real number is a possible numerical outcome of such an experiment.\n\nRepeat the experiment as many times as we wish and denote by $S(x)$ the number of instances among the first $N$ performances for which the numerical outcome was **less than** $x$.Then the frequency $\\frac{S(x)}{N}$ with which this event occurs tends to a limit as $N$ tends to infinity. This limit is **the probability** that **the outcome is less than** $\\mathbfcal{x}$, and is denoted by $P(x)$:\n\n$$\nP(x) = \\lim\\limits_{N \\to \\infty} \\frac{S(x)}{N}\n$$\n\nThe probability $P(x)$ has the following properties:\n\n1.  $0 \\leq P(x) \\leq 1$: $0 \\leq S(x) \\leq N \\implies 0 \\leq \\frac{S(x)}{N} \\leq 1 \\implies 0 \\leq P(x) \\leq 1$.\n\n2.  $P(x)$ is a **nondecreasing** function of $x$: $S(x)$ is a **nondecreasing** function of $x$, so that $\\frac{S(x)}{N}$ is a **nondecreasing** function of $x$; then so is the limit $P(x)$.\n\n3.  $P(x) \\to 0\\ (x \\to -\\infty)$.\n\n4.  $P(x) \\to 1\\ (x \\to \\infty)$.\n\n5.  $P(x)$ is a continuously differentiable function (denote the derivative of $P$ by $p$): $\\frac{\\mathrm{d}P(x)}{dx} = p(x)$.\n\nThe function $p(x)$ is called the **probability density function**.\n\n::: {.callout-note title=\"Addition rule for disjoint events\"}\nSuppose $E$ and $F$ are two events with probabilities $P(E)$ and $P(F)$ respectively.\n\nSuppose they are **disjoint** (i.e. $E \\cap F = \\emptyset$).\n\nThen we have\n\n$$\nP(E \\cup F) = P(E) + P(F)\n$$\n\nLet $E: x < a$, $F: a \\leq x < b$, then we have $E \\cup F: x < b$.\n\nThen we have\n\n$$\nP(E) = P(a),\\ P(E \\cup F) = P(b)\n$$\n\nWe conclude that\n\n$$\nP(F) = P(b) - P(a)\n$$\n\nis the probability of $a \\leq x < b$.\n:::\n\nAccording to the **mean value theorem**, for every $a$ and $b$, there is a number $c$ lying between $a$ and $b$ such that\n\n$$\nP(b) - P(a) = p(c)(b - a)\n$$\n\nAccording to the **fundamental theorem of calculus**\n\n$$\nP(b) - P(a) = \\int_a^b p(x)\\mathrm{d}x\n$$\n\nAccording to $P(a) \\to 0\\ (a \\to -\\infty)$\n\n$$\nP(b) = \\int_{-\\infty}^b p(x)\\mathrm{d}x\n$$\n\nAccording to $P(b) \\to 1\\ (b \\to \\infty)$\n\n$$\n1 = \\int_{-\\infty}^{\\infty} p(x)\\mathrm{d}x\n$$\n\nThis is the continuous analogue of the basic fact that $p_1 + p_2 + \\cdots + p_n = 1$ in discrete probability.\n\n6.  $p(x) \\geq 0$: $P(x)$ is a nondecreasing function of $x$.\n\n7.  The expectation is:\n\n$$\n\\bar{x} = \\int_{-\\infty}^{\\infty} xp(x)\\mathrm{d}x\n$$\n\n::: callout-note\nImagine the experiment performed as many times as we wish, and denote the sequence of outcomes by\n\n$$\na_1, a_2, ..., a_N, ...\n$$\n\nDivide the interval $I$ **in which all outcomes lie** into $n$ subintervals $I_1, ..., I_n$. Denote the endpoints by\n\n$$\ne_0 < e_1 < \\cdots < e_n\n$$\n\nThe probability of $e_{j-1} \\leq x < e_j$ (i.e. $x$ lies in the interval $I_j$) is\n\n$$\nP_j = P(e_j) - P(e_{j-1}) = p(x_j)(e_j - e_{j-1})\n$$\n\nwhere $x_j$ is a point in $I_j$ guaranteed by the mean value theorem, and $e_j - e_{j-1}$ denotes the length of $I_j$.\n\nWe now simplify the original experiment by recording merely the intervals $I_j$ in which the outcome falls, and calling the numerical outcome in this case $x_j$, the point in $I_j$ appears in the above formula; therefore, the actual outcome falling into $I_j$ differs from $x_j$ by at most $e_j - e_{j-1}$.\n\nNow consider the sequence of outcomes $a_1, a_2, ...$ of the **original experiment**. Denote the corresponding outcomes of the simplified experiment by $b_1, b_2, ...$. The simplified experiment has a finite number of outcomes. For such discrete experiments, we have the expectation\n\n$$\n\\bar{x}_n = \\lim\\limits_{N \\to \\infty} \\frac{b_1 + \\cdots + b_N}{N}\n$$\n\nwhere $n$ is the number of subintervals of $I$.\n\nIn fact, the expectation $\\bar{x}_n$ of the simplified experiment can also be calculated by formula\n\n$$\n\\bar{x}_n = P_1x_1 + \\cdots + P_nx_n\n$$\n\nThen we have\n\n$$\n\\bar{x}_n = p(x_1)x_1(e_1 - e_0) + \\cdots + p(x_n)x_n(e_n - e_{n-1})\n$$\n\nAs $n \\to \\infty$, we have\n\n$$\n\\bar{x} = \\lim\\limits_{n \\to \\infty} \\sum_{i=1}^n x_ip(x_i)\\Delta x_i = \\int_{e_0}^{e_n} xp(x)\\mathrm{d}x\n$$\n\nSo we conclude\n\n$$\n\\bar{x} = \\int_{-\\infty}^{\\infty} xp(x)\\mathrm{d}x\n$$\n:::\n\n8.  The probability density of a combined experiment of two experiments independent of each other:\n\n-   Case 1: the outcome of the first experiment may be any real number, but the second experiment can have only a finite number of outcomes.\n\nDenote by $P(a)$ the probability of $x < a$. The second experiment has $n$ possible outcomes $a_1, ..., a_n$ with probabilities $Q_1, ..., Q_n$.\n\n**We define *the numerical outcome of the combined experiment* to be *the sum of the separate numerical outcomes of the two experiments* that constitute it.**\n\nWe denote by $E(x)$ the event that the numerical outcome of the combined experiment is less than $x$, and denote its probability by $U(x)$.\n\nThen we have\n\n$$\nU(x) = Q_1P(x-a_1) + \\cdots + Q_nP(x-a_n)\n$$\n\n::: callout-note\nWe denote by $E_j(x)$ the event that the numerical outcome of the second experiment is $a_j$. The numerical outcome of the combined experiment is then less than $x$ if and only if the numerical outcome of the first experiment is less than $x - a_j$.\n\nThen we have\n\n$$\nE(x) = E_1(x) \\cup \\cdots \\cup E_n(x)\n$$\n\nwhere the events $E_j(x)$ are disjoint.\n\nIt follows from **the addition rule for disjoint events** that\n\n$$\nU(x) = P(E_1(x)) + \\cdots + P(E_n(x))\n$$\n\nSince the two experiments are independent, we have $P(E_j(x)) = Q_j P(x-a_j)$.\n\nSo we have\n\n$$\nU(x) = Q_1P(x-a_1) + \\cdots + Q_nP(x-a_n)\n$$\n:::\n\n-   Case 2: both experiments can have any real number as outcome.\n\nDenote by $P(a)$ and $Q(a)$ the probabilities that the outcome is less than $a$ in each of the two experiments, respectively.\n\nAssume the outcome of the second experiment always lies in some finite interval $I$. Then we subdivide $I$ into a finite number $n$ of subintervals $I_j = [e_{j-1}, e_j)$. Let $Q_j$ denote the probability of the outcome of the experiment lying in $I_j$.\n\nSuppose $Q(x)$ is continuously differentiable and denote its derivative by $q(x)$.\n\nAccording to the mean value theorem, we have\n\n$$\nQ_j = Q(e_j) - Q(e_{j-1}) = q(a_j)(e_j - e_{j-1})\n$$\n\nwhere $a_j$ is some point in $I_j$.\n\nWe discretize the second experiment by lumping together all outcomes that lie in $I_j$ and redefine the numerical outcome in that case to be $a_j$.\n\nThen we have\n\n$$\n\\begin{aligned}\nU_n(x) & = q(a_1)P(x-a_1)(e_1-e_0) + \\cdots + q(a_n)P(x-a_n)(e_n-e_{n-1}) \\\\\n       & = \\sum_{i=1}^{n} q(a_i)p(x-a_i)\\Delta a_i\n\\end{aligned}\n$$\n\nAs $n \\to \\infty$, we have\n\n$$\n\\begin{aligned}\nU(x) & = \\lim\\limits_{n \\to \\infty} U_n(x) \\\\\n     & = \\lim\\limits_{n \\to \\infty} \\sum_{i=1}^{n} q(a_i)p(x-a_i)\\Delta a_i \\\\\n     & = \\int\\limits_{I} q(a)P(x-a)\\mathrm{d}a\n\\end{aligned}\n$$\n\nThen we have\n\n$$\nU(x) = \\int_{-\\infty}^{\\infty} q(a)P(x-a)\\mathrm{d}a\n$$\n\nFurther, let us suppose $P(x)$ is continuously differentiable, and denote its derivative by $p(x)$.\n\nThen we have\n\n$$\nu(x) = \\int_{-\\infty}^{\\infty} q(a)p(x-a)\\mathrm{d}a\n$$\n\nwhere $u(x)$ is the derivative of $U(x)$.\n\nIn a word, we have proved the following fact:\n\n::: callout-note\nConsider two independent experiments whose outcomes lie in some finite interval and have probability $p$ and $q$ respectively.\n\nIn the combined experiment of the two experiments, define the outcome of the combined experiment to be the sum of the outcomes of the individual experiments.\n\nThen the combined experiment has the probability density:\n\n$$\nu(x) = \\int_{-\\infty}^{\\infty} q(a)p(x-a)\\mathrm{d}a\n$$\n:::\n\n9. The convolution of the functions $q$ and $p$:\n\nThe function $u$ defined by $u(x) = \\int_{-\\infty}^{\\infty} q(a)p(x-a)\\mathrm{d}a$ is called the convolution of the functions $q$ and $p$. This relation is denoted by $u = q*p$.\n\nThe convolution has the following properties:\n\n::: callout-note\nLet $q_1$, $q_2$, and $p$ be continuous functions defined for all real numbers $x$, and assume the functions are $0$ outside a finite interval. Then we have\n\n-   Convolution is distributive: $(q_1+q_2)*p = q_1*p + q_2*p$.\n\n-   Let $k$ be any constant. Then $(kq)*p = k(q*p)$.\n\n-   Convolution is commutative: $q*p = p*q$.\n\n-   The integral of the convolution is the product of the integrals of the factors:\n\n$$\n\\int_{-\\infty}^{\\infty} u(x) \\mathrm{d}x = \\int_{-\\infty}^{\\infty} p(x) \\mathrm{d}x \\int_{-\\infty}^{\\infty} q(a) \\mathrm{d}a\n$$\n:::\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"center","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../../styles.css"],"toc":true,"toc-depth":6,"number-sections":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.56","theme":{"light":"cerulean","dark":"slate"},"image":"../../neuron_icon.png","title-block-banner":true,"title":"Probability and statistics (misc)","author":"Rui Yang","date":"2024-09-22","date-modified":"last-modified","categories":["probability","statistics","misc"],"toc-location":"left","number-depth":6,"fig-cap-location":"bottom","lightbox":true,"tbl-cap-location":"top","page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}