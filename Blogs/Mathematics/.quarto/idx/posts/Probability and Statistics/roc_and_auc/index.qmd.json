{"title":"ROC and AUC","markdown":{"yaml":{"title":"ROC and AUC","author":"Rui Yang","date":"2024-09-15","date-modified":"last-modified","categories":["roc","auc"],"format":{"html":{"toc":true,"toc-depth":6,"toc-location":"left","fig-align":"center","number-depth":6,"number-sections":true,"fig-cap-location":"bottom","fig-format":"png","lightbox":true,"tbl-cap-location":"top","page-layout":"full"}},"execute":{"warning":false},"jupyter":"julia-1.10"},"headingText":"Receiver operating characteristic (ROC)","containsRefs":false,"markdown":"\n\n\n### Introduction\n\nA **receiver operating characteristic curve** (**ROC curve**) is a graphical plot that illustrates the performance of a binary classifier (two-class prediction) model at varying threshold values.\n\nThe ROC curve is the plot of the **true positive rate** (TPR) against the **false positive rate** (FPR) at each threshold seeting.\n\nROC analysis provides tools to select possibly optimal models and to discard suboptimal ones independently from the cost context or the class distribution. ROC analysis is related in a direct and natural way to the cost/benefit analysis of diagnostic decision making.\n\nA classification model (classifier or diagnosis) is a mapping of instances between certain classes/groups. Because the classifier or diagnosis result can be an arbitrary real value (continuous output), the classifier boundary between classes must be determined by a threshold value. Or it can be a discrete class label, indicating one of the classes.\n\nConsider an experiment from $\\mathbf{P}$ positive instances and $\\mathbf{N}$ negative instances for some condition. The four outcomes\n\n* True positive (TP): the outcome from a prediction is positve and the actual value is positive.\n\n* Flase positive (FP): the outcome from a prediction is positive but the actual value is negative.\n\n* True negative (TN): the outcome from a prediction is negative and the actual value is negative.\n\n* False negative (FN): the outcome from a prediction is negative but the actual value is positive.\n\ncan be formulated in a $2 \\times 2$ contingency table or confusion matrix, as follows:\n\n![](./figures/20240917-151245743_3c60d20b-7b99-49bb-931c-94f9d630d992.png){.lightbox fig-alt=\"Click to see a larger version of the image\" fig-align=\"center\"}\n\n### ROC space\n\nTo draw a ROC curve, only TPR and FPR are needed (as functions of some classifier parameter).\n\nA ROC space is defined by FPR and TPR as $x$ and $y$ axes, respectively, which depicts relative trade-offs between true positive (benefits) and false positive (costs).\n\nThe best possible prediction method would yield a point in the upper left corner or coordinate $(0, 1)$ of the ROC space, representing $100\\%$ sensitivity (no false negatives) and $100\\%$ specificity (no false positives). The $(0, 1)$ point is also called a perfect classification. A random guess would give a point along a diagonal line (the so-called line of no-discrimination) from the bottom left to the top right corners.\n\nThe diagonal line divides the ROC space into two parts. Points above the diagonal line represent good clasification results (better than random); points below the line represent bad results (worse than random). Note that the output of a consistently bad predictor could simply be inverted to obtain a good predictor.\n\n### Curves in ROC space\n\nIn binary classification, the class prediction for each instance is often made based on a continuous random variable $\\mathbf{X}$, which is a \"score\" computed for the instance. Given a threshold $\\mathbf{T}$, the instance is classified as \"positive\" if $\\mathbf{X} > \\mathbf{T}$, and \"negative\" otherwise. $\\mathbf{X}$ follows a probability density $f_1(x)$ if the instance actually belongs to class \"positive\", and $f_0(x)$ if otherwise.\n\nTherefore, the true positive rate is given by $\\mathbf{TPR(T)} = \\int_{T}^{\\infty} f_1(x) dx$ and the false positive rate is given by $\\mathbf{FPR(T)} = \\int_{T}^{\\infty} f_0(x) dx$.\n\nThe ROC curve plots parametrically $\\mathbf{TPR(T)}$ versus $\\mathbf{FPR(T)}$ with $\\mathbf{T}$ as the varying parameter.\n\nIn the hypothesis testing perspective, we can consider the power as TPR (the probability of correctly rejecting $H_0$), and the type I error as FPR (the probability of wrongly rejecting $H_0$), then the ROC curve is the power as a function of the type I error:\n\n```{julia}\nusing Random, Distributions, CairoMakie\n\nRandom.seed!(1234)\n\n# assume that under both H0 and H1\n# the random variable X is distributed as some normal distributions\nmu0, sigma0 = 10, 2\nmu1, sigma1 = 14, 1\n\ndist0 = Normal(mu0, sigma0)\ndist1 = Normal(mu1, sigma1)\n\nalphas = 0:0.01:1\npowers = @. ccdf(dist1, quantile(dist0, 1 - alphas))\n\nfig, ax = lines(alphas, powers; color=:red, label=\"ROC curve (power vs. alpha)\")\nvlines!(ax, [0.05]; color=:black, label=\"alpha = 0.05\")\naxislegend(ax; position=:rb)\nax.xlabel = \"Alpha\"\nax.ylabel = \"Power\"\nfig\n```\n\nAs shown above, as the type I error grows up to $1$, the power also increases up to $1$. But we wish to reach a balance point where we have a larger power and an acceptable type I error rate (e.g. $0.05$).\n\n## Area under the curve (AUC)\n\nIn addition to those evaluation metrics mentioned in the above table, another evaluation metric, called AUC (area under the ROC curve), defined as\n\n$$\n\\begin{align}\nTPR(T)&: T \\to y(x) \\\\\nFPR(T)&: T \\to x \\\\\nA &= \\int_{x = 0}^{1} TPR(FPR^{-1}(x)) dx \\\\\n&= \\int_{\\infty}^{-\\infty} TPR(T) \\cdot FPR'(T) dT\n\\end{align}\n$$\n\ncan be used to summarize sensitivity and specificity, but it does not inform regarding precision and negative predictive value.\n\nIn fact, AUC is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one (assuming \"positive\" ranks higher than \"negative\"). In other words, when given one randomly selected positive instance and one randomly selected negative instance, AUC is the probability that the classifier will be able to tell which one is which.\n","srcMarkdownNoYaml":"\n\n## Receiver operating characteristic (ROC)\n\n### Introduction\n\nA **receiver operating characteristic curve** (**ROC curve**) is a graphical plot that illustrates the performance of a binary classifier (two-class prediction) model at varying threshold values.\n\nThe ROC curve is the plot of the **true positive rate** (TPR) against the **false positive rate** (FPR) at each threshold seeting.\n\nROC analysis provides tools to select possibly optimal models and to discard suboptimal ones independently from the cost context or the class distribution. ROC analysis is related in a direct and natural way to the cost/benefit analysis of diagnostic decision making.\n\nA classification model (classifier or diagnosis) is a mapping of instances between certain classes/groups. Because the classifier or diagnosis result can be an arbitrary real value (continuous output), the classifier boundary between classes must be determined by a threshold value. Or it can be a discrete class label, indicating one of the classes.\n\nConsider an experiment from $\\mathbf{P}$ positive instances and $\\mathbf{N}$ negative instances for some condition. The four outcomes\n\n* True positive (TP): the outcome from a prediction is positve and the actual value is positive.\n\n* Flase positive (FP): the outcome from a prediction is positive but the actual value is negative.\n\n* True negative (TN): the outcome from a prediction is negative and the actual value is negative.\n\n* False negative (FN): the outcome from a prediction is negative but the actual value is positive.\n\ncan be formulated in a $2 \\times 2$ contingency table or confusion matrix, as follows:\n\n![](./figures/20240917-151245743_3c60d20b-7b99-49bb-931c-94f9d630d992.png){.lightbox fig-alt=\"Click to see a larger version of the image\" fig-align=\"center\"}\n\n### ROC space\n\nTo draw a ROC curve, only TPR and FPR are needed (as functions of some classifier parameter).\n\nA ROC space is defined by FPR and TPR as $x$ and $y$ axes, respectively, which depicts relative trade-offs between true positive (benefits) and false positive (costs).\n\nThe best possible prediction method would yield a point in the upper left corner or coordinate $(0, 1)$ of the ROC space, representing $100\\%$ sensitivity (no false negatives) and $100\\%$ specificity (no false positives). The $(0, 1)$ point is also called a perfect classification. A random guess would give a point along a diagonal line (the so-called line of no-discrimination) from the bottom left to the top right corners.\n\nThe diagonal line divides the ROC space into two parts. Points above the diagonal line represent good clasification results (better than random); points below the line represent bad results (worse than random). Note that the output of a consistently bad predictor could simply be inverted to obtain a good predictor.\n\n### Curves in ROC space\n\nIn binary classification, the class prediction for each instance is often made based on a continuous random variable $\\mathbf{X}$, which is a \"score\" computed for the instance. Given a threshold $\\mathbf{T}$, the instance is classified as \"positive\" if $\\mathbf{X} > \\mathbf{T}$, and \"negative\" otherwise. $\\mathbf{X}$ follows a probability density $f_1(x)$ if the instance actually belongs to class \"positive\", and $f_0(x)$ if otherwise.\n\nTherefore, the true positive rate is given by $\\mathbf{TPR(T)} = \\int_{T}^{\\infty} f_1(x) dx$ and the false positive rate is given by $\\mathbf{FPR(T)} = \\int_{T}^{\\infty} f_0(x) dx$.\n\nThe ROC curve plots parametrically $\\mathbf{TPR(T)}$ versus $\\mathbf{FPR(T)}$ with $\\mathbf{T}$ as the varying parameter.\n\nIn the hypothesis testing perspective, we can consider the power as TPR (the probability of correctly rejecting $H_0$), and the type I error as FPR (the probability of wrongly rejecting $H_0$), then the ROC curve is the power as a function of the type I error:\n\n```{julia}\nusing Random, Distributions, CairoMakie\n\nRandom.seed!(1234)\n\n# assume that under both H0 and H1\n# the random variable X is distributed as some normal distributions\nmu0, sigma0 = 10, 2\nmu1, sigma1 = 14, 1\n\ndist0 = Normal(mu0, sigma0)\ndist1 = Normal(mu1, sigma1)\n\nalphas = 0:0.01:1\npowers = @. ccdf(dist1, quantile(dist0, 1 - alphas))\n\nfig, ax = lines(alphas, powers; color=:red, label=\"ROC curve (power vs. alpha)\")\nvlines!(ax, [0.05]; color=:black, label=\"alpha = 0.05\")\naxislegend(ax; position=:rb)\nax.xlabel = \"Alpha\"\nax.ylabel = \"Power\"\nfig\n```\n\nAs shown above, as the type I error grows up to $1$, the power also increases up to $1$. But we wish to reach a balance point where we have a larger power and an acceptable type I error rate (e.g. $0.05$).\n\n## Area under the curve (AUC)\n\nIn addition to those evaluation metrics mentioned in the above table, another evaluation metric, called AUC (area under the ROC curve), defined as\n\n$$\n\\begin{align}\nTPR(T)&: T \\to y(x) \\\\\nFPR(T)&: T \\to x \\\\\nA &= \\int_{x = 0}^{1} TPR(FPR^{-1}(x)) dx \\\\\n&= \\int_{\\infty}^{-\\infty} TPR(T) \\cdot FPR'(T) dT\n\\end{align}\n$$\n\ncan be used to summarize sensitivity and specificity, but it does not inform regarding precision and negative predictive value.\n\nIn fact, AUC is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one (assuming \"positive\" ranks higher than \"negative\"). In other words, when given one randomly selected positive instance and one randomly selected negative instance, AUC is the probability that the classifier will be able to tell which one is which.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"center","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../../styles.css"],"toc":true,"toc-depth":6,"number-sections":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.56","theme":{"light":"cerulean","dark":"slate"},"image":"../../neuron_icon.png","title-block-banner":true,"title":"ROC and AUC","author":"Rui Yang","date":"2024-09-15","date-modified":"last-modified","categories":["roc","auc"],"jupyter":"julia-1.10","toc-location":"left","number-depth":6,"fig-cap-location":"bottom","lightbox":true,"tbl-cap-location":"top","page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}